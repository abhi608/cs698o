{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from __future__ import division, print_function, unicode_literals\n",
    "import os, sys\n",
    "sys.path.append('/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/')\n",
    "sys.path.append('/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg')\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as tnf\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib.pyplot import imshow\n",
    "# %matplotlib inline\n",
    "# plt.ion()\n",
    "# Import other modules if required\n",
    "from PIL import Image, ImageOps\n",
    "from collections import namedtuple\n",
    "\n",
    "resnet_input = 224 #size of resnet18 input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose your hyper-parameters using validation data\n",
    "Rectangle = namedtuple('Rectangle', 'xmin ymin xmax ymax')\n",
    "\n",
    "batch_size = 25\n",
    "num_epochs = 5\n",
    "learning_rate =  0.005\n",
    "hyp_momentum = 0.9\n",
    "train_dataset_split_ratio = 0.75\n",
    "use_gpu = True and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "Use the following links to locally download the data:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>The dataset consists of images from 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, ie. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the given 20 classes(aeroplane, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, train, TV). For parsing the xml file, you can import xml.etree.ElementTree for you. <br/>\n",
    "<br/> Organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be 21. This is important for applying the sliding window method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('__background__', 'aeroplane', 'bicycle', 'car', 'cat', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "    # Begin\n",
    "    folder = 'train'\n",
    "    raw_image = '../datasets/' + folder + '/JPEGImages/'\n",
    "    anno_image = '../datasets/' + folder + '/Annotations/'\n",
    "    train_dataset = '../datasets/' + folder + '/final/'\n",
    "    \n",
    "    train_classes = {}\n",
    "    for x in classes:\n",
    "        train_classes[x] = []\n",
    "        os.mkdir(os.path.join(train_dataset, x))\n",
    "\n",
    "    for anno_filename in os.listdir(anno_image):\n",
    "        tree = \tET.parse(anno_image+anno_filename)\n",
    "        root = tree.getroot()\n",
    "        file_path = os.path.join(raw_image, str(root.find('filename').text))\n",
    "        # print(file_path)\n",
    "        img = cv2.imread(file_path)\n",
    "        for i,object in enumerate(root.findall('object')):\n",
    "            label = str(object.find('name').text)\n",
    "            if not classes.count(label):\n",
    "                continue\n",
    "            bndbox = object.find('bndbox')\n",
    "            xmin = int(bndbox[0].text)\n",
    "            ymin = int(bndbox[1].text)\n",
    "            xmax = int(bndbox[2].text)\n",
    "            ymax = int(bndbox[3].text)\n",
    "            crop_img = img[ymin:ymax, xmin:xmax]\n",
    "            op_file_path = train_dataset + label + '/' + anno_filename.split('.')[0] + \\\n",
    "                                '_' + str(i) + '.jpg'\n",
    "            # print(op_file_path)\n",
    "            cv2.imwrite(op_file_path, crop_img)\n",
    "            train_classes[label].append(crop_img)\n",
    "\n",
    "# build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class voc_dataset_train(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, start_ratio, end_ratio, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.images, self.labels = [], []\n",
    "        \n",
    "        if train:\n",
    "            root_dir = os.path.join(root_dir, 'train/final')\n",
    "        else:\n",
    "            root_dir = os.path.join(root_dir, 'test')\n",
    "        \n",
    "        label_list = os.listdir(root_dir)\n",
    "        label_list.sort()\n",
    "        print (label_list)\n",
    "        for label in label_list:\n",
    "            label_dir = os.path.join(root_dir, label)\n",
    "            label_map = classes.index(label)\n",
    "            # print(label_map, len(os.listdir(label_dir)))\n",
    "            file_list = os.listdir(label_dir)\n",
    "            file_list.sort()\n",
    "            start_ind = int(start_ratio * len(file_list))\n",
    "            end_ind = int(end_ratio * len(file_list))\n",
    "            for i, image in enumerate(file_list):\n",
    "                if i < start_ind or i >= end_ind:\n",
    "                    continue\n",
    "                    \n",
    "                self.images += [os.path.join(label_dir, image)]\n",
    "                self.labels += [label_map]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Begin\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Begin\n",
    "        img = Image.open(self.images[idx])\n",
    "        img = self.transform(img)\n",
    "        return (img, self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class custom_dataset_test(object):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.idx_to_serve = 0\n",
    "        self.annots = []\n",
    "        self.bndbox_pts = ['xmin','ymin','xmax','ymax']\n",
    "        \n",
    "        self.root_dir = os.path.join(root_dir, 'test')\n",
    "        self.image_dir = os.path.join(self.root_dir, 'JPEGImages')\n",
    "        self.annot_dir = os.path.join(self.root_dir, 'Annotations')\n",
    "        \n",
    "        annot_list = os.listdir(self.annot_dir)\n",
    "        # print(len(annot_list))\n",
    "        annot_list.sort()\n",
    "        for annot_fname in annot_list:\n",
    "            self.annots += [os.path.join(self.annot_dir, annot_fname)]\n",
    "            \n",
    "    def get_len(self):\n",
    "        return len(self.annots)\n",
    "    \n",
    "    def reset_idx(self):\n",
    "        self.idx_to_serve = 0\n",
    "    \n",
    "    def get_next_img(self):\n",
    "        ret_tup = self.get_img(self.idx_to_serve)\n",
    "        self.idx_to_serve = 1 + self.idx_to_serve\n",
    "        return ret_tup\n",
    "        \n",
    "    def get_img(self, idx):\n",
    "        if idx >= self.get_len():\n",
    "            return None\n",
    "        \n",
    "        annot_tree = ET.parse(self.annots[idx])\n",
    "        root = annot_tree.getroot()\n",
    "        local_annot = []\n",
    "        for obj in root.findall('object'):\n",
    "            label = obj.find('name').text\n",
    "            if str(label) in classes:\n",
    "                bndbox = obj.find('bndbox')\n",
    "                local_annot += [(classes.index(label), [int(bndbox.find(pt).text) for pt in self.bndbox_pts])]\n",
    "        local_annot.sort(key=lambda tup: tup[0])\n",
    "        \n",
    "        img = Image.open(os.path.join(self.image_dir, str(root.find('filename').text)))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return (img, local_annot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate the netwok\n",
    "<br/>You can train the network on the created dataset. This will yield a classification network on the 6 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "['__background__', 'aeroplane', 'bicycle', 'car', 'cat', 'dog']\n",
      "Validation\n",
      "['__background__', 'aeroplane', 'bicycle', 'car', 'cat', 'dog']\n",
      "Test\n"
     ]
    }
   ],
   "source": [
    "# Let's look at one batch of train and test images\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "composed_transform = transforms.Compose([transforms.Scale((resnet_input, resnet_input)),\n",
    "                                         transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.ToTensor()])\n",
    "\n",
    "composed_transform_test = transforms.Compose([transforms.RandomHorizontalFlip()])\n",
    "composed_transform_test2 = transforms.Compose([transforms.Scale((resnet_input, resnet_input)),\n",
    "                                              transforms.ToTensor()\n",
    "                                              ])\n",
    "\n",
    "dataset_dir = '../datasets/'\n",
    "\n",
    "print('Train')\n",
    "train_dataset = voc_dataset_train(root_dir=dataset_dir,\n",
    "                                  train=True, \n",
    "                                  transform=composed_transform,\n",
    "                                  start_ratio=0,\n",
    "                                  end_ratio=0.75) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# train_dataiter = iter(train_loader)\n",
    "# train_images, train_labels = train_dataiter.next()\n",
    "# print(train_labels)\n",
    "# imshow(torchvision.utils.make_grid(train_images))\n",
    "\n",
    "print('Validation')\n",
    "valid_dataset = voc_dataset_train(root_dir=dataset_dir,\n",
    "                                  train=True, \n",
    "                                  transform=composed_transform,\n",
    "                                  start_ratio=0.75,\n",
    "                                  end_ratio=1) # Supply proper root_dir\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print('Test')\n",
    "test_dataset = custom_dataset_test(root_dir=dataset_dir, transform=composed_transform_test)\n",
    "\n",
    "# test_image, test_label = test_dataset.get_img(0)\n",
    "# print(str(test_label))\n",
    "# tensor_img = transforms.Compose([transforms.ToTensor()])(test_image)\n",
    "# imshow(tensor_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Use the pre-trained network to fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, len(classes))\n",
    "\n",
    "if use_gpu:\n",
    "    resnet18 = resnet18.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Update if any errors occur\n",
    "optimizer = torch.optim.SGD(resnet18.parameters(), learning_rate, hyp_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Begin\n",
    "    loss_store = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Convert torch tensor to Variable\n",
    "            \n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if (use_gpu):\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            \n",
    "            running_loss += loss.data[0]\n",
    "            \n",
    "            if (i+1)%10 == 0:\n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "        loss_store += [running_loss]\n",
    "\n",
    "    plt.plot(loss_store)\n",
    "    plt.title(\"Sum of Loss over subsequent epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Sum of loss\")\n",
    "    plt.show()\n",
    "    \n",
    "def validate_classification():\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(valid_loader):          \n",
    "        images = Variable(images)\n",
    "        if (use_gpu):\n",
    "            images = images.cuda()\n",
    "            \n",
    "        outputs = resnet18(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "    print('Accuracy of the network on the %d test images: %d %%' % (total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [10/61], Loss: 0.0202\n",
      "Epoch [1/5], Step [20/61], Loss: 0.0081\n",
      "Epoch [1/5], Step [30/61], Loss: 0.0091\n",
      "Epoch [1/5], Step [40/61], Loss: 0.0111\n",
      "Epoch [1/5], Step [50/61], Loss: 0.0207\n",
      "Epoch [1/5], Step [60/61], Loss: 0.0041\n",
      "Epoch [2/5], Step [10/61], Loss: 0.0013\n",
      "Epoch [2/5], Step [20/61], Loss: 0.0032\n",
      "Epoch [2/5], Step [30/61], Loss: 0.0030\n",
      "Epoch [2/5], Step [40/61], Loss: 0.0018\n",
      "Epoch [2/5], Step [50/61], Loss: 0.0038\n"
     ]
    }
   ],
   "source": [
    "%time train()\n",
    "torch.save(resnet18.state_dict(), 'resnet18_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1032 test images: 93 %\n",
      "CPU times: user 26.8 s, sys: 3.1 s, total: 29.9 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "resnet18.load_state_dict(torch.load('resnet18_model.pkl'))\n",
    "%time validate_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "For applying detection, use a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "Take some windows of varying size and aspect ratios and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value. There is a similar approach used in the paper -Faster RCNN by Ross Girshick, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide. You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_window(img, step_size=16, window_size=(64,64)):\n",
    "    # Begin\n",
    "    pad_updown, pad_side = window_size[0]//2, window_size[1]//2\n",
    "    old_size = img.size\n",
    "    new_size = (old_size[0]+window_size[0], old_size[1]+window_size[1])\n",
    "    new_img = Image.new(\"RGB\", new_size)\n",
    "    new_img.paste(img, (pad_updown, pad_side))\n",
    "    \n",
    "    # plt.imshow(new_img)\n",
    "    # new_img.show()\n",
    "    # print(new_size)\n",
    "    arr = []\n",
    "    for y in range(pad_updown, new_size[0]-pad_updown, step_size):\n",
    "        for x in range(pad_side, new_size[1]-pad_side, step_size):\n",
    "            # print(x,y)\n",
    "            cropped_img = new_img.crop((x, y, x+window_size[0], y+window_size[1]))\n",
    "            arr.append([[x, y, x + window_size[0]-1, y + window_size[1]-1], \n",
    "                        composed_transform_test2(cropped_img)])\n",
    "    return arr\n",
    "\n",
    "# img, lab = test_dataset.get_img(0)\n",
    "# tmp = sliding_window(img)\n",
    "# arr = [x[1] for x in tmp]\n",
    "# imshow(torchvision.utils.make_grid(arr[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply non_maximum_supression to reduce the number of boxes. You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_maximum_supression(boxes, threshold=0.3):\n",
    "    # Begin\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    " \n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    scores = boxes[:,0]\n",
    "    x1 = boxes[:,1]\n",
    "    y1 = boxes[:,2]\n",
    "    x2 = boxes[:,3]\n",
    "    y2 = boxes[:,4]\n",
    "\n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(scores)\n",
    "    \n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list, add the index\n",
    "        # value to the list of picked indexes, then initialize\n",
    "        # the suppression list (i.e. indexes that will be deleted)\n",
    "        # using the last index\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        suppress = [last]\n",
    "        \n",
    "        # loop over all indexes in the indexes list\n",
    "        for pos in xrange(0, last):\n",
    "            # grab the current index\n",
    "            j = idxs[pos]\n",
    "\n",
    "            # find the largest (x, y) coordinates for the start of\n",
    "            # the bounding box and the smallest (x, y) coordinates\n",
    "            # for the end of the bounding box\n",
    "            xx1 = max(x1[i], x1[j])\n",
    "            yy1 = max(y1[i], y1[j])\n",
    "            xx2 = min(x2[i], x2[j])\n",
    "            yy2 = min(y2[i], y2[j])\n",
    "\n",
    "            # compute the width and height of the bounding box\n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    "\n",
    "            # compute the ratio of overlap between the computed\n",
    "            # bounding box and the bounding box in the area list\n",
    "            intersection = float(w * h)\n",
    "            overlap = intersection / area[i]\n",
    "\n",
    "            # if there is sufficient overlap, suppress the\n",
    "            # current bounding box\n",
    "            if overlap > threshold:\n",
    "                suppress.append(pos)\n",
    "\n",
    "        # delete all indexes from the index list that are in the\n",
    "        # suppression list\n",
    "        idxs = np.delete(idxs, suppress)\n",
    "\n",
    "    # return only the bounding boxes that were picked\n",
    "    return boxes[pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_get_tensor(tensor_list):\n",
    "    nlist = []\n",
    "    for img_tup in tensor_list:\n",
    "        nlist += [img_tup[1]]\n",
    "    \n",
    "    to_tensor = torch.cat(nlist[:], 1)\n",
    "    return to_tensor.resize_(len(nlist), 3, resnet_input, resnet_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def area_intersection(a, b):  # returns None if rectangles don't intersect\n",
    "    dx = min(a.xmax, b.xmax) - max(a.xmin, b.xmin)\n",
    "    dy = min(a.ymax, b.ymax) - max(a.ymin, b.ymin)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return dx*dy\n",
    "    return 0\n",
    "\n",
    "def area(a):\n",
    "    dx = a.xmax - a.xmin\n",
    "    dy = a.ymax - a.ymin\n",
    "    return dx*dy\n",
    "\n",
    "def intersection_over_union(rect1, rect2):\n",
    "    intersect_area = area_intersection(rect1, rect2)\n",
    "    total_area = area(rect1) + area(rect2) - intersect_area\n",
    "    return intersect_area/total_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(resnet18, threshold_score=20.0, map_iou_threshold=0.5):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # Also print out the accuracy of the model\n",
    "    mmap = 0\n",
    "    count = 0\n",
    "    for ind in range(test_dataset.get_len()):\n",
    "        img, anno = test_dataset.get_img(ind)\n",
    "        \n",
    "        pred = []\n",
    "        for x in range(len(classes)):\n",
    "            pred += [[]]\n",
    "        test_batch = 25\n",
    "        for side1 in [64,128,256]:\n",
    "            for ratio in [0.5,1,2.0]:\n",
    "                windows = sliding_window(img=img, window_size=(side1,int(side1*ratio)))\n",
    "                # print(len(windows))\n",
    "\n",
    "                w_ind, w_lim = 0, len(windows)\n",
    "                while w_ind < w_lim:\n",
    "                    my_inp = filter_get_tensor(windows[w_ind:w_ind+test_batch])\n",
    "                    # imshow(torchvision.utils.make_grid(my_inp[0:2]))\n",
    "                    # print(my_inp.size())\n",
    "\n",
    "                    my_inp = Variable(my_inp)\n",
    "                    if use_gpu:\n",
    "                        my_inp = my_inp.cuda()\n",
    "                    output = resnet18(my_inp) # matrix of size nx6, where n <= test_batch\n",
    "                    # print(output)\n",
    "\n",
    "                    for inp in range(output.size()[0]):\n",
    "                        for op_label in range(1, len(classes)):\n",
    "                            if threshold_score < (output.data)[inp][op_label]:\n",
    "                                pred[op_label] += [[(output.data)[inp][op_label]] + windows[w_ind+inp][0]]\n",
    "\n",
    "                    w_ind += test_batch\n",
    "                    \n",
    "        output_classes = [[]]\n",
    "        anno_classes = [[]]\n",
    "        for i in xrange(1,6):\n",
    "            anno_classes.append([])\n",
    "            # print('Prediction for label ', i, pred[i])\n",
    "            output_classes.append(non_maximum_supression(np.array(pred[i]))[1:])\n",
    "        \n",
    "        for i, temp in enumerate(anno):\n",
    "            anno_classes[anno[i][0]].append((anno[i][1][0], anno[i][1][1], anno[i][1][2], anno[i][1][3]))\n",
    "            \n",
    "        flag, flag1 = False, False\n",
    "        no_of_classes = 0\n",
    "        mean_average_precision = 0\n",
    "        for i, temp1 in enumerate(anno_classes):\n",
    "            average_precision, precision = 0, 0\n",
    "            seen, approved = 0, 0\n",
    "            if len(anno_classes[i]) > 0:\n",
    "                if flag1 == False:\n",
    "                    count = count + 1\n",
    "                    flag1 = True\n",
    "                for j, temp2 in enumerate(output_classes[i]):\n",
    "                    flag = False\n",
    "                    for k, temp3 in enumerate(anno_classes[i]):\n",
    "                        iou = intersection_over_union(Rectangle(anno_classes[i][k][0], anno_classes[i][k][1], anno_classes[i][k][2], anno_classes[i][k][3]),\n",
    "                                                     Rectangle(output_classes[i][j][0], output_classes[i][j][1], output_classes[i][j][2], output_classes[i][j][3]))\n",
    "                        if iou > map_iou_threshold:\n",
    "                            flag = True\n",
    "                            break\n",
    "                    seen = seen + 1\n",
    "                    if flag == True:\n",
    "                        approved = approved + 1\n",
    "                        precision = precision + approved/seen\n",
    "                if approved != 0:\n",
    "                    average_precision = precision/approved\n",
    "                mean_average_precision = mean_average_precision + average_precision\n",
    "                no_of_classes = no_of_classes + 1\n",
    "                print(\"[{}] class_label, AP, mAP: {}, {}, {}\".format(count, classes[i], average_precision, mean_average_precision))\n",
    "        if no_of_classes != 0:\n",
    "            mean_average_precision = mean_average_precision/no_of_classes\n",
    "        mmap = mmap + mean_average_precision\n",
    "        print(\"[{}] mmap_current: {}\".format(count, mmap/count))\n",
    "\n",
    "    mmap = mmap / count\n",
    "    print(\"MMAP:\", mmap)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] class_label, AP, mAP: dog, 0, 0\n",
      "[1] mmap_current: 0\n",
      "[1] mmap_current: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9083915b1c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time test(resnet18)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ecc9d73dbeb0>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(resnet18, threshold_score, map_iou_threshold)\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                         \u001b[0mmy_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_inp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# matrix of size nx6, where n <= test_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                     \u001b[0;31m# print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 254\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     50\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     51\u001b[0m                _pair(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    }
   ],
   "source": [
    "%time test(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
