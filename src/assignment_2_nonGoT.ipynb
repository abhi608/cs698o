{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as tnf\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "# Import other modules if required\n",
    "\n",
    "resnet_input = #size of resnet18 input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose your hyper-parameters using validation data\n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "learning_rate =  0.001\n",
    "hyp_momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "Use the following links to locally download the data:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>The dataset consists of images from 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, ie. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the given 20 classes(aeroplane, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, train, TV). For parsing the xml file, you can import xml.etree.ElementTree for you. <br/>\n",
    "<br/> Organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be 21. This is important for applying the sliding window method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "           'cow', 'diningtable', 'dog', 'horse',\n",
    "           'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "    # Begin\n",
    "    folders = (\"train\",\"test\")\n",
    "\n",
    "    train_classes = {\n",
    "        'aeroplane' : [],\n",
    "        'bicycle' : [],\n",
    "        'bird' : [],\n",
    "        'boat' : [],\n",
    "        'bottle' : [],\n",
    "        'bus' : [],\n",
    "        'car' : [],\n",
    "        'cat' : [],\n",
    "        'chair' : [],\n",
    "        'cow' : [],\n",
    "        'diningtable' : [],\n",
    "        'dog' : [],\n",
    "        'horse' : [],\n",
    "        'motorbike' : [],\n",
    "        'person' : [],\n",
    "        'pottedplant' : [],\n",
    "        'sheep' : [],\n",
    "        'sofa' : [],\n",
    "        'train' : [],\n",
    "        'tvmonitor' : []\n",
    "    }\n",
    "    test_classes = train_classes\n",
    "\n",
    "\n",
    "    for dir in folders:\n",
    "        print dir\n",
    "        raw_image = '../datasets/'+dir+'/JPEGImages/'\n",
    "        anno_image = '../datasets/'+dir+'/Annotations/'\n",
    "        train_dataset = '../datasets/train/final/'\n",
    "        for anno_filename in os.listdir(anno_image):\n",
    "            tree = \tET.parse(anno_image+anno_filename)\n",
    "            tmp = anno_filename.split('.')\n",
    "            img_filename = tmp[0] + '.jpg'\n",
    "            print anno_filename, img_filename\n",
    "            img = cv2.imread(raw_image+img_filename)\n",
    "            root = tree.getroot()\n",
    "            print root\n",
    "            for i,object in enumerate(root.findall('object')):\n",
    "                class1 = str(object.find('name').text)\n",
    "                bndbox = object.find('bndbox')\n",
    "                print class1, bndbox[0].text, bndbox[1].text, bndbox[2].text, bndbox[3].text\n",
    "                class1 = str(object[0].text)\n",
    "                xmin = int(bndbox[0].text)\n",
    "                ymin = int(bndbox[1].text)\n",
    "                xmax = int(bndbox[2].text)\n",
    "                ymax = int(bndbox[3].text)\n",
    "                crop_img = img[ymin:ymax, xmin:xmax]\n",
    "                if dir == 'train':\n",
    "                    cv2.imwrite(train_dataset+class1+'/'+tmp[0]+'_'+str(i)+'.jpg', crop_img)\n",
    "                    train_classes[class1].append(crop_img)\n",
    "                # else:\n",
    "                # \ttest_classes[class1].append(crop_img)\n",
    "            # break\n",
    "\n",
    "    print train_classes, test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class voc_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        build_dataset()\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.images, self.labels = [], []\n",
    "        \n",
    "        if train:\n",
    "            root_dir = os.path.join(root_dir, 'train/final')\n",
    "        else:\n",
    "            root_dir = os.path.join(root_dir, 'test')\n",
    "        \n",
    "        label_list = os.listdir(root_dir)\n",
    "        label_list.sort()\n",
    "        for label in label_list:\n",
    "            label_dir = os.path.join(root_dir, label)\n",
    "            label_map = classes.index(label)\n",
    "            # print(ord(label) - ord('A'), len(os.listdir(label_dir)))\n",
    "            for image in os.listdir(label_dir):\n",
    "                self.images += [os.path.join(label_dir, image)]\n",
    "                self.labels += [label_map]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Begin\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "       # Begin\n",
    "        img = Image.open(self.images[idx])\n",
    "        img = self.transform(img) #img.convert('RGB')\n",
    "        return (img, self.labels[idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the netwok\n",
    "<br/>You can train the network on the created dataset. This will yield a classification network on the 21 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((resnet_input,resnet_input)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.RandomHorizontalFlip()])\n",
    "train_dataset = voc_dataset(root_dir='../datasets/', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = voc_dataset(root_dir='', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Use the pre-trained network to fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 21)\n",
    "\n",
    "# Add code for using CUDA here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Update if any errors occur\n",
    "optimizer = optim.SGD(resnet18.parameters(), learning_rate, hyp_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "For applying detection, use a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "Take some windows of varying size and aspect ratios and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value. There is a similar approach used in the paper -Faster RCNN by Ross Girshick, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide. You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_window():\n",
    "    # Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply non_maximum_supression to reduce the number of boxes. You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_maximum_supression(boxes,threshold = 0.3):\n",
    "    # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(resnet18):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # Also print out the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time test(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
