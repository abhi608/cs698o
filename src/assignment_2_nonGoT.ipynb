{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import os, sys\n",
    "# sys.path.append('/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/')\n",
    "# sys.path.append('/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg')\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as tnf\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "# Import other modules if required\n",
    "\n",
    "resnet_input = 224 #size of resnet18 input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose your hyper-parameters using validation data\n",
    "batch_size = 10\n",
    "num_epochs = 5\n",
    "learning_rate =  0.001\n",
    "hyp_momentum = 0.9\n",
    "use_gpu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "Use the following links to locally download the data:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>The dataset consists of images from 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, ie. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the given 20 classes(aeroplane, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, train, TV). For parsing the xml file, you can import xml.etree.ElementTree for you. <br/>\n",
    "<br/> Organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be 21. This is important for applying the sliding window method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "           'cow', 'diningtable', 'dog', 'horse',\n",
    "           'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "    # Begin\n",
    "    folders = (\"train\",\"test\")\n",
    "\n",
    "    train_classes = {\n",
    "        'aeroplane' : [],\n",
    "        'bicycle' : [],\n",
    "        'bird' : [],\n",
    "        'boat' : [],\n",
    "        'bottle' : [],\n",
    "        'bus' : [],\n",
    "        'car' : [],\n",
    "        'cat' : [],\n",
    "        'chair' : [],\n",
    "        'cow' : [],\n",
    "        'diningtable' : [],\n",
    "        'dog' : [],\n",
    "        'horse' : [],\n",
    "        'motorbike' : [],\n",
    "        'person' : [],\n",
    "        'pottedplant' : [],\n",
    "        'sheep' : [],\n",
    "        'sofa' : [],\n",
    "        'train' : [],\n",
    "        'tvmonitor' : []\n",
    "    }\n",
    "    test_classes = train_classes\n",
    "\n",
    "\n",
    "    for dir in folders:\n",
    "        raw_image = '../datasets/'+dir+'/JPEGImages/'\n",
    "        anno_image = '../datasets/'+dir+'/Annotations/'\n",
    "        train_dataset = '../datasets/train/final/'\n",
    "        for anno_filename in os.listdir(anno_image):\n",
    "            tree = \tET.parse(anno_image+anno_filename)\n",
    "            tmp = anno_filename.split('.')\n",
    "            img_filename = tmp[0] + '.jpg'\n",
    "            img = cv2.imread(raw_image+img_filename)\n",
    "            root = tree.getroot()\n",
    "            for i,object in enumerate(root.findall('object')):\n",
    "                class1 = str(object.find('name').text)\n",
    "                bndbox = object.find('bndbox')\n",
    "                class1 = str(object[0].text)\n",
    "                xmin = int(bndbox[0].text)\n",
    "                ymin = int(bndbox[1].text)\n",
    "                xmax = int(bndbox[2].text)\n",
    "                ymax = int(bndbox[3].text)\n",
    "                crop_img = img[ymin:ymax, xmin:xmax]\n",
    "                if dir == 'train':\n",
    "                    cv2.imwrite(train_dataset+class1+'/'+tmp[0]+'_'+str(i)+'.jpg', crop_img)\n",
    "                    train_classes[class1].append(crop_img)\n",
    "                # else:\n",
    "                # \ttest_classes[class1].append(crop_img)\n",
    "            # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class voc_dataset_train(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.images, self.labels = [], []\n",
    "        \n",
    "        if train:\n",
    "            root_dir = os.path.join(root_dir, 'train/final')\n",
    "        else:\n",
    "            root_dir = os.path.join(root_dir, 'test')\n",
    "        \n",
    "        label_list = os.listdir(root_dir)\n",
    "        label_list.sort()\n",
    "        print (label_list)\n",
    "        for label in label_list:\n",
    "            label_dir = os.path.join(root_dir, label)\n",
    "            label_map = classes.index(label)\n",
    "            # print(label_map, len(os.listdir(label_dir)))\n",
    "            for image in os.listdir(label_dir):\n",
    "                self.images += [os.path.join(label_dir, image)]\n",
    "                self.labels += [label_map]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Begin\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Begin\n",
    "        img = Image.open(self.images[idx])\n",
    "        img = self.transform(img)\n",
    "        return (img, self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class voc_dataset_test(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train=False, transform=None):\n",
    "        # Begin\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.annots = []\n",
    "        self.bndbox_pts = ['xmin','ymin','xmax','ymax']\n",
    "        \n",
    "        self.root_dir = os.path.join(root_dir, 'test')\n",
    "        self.image_dir = os.path.join(self.root_dir, 'JPEGImages')\n",
    "        self.annot_dir = os.path.join(self.root_dir, 'Annotations')\n",
    "        \n",
    "        annot_list = os.listdir(self.annot_dir)\n",
    "        annot_list.sort()\n",
    "        for annot_fname in annot_list:\n",
    "            self.annots += [os.path.join(self.annot_dir, annot_fname)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Begin\n",
    "        return len(self.annots)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Begin\n",
    "        annot_tree = ET.parse(self.annots[idx])\n",
    "        root = annot_tree.getroot()\n",
    "        img = cv2.imread(os.path.join(self.image_dir, str(root.find('filename').text)))\n",
    "        local_annot = []\n",
    "        for obj in root.findall('object'):\n",
    "            label = obj.find('name').text\n",
    "            bndbox = obj.find('bndbox')\n",
    "            local_annot += [(label, [int(bndbox.find(pt).text) for pt in self.bndbox_pts])]\n",
    "            \n",
    "        return (img, local_annot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the netwok\n",
    "<br/>You can train the network on the created dataset. This will yield a classification network on the 21 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'aeroplane', u'bicycle', u'bird', u'boat', u'bottle', u'bus', u'car', u'cat', u'chair', u'cow', u'diningtable', u'dog', u'horse', u'motorbike', u'person', u'pottedplant', u'sheep', u'sofa', u'train', u'tvmonitor']\n",
      "[[('dog',), [\n",
      " 48\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 240\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 195\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 371\n",
      "[torch.LongTensor of size 1]\n",
      "]], [('person',), [\n",
      " 8\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 12\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 352\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 498\n",
      "[torch.LongTensor of size 1]\n",
      "]]]\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((resnet_input,resnet_input)),\n",
    "                                         transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.ToTensor()])\n",
    "\n",
    "dataset_dir = '../datasets/'\n",
    "train_dataset = voc_dataset_train(root_dir=dataset_dir, train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = voc_dataset_test(root_dir=dataset_dir, train=False, transform=transforms.RandomHorizontalFlip())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# # Let's look at one batch of train and test images\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "train_dataiter = iter(train_loader)\n",
    "train_images, train_labels = train_dataiter.next()\n",
    "# print(\"Train images\")\n",
    "# print(train_labels)\n",
    "# imshow(torchvision.utils.make_grid(train_images))\n",
    "\n",
    "test_dataiter = iter(test_loader)\n",
    "test_images, test_labels = test_dataiter.next()\n",
    "print(str(test_labels))\n",
    "# imshow(test_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Use the pre-trained network to fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 21)\n",
    "\n",
    "# Add code for using CUDA here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Update if any errors occur\n",
    "optimizer = torch.optim.SGD(resnet18.parameters(), learning_rate, hyp_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Begin\n",
    "    loss_store = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if (use_gpu):\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            \n",
    "            running_loss += loss.data[0]\n",
    "            \n",
    "            if (i+1)%10 == 0:\n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "        loss_store += [running_loss]\n",
    "\n",
    "    plt.plot(loss_store)\n",
    "    plt.title(\"Sum of Loss over subsequent epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Sum of loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [10/1646], Loss: 2.4698\n",
      "Epoch [1/5], Step [20/1646], Loss: 2.0149\n",
      "Epoch [1/5], Step [30/1646], Loss: 1.8933\n",
      "Epoch [1/5], Step [40/1646], Loss: 1.5283\n",
      "Epoch [1/5], Step [50/1646], Loss: 1.0842\n",
      "Epoch [1/5], Step [60/1646], Loss: 2.1607\n",
      "Epoch [1/5], Step [70/1646], Loss: 1.3776\n",
      "Epoch [1/5], Step [80/1646], Loss: 1.2806\n",
      "Epoch [1/5], Step [90/1646], Loss: 1.6860\n",
      "Epoch [1/5], Step [100/1646], Loss: 1.7764\n",
      "Epoch [1/5], Step [110/1646], Loss: 1.0725\n",
      "Epoch [1/5], Step [120/1646], Loss: 1.2006\n",
      "Epoch [1/5], Step [130/1646], Loss: 0.6563\n",
      "Epoch [1/5], Step [140/1646], Loss: 1.4340\n",
      "Epoch [1/5], Step [150/1646], Loss: 1.2169\n",
      "Epoch [1/5], Step [160/1646], Loss: 1.3653\n",
      "Epoch [1/5], Step [170/1646], Loss: 1.0261\n",
      "Epoch [1/5], Step [180/1646], Loss: 0.9962\n",
      "Epoch [1/5], Step [190/1646], Loss: 1.0139\n",
      "Epoch [1/5], Step [200/1646], Loss: 1.1964\n",
      "Epoch [1/5], Step [210/1646], Loss: 0.4838\n",
      "Epoch [1/5], Step [220/1646], Loss: 0.9952\n",
      "Epoch [1/5], Step [230/1646], Loss: 1.4261\n",
      "Epoch [1/5], Step [240/1646], Loss: 0.9012\n",
      "Epoch [1/5], Step [250/1646], Loss: 1.1868\n",
      "Epoch [1/5], Step [260/1646], Loss: 0.4832\n",
      "Epoch [1/5], Step [270/1646], Loss: 0.9109\n",
      "Epoch [1/5], Step [280/1646], Loss: 1.0833\n",
      "Epoch [1/5], Step [290/1646], Loss: 0.2977\n",
      "Epoch [1/5], Step [300/1646], Loss: 0.3843\n",
      "Epoch [1/5], Step [310/1646], Loss: 0.8212\n",
      "Epoch [1/5], Step [320/1646], Loss: 1.3273\n",
      "Epoch [1/5], Step [330/1646], Loss: 0.7653\n",
      "Epoch [1/5], Step [340/1646], Loss: 0.7865\n",
      "Epoch [1/5], Step [350/1646], Loss: 1.8326\n",
      "Epoch [1/5], Step [360/1646], Loss: 0.9002\n",
      "Epoch [1/5], Step [370/1646], Loss: 1.1314\n",
      "Epoch [1/5], Step [380/1646], Loss: 0.2873\n",
      "Epoch [1/5], Step [390/1646], Loss: 1.2996\n",
      "Epoch [1/5], Step [400/1646], Loss: 0.1372\n",
      "Epoch [1/5], Step [410/1646], Loss: 0.4776\n",
      "Epoch [1/5], Step [420/1646], Loss: 0.4959\n",
      "Epoch [1/5], Step [430/1646], Loss: 0.5371\n",
      "Epoch [1/5], Step [440/1646], Loss: 0.9587\n",
      "Epoch [1/5], Step [450/1646], Loss: 0.4020\n",
      "Epoch [1/5], Step [460/1646], Loss: 0.5451\n",
      "Epoch [1/5], Step [470/1646], Loss: 0.5621\n",
      "Epoch [1/5], Step [480/1646], Loss: 1.2225\n",
      "Epoch [1/5], Step [490/1646], Loss: 1.4713\n",
      "Epoch [1/5], Step [500/1646], Loss: 0.5030\n",
      "Epoch [1/5], Step [510/1646], Loss: 0.6967\n",
      "Epoch [1/5], Step [520/1646], Loss: 0.3007\n",
      "Epoch [1/5], Step [530/1646], Loss: 0.6200\n",
      "Epoch [1/5], Step [540/1646], Loss: 0.2549\n",
      "Epoch [1/5], Step [550/1646], Loss: 0.5022\n",
      "Epoch [1/5], Step [560/1646], Loss: 0.4788\n",
      "Epoch [1/5], Step [570/1646], Loss: 0.2162\n",
      "Epoch [1/5], Step [580/1646], Loss: 0.7230\n",
      "Epoch [1/5], Step [590/1646], Loss: 0.5211\n",
      "Epoch [1/5], Step [600/1646], Loss: 0.2425\n",
      "Epoch [1/5], Step [610/1646], Loss: 0.1950\n",
      "Epoch [1/5], Step [620/1646], Loss: 0.5387\n",
      "Epoch [1/5], Step [630/1646], Loss: 0.6977\n",
      "Epoch [1/5], Step [640/1646], Loss: 0.5852\n",
      "Epoch [1/5], Step [650/1646], Loss: 0.7491\n",
      "Epoch [1/5], Step [660/1646], Loss: 1.1719\n",
      "Epoch [1/5], Step [670/1646], Loss: 0.4129\n",
      "Epoch [1/5], Step [680/1646], Loss: 0.5298\n",
      "Epoch [1/5], Step [690/1646], Loss: 0.9246\n",
      "Epoch [1/5], Step [700/1646], Loss: 0.6695\n",
      "Epoch [1/5], Step [710/1646], Loss: 0.9269\n",
      "Epoch [1/5], Step [720/1646], Loss: 0.2318\n",
      "Epoch [1/5], Step [730/1646], Loss: 1.0972\n",
      "Epoch [1/5], Step [740/1646], Loss: 0.1720\n",
      "Epoch [1/5], Step [750/1646], Loss: 0.4060\n",
      "Epoch [1/5], Step [760/1646], Loss: 1.6094\n",
      "Epoch [1/5], Step [770/1646], Loss: 0.3407\n",
      "Epoch [1/5], Step [780/1646], Loss: 0.9899\n",
      "Epoch [1/5], Step [790/1646], Loss: 1.8424\n",
      "Epoch [1/5], Step [800/1646], Loss: 0.5022\n",
      "Epoch [1/5], Step [810/1646], Loss: 1.1600\n",
      "Epoch [1/5], Step [820/1646], Loss: 1.5324\n",
      "Epoch [1/5], Step [830/1646], Loss: 0.4835\n",
      "Epoch [1/5], Step [840/1646], Loss: 0.6812\n",
      "Epoch [1/5], Step [850/1646], Loss: 0.7439\n",
      "Epoch [1/5], Step [860/1646], Loss: 0.7062\n",
      "Epoch [1/5], Step [870/1646], Loss: 0.5431\n",
      "Epoch [1/5], Step [880/1646], Loss: 0.5502\n",
      "Epoch [1/5], Step [890/1646], Loss: 0.7086\n",
      "Epoch [1/5], Step [900/1646], Loss: 1.1451\n",
      "Epoch [1/5], Step [910/1646], Loss: 0.6733\n",
      "Epoch [1/5], Step [920/1646], Loss: 0.8742\n",
      "Epoch [1/5], Step [930/1646], Loss: 0.1975\n",
      "Epoch [1/5], Step [940/1646], Loss: 0.2754\n",
      "Epoch [1/5], Step [950/1646], Loss: 0.7244\n",
      "Epoch [1/5], Step [960/1646], Loss: 1.2045\n",
      "Epoch [1/5], Step [970/1646], Loss: 0.9502\n",
      "Epoch [1/5], Step [980/1646], Loss: 0.2578\n",
      "Epoch [1/5], Step [990/1646], Loss: 0.2841\n",
      "Epoch [1/5], Step [1000/1646], Loss: 0.5961\n",
      "Epoch [1/5], Step [1010/1646], Loss: 0.5076\n",
      "Epoch [1/5], Step [1020/1646], Loss: 1.3332\n",
      "Epoch [1/5], Step [1030/1646], Loss: 0.1674\n",
      "Epoch [1/5], Step [1040/1646], Loss: 0.3568\n",
      "Epoch [1/5], Step [1050/1646], Loss: 0.4769\n",
      "Epoch [1/5], Step [1060/1646], Loss: 0.5941\n",
      "Epoch [1/5], Step [1070/1646], Loss: 1.1359\n",
      "Epoch [1/5], Step [1080/1646], Loss: 0.5019\n",
      "Epoch [1/5], Step [1090/1646], Loss: 0.4581\n",
      "Epoch [1/5], Step [1100/1646], Loss: 0.8147\n",
      "Epoch [1/5], Step [1110/1646], Loss: 0.5831\n",
      "Epoch [1/5], Step [1120/1646], Loss: 0.6630\n",
      "Epoch [1/5], Step [1130/1646], Loss: 0.8972\n",
      "Epoch [1/5], Step [1140/1646], Loss: 0.4381\n",
      "Epoch [1/5], Step [1150/1646], Loss: 0.7706\n",
      "Epoch [1/5], Step [1160/1646], Loss: 0.8494\n",
      "Epoch [1/5], Step [1170/1646], Loss: 0.7269\n",
      "Epoch [1/5], Step [1180/1646], Loss: 0.8555\n",
      "Epoch [1/5], Step [1190/1646], Loss: 0.9987\n",
      "Epoch [1/5], Step [1200/1646], Loss: 0.7029\n",
      "Epoch [1/5], Step [1210/1646], Loss: 0.6368\n",
      "Epoch [1/5], Step [1220/1646], Loss: 0.4550\n",
      "Epoch [1/5], Step [1230/1646], Loss: 0.1788\n",
      "Epoch [1/5], Step [1240/1646], Loss: 0.3661\n",
      "Epoch [1/5], Step [1250/1646], Loss: 0.1537\n",
      "Epoch [1/5], Step [1260/1646], Loss: 0.5986\n",
      "Epoch [1/5], Step [1270/1646], Loss: 0.2433\n",
      "Epoch [1/5], Step [1280/1646], Loss: 0.7919\n",
      "Epoch [1/5], Step [1290/1646], Loss: 0.9952\n",
      "Epoch [1/5], Step [1300/1646], Loss: 0.2317\n",
      "Epoch [1/5], Step [1310/1646], Loss: 1.1340\n",
      "Epoch [1/5], Step [1320/1646], Loss: 0.2109\n",
      "Epoch [1/5], Step [1330/1646], Loss: 0.7546\n",
      "Epoch [1/5], Step [1340/1646], Loss: 0.6577\n",
      "Epoch [1/5], Step [1350/1646], Loss: 0.3339\n",
      "Epoch [1/5], Step [1360/1646], Loss: 1.4899\n",
      "Epoch [1/5], Step [1370/1646], Loss: 0.6154\n",
      "Epoch [1/5], Step [1380/1646], Loss: 0.4388\n",
      "Epoch [1/5], Step [1390/1646], Loss: 1.0635\n",
      "Epoch [1/5], Step [1400/1646], Loss: 0.6987\n",
      "Epoch [1/5], Step [1410/1646], Loss: 0.5019\n",
      "Epoch [1/5], Step [1420/1646], Loss: 0.1637\n",
      "Epoch [1/5], Step [1430/1646], Loss: 1.0303\n",
      "Epoch [1/5], Step [1440/1646], Loss: 0.1857\n",
      "Epoch [1/5], Step [1450/1646], Loss: 0.4189\n",
      "Epoch [1/5], Step [1460/1646], Loss: 0.3467\n",
      "Epoch [1/5], Step [1470/1646], Loss: 0.7916\n",
      "Epoch [1/5], Step [1480/1646], Loss: 0.3170\n",
      "Epoch [1/5], Step [1490/1646], Loss: 0.2845\n",
      "Epoch [1/5], Step [1500/1646], Loss: 0.4695\n",
      "Epoch [1/5], Step [1510/1646], Loss: 0.1972\n",
      "Epoch [1/5], Step [1520/1646], Loss: 0.6044\n",
      "Epoch [1/5], Step [1530/1646], Loss: 0.6232\n",
      "Epoch [1/5], Step [1540/1646], Loss: 0.6914\n",
      "Epoch [1/5], Step [1550/1646], Loss: 1.1854\n",
      "Epoch [1/5], Step [1560/1646], Loss: 0.4001\n",
      "Epoch [1/5], Step [1570/1646], Loss: 0.2171\n",
      "Epoch [1/5], Step [1580/1646], Loss: 0.4610\n",
      "Epoch [1/5], Step [1590/1646], Loss: 1.4053\n",
      "Epoch [1/5], Step [1600/1646], Loss: 0.2521\n",
      "Epoch [1/5], Step [1610/1646], Loss: 0.4922\n",
      "Epoch [1/5], Step [1620/1646], Loss: 1.1542\n",
      "Epoch [1/5], Step [1630/1646], Loss: 0.4303\n",
      "Epoch [1/5], Step [1640/1646], Loss: 0.0677\n",
      "Epoch [2/5], Step [10/1646], Loss: 0.1724\n",
      "Epoch [2/5], Step [20/1646], Loss: 1.2194\n",
      "Epoch [2/5], Step [30/1646], Loss: 0.1801\n",
      "Epoch [2/5], Step [40/1646], Loss: 0.3049\n",
      "Epoch [2/5], Step [50/1646], Loss: 0.2448\n",
      "Epoch [2/5], Step [60/1646], Loss: 0.1815\n",
      "Epoch [2/5], Step [70/1646], Loss: 0.3132\n",
      "Epoch [2/5], Step [80/1646], Loss: 0.5956\n",
      "Epoch [2/5], Step [90/1646], Loss: 0.3320\n",
      "Epoch [2/5], Step [100/1646], Loss: 0.2807\n",
      "Epoch [2/5], Step [110/1646], Loss: 0.0532\n",
      "Epoch [2/5], Step [120/1646], Loss: 0.2414\n",
      "Epoch [2/5], Step [130/1646], Loss: 0.5485\n",
      "Epoch [2/5], Step [140/1646], Loss: 0.8436\n",
      "Epoch [2/5], Step [150/1646], Loss: 0.5199\n",
      "Epoch [2/5], Step [160/1646], Loss: 0.6393\n",
      "Epoch [2/5], Step [170/1646], Loss: 0.1348\n",
      "Epoch [2/5], Step [180/1646], Loss: 0.3437\n",
      "Epoch [2/5], Step [190/1646], Loss: 0.7829\n",
      "Epoch [2/5], Step [200/1646], Loss: 0.4254\n",
      "Epoch [2/5], Step [210/1646], Loss: 0.4778\n",
      "Epoch [2/5], Step [220/1646], Loss: 0.1363\n",
      "Epoch [2/5], Step [230/1646], Loss: 0.5137\n",
      "Epoch [2/5], Step [240/1646], Loss: 0.0234\n",
      "Epoch [2/5], Step [250/1646], Loss: 0.5786\n",
      "Epoch [2/5], Step [260/1646], Loss: 0.1531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [270/1646], Loss: 0.5551\n",
      "Epoch [2/5], Step [280/1646], Loss: 0.0989\n",
      "Epoch [2/5], Step [290/1646], Loss: 0.2406\n",
      "Epoch [2/5], Step [300/1646], Loss: 0.4813\n",
      "Epoch [2/5], Step [310/1646], Loss: 0.0515\n",
      "Epoch [2/5], Step [320/1646], Loss: 0.3101\n",
      "Epoch [2/5], Step [330/1646], Loss: 0.5970\n",
      "Epoch [2/5], Step [340/1646], Loss: 0.3329\n",
      "Epoch [2/5], Step [350/1646], Loss: 0.1439\n",
      "Epoch [2/5], Step [360/1646], Loss: 0.5210\n",
      "Epoch [2/5], Step [370/1646], Loss: 0.1432\n",
      "Epoch [2/5], Step [380/1646], Loss: 0.3005\n",
      "Epoch [2/5], Step [390/1646], Loss: 0.0949\n",
      "Epoch [2/5], Step [400/1646], Loss: 0.3097\n",
      "Epoch [2/5], Step [410/1646], Loss: 0.6737\n",
      "Epoch [2/5], Step [420/1646], Loss: 0.4232\n",
      "Epoch [2/5], Step [430/1646], Loss: 0.6555\n",
      "Epoch [2/5], Step [440/1646], Loss: 0.1793\n",
      "Epoch [2/5], Step [450/1646], Loss: 0.6443\n",
      "Epoch [2/5], Step [460/1646], Loss: 0.6361\n",
      "Epoch [2/5], Step [470/1646], Loss: 0.5162\n",
      "Epoch [2/5], Step [480/1646], Loss: 0.9541\n",
      "Epoch [2/5], Step [490/1646], Loss: 0.7669\n",
      "Epoch [2/5], Step [500/1646], Loss: 0.1062\n",
      "Epoch [2/5], Step [510/1646], Loss: 0.4121\n",
      "Epoch [2/5], Step [520/1646], Loss: 0.6446\n",
      "Epoch [2/5], Step [530/1646], Loss: 1.1132\n",
      "Epoch [2/5], Step [540/1646], Loss: 0.6337\n",
      "Epoch [2/5], Step [550/1646], Loss: 0.2165\n",
      "Epoch [2/5], Step [560/1646], Loss: 0.3819\n",
      "Epoch [2/5], Step [570/1646], Loss: 0.1122\n",
      "Epoch [2/5], Step [580/1646], Loss: 0.8397\n",
      "Epoch [2/5], Step [590/1646], Loss: 0.4763\n",
      "Epoch [2/5], Step [600/1646], Loss: 0.2881\n",
      "Epoch [2/5], Step [610/1646], Loss: 0.6001\n",
      "Epoch [2/5], Step [620/1646], Loss: 0.3914\n",
      "Epoch [2/5], Step [630/1646], Loss: 0.7569\n",
      "Epoch [2/5], Step [640/1646], Loss: 0.2398\n",
      "Epoch [2/5], Step [650/1646], Loss: 0.8230\n",
      "Epoch [2/5], Step [660/1646], Loss: 0.5450\n",
      "Epoch [2/5], Step [670/1646], Loss: 0.9985\n",
      "Epoch [2/5], Step [680/1646], Loss: 0.9358\n",
      "Epoch [2/5], Step [690/1646], Loss: 0.3155\n",
      "Epoch [2/5], Step [700/1646], Loss: 0.7414\n",
      "Epoch [2/5], Step [710/1646], Loss: 0.7955\n",
      "Epoch [2/5], Step [720/1646], Loss: 0.4428\n",
      "Epoch [2/5], Step [730/1646], Loss: 0.1744\n",
      "Epoch [2/5], Step [740/1646], Loss: 0.1638\n",
      "Epoch [2/5], Step [750/1646], Loss: 0.4496\n",
      "Epoch [2/5], Step [760/1646], Loss: 0.0791\n",
      "Epoch [2/5], Step [770/1646], Loss: 0.2262\n",
      "Epoch [2/5], Step [780/1646], Loss: 0.5320\n",
      "Epoch [2/5], Step [790/1646], Loss: 0.2105\n",
      "Epoch [2/5], Step [800/1646], Loss: 0.9997\n",
      "Epoch [2/5], Step [810/1646], Loss: 0.3198\n",
      "Epoch [2/5], Step [820/1646], Loss: 0.2377\n",
      "Epoch [2/5], Step [830/1646], Loss: 1.1491\n",
      "Epoch [2/5], Step [840/1646], Loss: 0.2425\n",
      "Epoch [2/5], Step [850/1646], Loss: 0.2880\n",
      "Epoch [2/5], Step [860/1646], Loss: 0.8994\n",
      "Epoch [2/5], Step [870/1646], Loss: 0.5329\n",
      "Epoch [2/5], Step [880/1646], Loss: 0.2613\n",
      "Epoch [2/5], Step [890/1646], Loss: 0.3645\n",
      "Epoch [2/5], Step [900/1646], Loss: 0.2731\n",
      "Epoch [2/5], Step [910/1646], Loss: 0.1924\n",
      "Epoch [2/5], Step [920/1646], Loss: 0.5125\n",
      "Epoch [2/5], Step [930/1646], Loss: 0.2495\n",
      "Epoch [2/5], Step [940/1646], Loss: 0.0610\n",
      "Epoch [2/5], Step [950/1646], Loss: 1.2400\n",
      "Epoch [2/5], Step [960/1646], Loss: 1.0062\n",
      "Epoch [2/5], Step [970/1646], Loss: 0.3570\n",
      "Epoch [2/5], Step [980/1646], Loss: 0.2077\n",
      "Epoch [2/5], Step [990/1646], Loss: 0.2009\n",
      "Epoch [2/5], Step [1000/1646], Loss: 0.1554\n",
      "Epoch [2/5], Step [1010/1646], Loss: 0.6578\n",
      "Epoch [2/5], Step [1020/1646], Loss: 0.5210\n",
      "Epoch [2/5], Step [1030/1646], Loss: 0.0793\n",
      "Epoch [2/5], Step [1040/1646], Loss: 0.2634\n",
      "Epoch [2/5], Step [1050/1646], Loss: 1.0110\n",
      "Epoch [2/5], Step [1060/1646], Loss: 0.3936\n",
      "Epoch [2/5], Step [1070/1646], Loss: 0.1664\n",
      "Epoch [2/5], Step [1080/1646], Loss: 0.1626\n",
      "Epoch [2/5], Step [1090/1646], Loss: 0.2724\n",
      "Epoch [2/5], Step [1100/1646], Loss: 0.5161\n",
      "Epoch [2/5], Step [1110/1646], Loss: 0.7484\n",
      "Epoch [2/5], Step [1120/1646], Loss: 0.4584\n",
      "Epoch [2/5], Step [1130/1646], Loss: 0.1770\n",
      "Epoch [2/5], Step [1140/1646], Loss: 0.2166\n",
      "Epoch [2/5], Step [1150/1646], Loss: 0.0995\n",
      "Epoch [2/5], Step [1160/1646], Loss: 0.1342\n",
      "Epoch [2/5], Step [1170/1646], Loss: 0.2039\n",
      "Epoch [2/5], Step [1180/1646], Loss: 0.3335\n",
      "Epoch [2/5], Step [1190/1646], Loss: 0.8453\n",
      "Epoch [2/5], Step [1200/1646], Loss: 0.5757\n",
      "Epoch [2/5], Step [1210/1646], Loss: 0.9276\n",
      "Epoch [2/5], Step [1220/1646], Loss: 0.0648\n",
      "Epoch [2/5], Step [1230/1646], Loss: 0.1104\n",
      "Epoch [2/5], Step [1240/1646], Loss: 1.4269\n",
      "Epoch [2/5], Step [1250/1646], Loss: 0.0364\n",
      "Epoch [2/5], Step [1260/1646], Loss: 0.2767\n",
      "Epoch [2/5], Step [1270/1646], Loss: 1.1309\n",
      "Epoch [2/5], Step [1280/1646], Loss: 0.0513\n",
      "Epoch [2/5], Step [1290/1646], Loss: 0.1560\n",
      "Epoch [2/5], Step [1300/1646], Loss: 0.6891\n",
      "Epoch [2/5], Step [1310/1646], Loss: 0.7227\n",
      "Epoch [2/5], Step [1320/1646], Loss: 0.1591\n",
      "Epoch [2/5], Step [1330/1646], Loss: 0.1748\n",
      "Epoch [2/5], Step [1340/1646], Loss: 1.2445\n",
      "Epoch [2/5], Step [1350/1646], Loss: 0.9638\n",
      "Epoch [2/5], Step [1360/1646], Loss: 0.4154\n",
      "Epoch [2/5], Step [1370/1646], Loss: 0.7639\n",
      "Epoch [2/5], Step [1380/1646], Loss: 1.0861\n",
      "Epoch [2/5], Step [1390/1646], Loss: 0.4209\n",
      "Epoch [2/5], Step [1400/1646], Loss: 0.1181\n",
      "Epoch [2/5], Step [1410/1646], Loss: 0.0761\n",
      "Epoch [2/5], Step [1420/1646], Loss: 0.8016\n",
      "Epoch [2/5], Step [1430/1646], Loss: 0.2114\n",
      "Epoch [2/5], Step [1440/1646], Loss: 0.3888\n",
      "Epoch [2/5], Step [1450/1646], Loss: 0.1050\n",
      "Epoch [2/5], Step [1460/1646], Loss: 0.8564\n",
      "Epoch [2/5], Step [1470/1646], Loss: 0.1179\n",
      "Epoch [2/5], Step [1480/1646], Loss: 0.1996\n",
      "Epoch [2/5], Step [1490/1646], Loss: 0.2996\n",
      "Epoch [2/5], Step [1500/1646], Loss: 0.2763\n",
      "Epoch [2/5], Step [1510/1646], Loss: 0.0149\n",
      "Epoch [2/5], Step [1520/1646], Loss: 1.3089\n",
      "Epoch [2/5], Step [1530/1646], Loss: 0.2035\n",
      "Epoch [2/5], Step [1540/1646], Loss: 0.0394\n",
      "Epoch [2/5], Step [1550/1646], Loss: 0.6843\n",
      "Epoch [2/5], Step [1560/1646], Loss: 0.4813\n",
      "Epoch [2/5], Step [1570/1646], Loss: 0.3190\n",
      "Epoch [2/5], Step [1580/1646], Loss: 0.3433\n",
      "Epoch [2/5], Step [1590/1646], Loss: 0.3716\n",
      "Epoch [2/5], Step [1600/1646], Loss: 0.6770\n",
      "Epoch [2/5], Step [1610/1646], Loss: 0.3871\n",
      "Epoch [2/5], Step [1620/1646], Loss: 0.3546\n",
      "Epoch [2/5], Step [1630/1646], Loss: 1.0058\n",
      "Epoch [2/5], Step [1640/1646], Loss: 0.8441\n",
      "Epoch [3/5], Step [10/1646], Loss: 0.3305\n",
      "Epoch [3/5], Step [20/1646], Loss: 0.2763\n",
      "Epoch [3/5], Step [30/1646], Loss: 0.0882\n",
      "Epoch [3/5], Step [40/1646], Loss: 0.2982\n",
      "Epoch [3/5], Step [50/1646], Loss: 0.8492\n",
      "Epoch [3/5], Step [60/1646], Loss: 0.1942\n",
      "Epoch [3/5], Step [70/1646], Loss: 0.0709\n",
      "Epoch [3/5], Step [80/1646], Loss: 1.2384\n",
      "Epoch [3/5], Step [90/1646], Loss: 0.4589\n",
      "Epoch [3/5], Step [100/1646], Loss: 0.0298\n",
      "Epoch [3/5], Step [110/1646], Loss: 0.1445\n",
      "Epoch [3/5], Step [120/1646], Loss: 0.5085\n",
      "Epoch [3/5], Step [130/1646], Loss: 0.0826\n",
      "Epoch [3/5], Step [140/1646], Loss: 0.3659\n",
      "Epoch [3/5], Step [150/1646], Loss: 0.2027\n",
      "Epoch [3/5], Step [160/1646], Loss: 0.3608\n",
      "Epoch [3/5], Step [170/1646], Loss: 0.2218\n",
      "Epoch [3/5], Step [180/1646], Loss: 0.1769\n",
      "Epoch [3/5], Step [190/1646], Loss: 0.0971\n",
      "Epoch [3/5], Step [200/1646], Loss: 0.3135\n",
      "Epoch [3/5], Step [210/1646], Loss: 0.1576\n",
      "Epoch [3/5], Step [220/1646], Loss: 0.1651\n",
      "Epoch [3/5], Step [230/1646], Loss: 1.3142\n",
      "Epoch [3/5], Step [240/1646], Loss: 0.4117\n",
      "Epoch [3/5], Step [250/1646], Loss: 0.6891\n",
      "Epoch [3/5], Step [260/1646], Loss: 0.3470\n",
      "Epoch [3/5], Step [270/1646], Loss: 0.3044\n",
      "Epoch [3/5], Step [280/1646], Loss: 0.0577\n",
      "Epoch [3/5], Step [290/1646], Loss: 1.1601\n",
      "Epoch [3/5], Step [300/1646], Loss: 0.0611\n",
      "Epoch [3/5], Step [310/1646], Loss: 0.4946\n",
      "Epoch [3/5], Step [320/1646], Loss: 0.4211\n",
      "Epoch [3/5], Step [330/1646], Loss: 0.5617\n",
      "Epoch [3/5], Step [340/1646], Loss: 0.4682\n",
      "Epoch [3/5], Step [350/1646], Loss: 0.3562\n",
      "Epoch [3/5], Step [360/1646], Loss: 0.2712\n",
      "Epoch [3/5], Step [370/1646], Loss: 0.0417\n",
      "Epoch [3/5], Step [380/1646], Loss: 0.1671\n",
      "Epoch [3/5], Step [390/1646], Loss: 0.3347\n",
      "Epoch [3/5], Step [400/1646], Loss: 0.5946\n",
      "Epoch [3/5], Step [410/1646], Loss: 0.3398\n",
      "Epoch [3/5], Step [420/1646], Loss: 0.3782\n",
      "Epoch [3/5], Step [430/1646], Loss: 0.3925\n",
      "Epoch [3/5], Step [440/1646], Loss: 0.4723\n",
      "Epoch [3/5], Step [450/1646], Loss: 0.1407\n",
      "Epoch [3/5], Step [460/1646], Loss: 0.0741\n",
      "Epoch [3/5], Step [470/1646], Loss: 0.0989\n",
      "Epoch [3/5], Step [480/1646], Loss: 0.5323\n",
      "Epoch [3/5], Step [490/1646], Loss: 0.3812\n",
      "Epoch [3/5], Step [500/1646], Loss: 0.2444\n",
      "Epoch [3/5], Step [510/1646], Loss: 0.0383\n",
      "Epoch [3/5], Step [520/1646], Loss: 0.1872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [530/1646], Loss: 0.1784\n",
      "Epoch [3/5], Step [540/1646], Loss: 0.3352\n",
      "Epoch [3/5], Step [550/1646], Loss: 0.2995\n",
      "Epoch [3/5], Step [560/1646], Loss: 0.9170\n",
      "Epoch [3/5], Step [570/1646], Loss: 0.4536\n",
      "Epoch [3/5], Step [580/1646], Loss: 0.4958\n",
      "Epoch [3/5], Step [590/1646], Loss: 0.3420\n",
      "Epoch [3/5], Step [600/1646], Loss: 0.3269\n",
      "Epoch [3/5], Step [610/1646], Loss: 0.1751\n",
      "Epoch [3/5], Step [620/1646], Loss: 0.4253\n",
      "Epoch [3/5], Step [630/1646], Loss: 0.2990\n",
      "Epoch [3/5], Step [640/1646], Loss: 1.1210\n",
      "Epoch [3/5], Step [650/1646], Loss: 0.0202\n",
      "Epoch [3/5], Step [660/1646], Loss: 0.3370\n",
      "Epoch [3/5], Step [670/1646], Loss: 0.9314\n",
      "Epoch [3/5], Step [680/1646], Loss: 0.3392\n",
      "Epoch [3/5], Step [690/1646], Loss: 0.2925\n",
      "Epoch [3/5], Step [700/1646], Loss: 0.3817\n",
      "Epoch [3/5], Step [710/1646], Loss: 0.3176\n",
      "Epoch [3/5], Step [720/1646], Loss: 0.3885\n",
      "Epoch [3/5], Step [730/1646], Loss: 0.0829\n",
      "Epoch [3/5], Step [740/1646], Loss: 0.1932\n",
      "Epoch [3/5], Step [750/1646], Loss: 0.1430\n",
      "Epoch [3/5], Step [760/1646], Loss: 0.0958\n",
      "Epoch [3/5], Step [770/1646], Loss: 0.4304\n",
      "Epoch [3/5], Step [780/1646], Loss: 0.3404\n",
      "Epoch [3/5], Step [790/1646], Loss: 0.0698\n",
      "Epoch [3/5], Step [800/1646], Loss: 0.8392\n",
      "Epoch [3/5], Step [810/1646], Loss: 0.0783\n",
      "Epoch [3/5], Step [820/1646], Loss: 0.4732\n",
      "Epoch [3/5], Step [830/1646], Loss: 0.0990\n",
      "Epoch [3/5], Step [840/1646], Loss: 0.6048\n",
      "Epoch [3/5], Step [850/1646], Loss: 0.2347\n",
      "Epoch [3/5], Step [860/1646], Loss: 0.0866\n",
      "Epoch [3/5], Step [870/1646], Loss: 0.0764\n",
      "Epoch [3/5], Step [880/1646], Loss: 0.3242\n",
      "Epoch [3/5], Step [890/1646], Loss: 0.5588\n",
      "Epoch [3/5], Step [900/1646], Loss: 0.6271\n",
      "Epoch [3/5], Step [910/1646], Loss: 0.2157\n",
      "Epoch [3/5], Step [920/1646], Loss: 0.0990\n",
      "Epoch [3/5], Step [930/1646], Loss: 0.2022\n",
      "Epoch [3/5], Step [940/1646], Loss: 1.0381\n",
      "Epoch [3/5], Step [950/1646], Loss: 0.1541\n",
      "Epoch [3/5], Step [960/1646], Loss: 0.0368\n",
      "Epoch [3/5], Step [970/1646], Loss: 0.1387\n",
      "Epoch [3/5], Step [980/1646], Loss: 0.1025\n",
      "Epoch [3/5], Step [990/1646], Loss: 0.3955\n",
      "Epoch [3/5], Step [1000/1646], Loss: 0.5084\n",
      "Epoch [3/5], Step [1010/1646], Loss: 0.2375\n",
      "Epoch [3/5], Step [1020/1646], Loss: 0.3480\n",
      "Epoch [3/5], Step [1030/1646], Loss: 0.3722\n",
      "Epoch [3/5], Step [1040/1646], Loss: 0.4576\n",
      "Epoch [3/5], Step [1050/1646], Loss: 0.1436\n",
      "Epoch [3/5], Step [1060/1646], Loss: 0.2219\n",
      "Epoch [3/5], Step [1070/1646], Loss: 0.0987\n",
      "Epoch [3/5], Step [1080/1646], Loss: 0.5499\n",
      "Epoch [3/5], Step [1090/1646], Loss: 0.2020\n",
      "Epoch [3/5], Step [1100/1646], Loss: 0.9280\n",
      "Epoch [3/5], Step [1110/1646], Loss: 0.1578\n",
      "Epoch [3/5], Step [1120/1646], Loss: 0.2252\n",
      "Epoch [3/5], Step [1130/1646], Loss: 0.1796\n",
      "Epoch [3/5], Step [1140/1646], Loss: 0.1379\n",
      "Epoch [3/5], Step [1150/1646], Loss: 0.2789\n",
      "Epoch [3/5], Step [1160/1646], Loss: 0.0791\n",
      "Epoch [3/5], Step [1170/1646], Loss: 0.0516\n",
      "Epoch [3/5], Step [1180/1646], Loss: 0.1321\n",
      "Epoch [3/5], Step [1190/1646], Loss: 1.3995\n",
      "Epoch [3/5], Step [1200/1646], Loss: 0.4116\n",
      "Epoch [3/5], Step [1210/1646], Loss: 0.3851\n",
      "Epoch [3/5], Step [1220/1646], Loss: 0.7031\n",
      "Epoch [3/5], Step [1230/1646], Loss: 0.7006\n",
      "Epoch [3/5], Step [1240/1646], Loss: 0.2492\n",
      "Epoch [3/5], Step [1250/1646], Loss: 0.4269\n",
      "Epoch [3/5], Step [1260/1646], Loss: 0.2582\n",
      "Epoch [3/5], Step [1270/1646], Loss: 0.4252\n",
      "Epoch [3/5], Step [1280/1646], Loss: 0.3704\n",
      "Epoch [3/5], Step [1290/1646], Loss: 0.0966\n",
      "Epoch [3/5], Step [1300/1646], Loss: 0.8126\n",
      "Epoch [3/5], Step [1310/1646], Loss: 0.0484\n",
      "Epoch [3/5], Step [1320/1646], Loss: 0.4528\n",
      "Epoch [3/5], Step [1330/1646], Loss: 0.5344\n",
      "Epoch [3/5], Step [1340/1646], Loss: 0.6770\n",
      "Epoch [3/5], Step [1350/1646], Loss: 0.2836\n",
      "Epoch [3/5], Step [1360/1646], Loss: 0.1440\n",
      "Epoch [3/5], Step [1370/1646], Loss: 0.1785\n",
      "Epoch [3/5], Step [1380/1646], Loss: 0.0927\n",
      "Epoch [3/5], Step [1390/1646], Loss: 0.1696\n",
      "Epoch [3/5], Step [1400/1646], Loss: 0.3183\n",
      "Epoch [3/5], Step [1410/1646], Loss: 0.8453\n",
      "Epoch [3/5], Step [1420/1646], Loss: 0.2831\n",
      "Epoch [3/5], Step [1430/1646], Loss: 0.1868\n",
      "Epoch [3/5], Step [1440/1646], Loss: 0.2418\n",
      "Epoch [3/5], Step [1450/1646], Loss: 0.5447\n",
      "Epoch [3/5], Step [1460/1646], Loss: 0.3066\n",
      "Epoch [3/5], Step [1470/1646], Loss: 0.5387\n",
      "Epoch [3/5], Step [1480/1646], Loss: 0.9962\n",
      "Epoch [3/5], Step [1490/1646], Loss: 0.0706\n",
      "Epoch [3/5], Step [1500/1646], Loss: 0.0886\n",
      "Epoch [3/5], Step [1510/1646], Loss: 0.1858\n",
      "Epoch [3/5], Step [1520/1646], Loss: 0.7820\n",
      "Epoch [3/5], Step [1530/1646], Loss: 0.1933\n",
      "Epoch [3/5], Step [1540/1646], Loss: 0.0834\n",
      "Epoch [3/5], Step [1550/1646], Loss: 0.3312\n",
      "Epoch [3/5], Step [1560/1646], Loss: 0.3955\n",
      "Epoch [3/5], Step [1570/1646], Loss: 1.2759\n",
      "Epoch [3/5], Step [1580/1646], Loss: 0.1203\n",
      "Epoch [3/5], Step [1590/1646], Loss: 0.0900\n",
      "Epoch [3/5], Step [1600/1646], Loss: 0.0740\n",
      "Epoch [3/5], Step [1610/1646], Loss: 0.2525\n",
      "Epoch [3/5], Step [1620/1646], Loss: 0.5669\n",
      "Epoch [3/5], Step [1630/1646], Loss: 0.2538\n",
      "Epoch [3/5], Step [1640/1646], Loss: 0.0676\n",
      "Epoch [4/5], Step [10/1646], Loss: 0.1126\n",
      "Epoch [4/5], Step [20/1646], Loss: 0.1397\n",
      "Epoch [4/5], Step [30/1646], Loss: 0.1323\n",
      "Epoch [4/5], Step [40/1646], Loss: 0.2666\n",
      "Epoch [4/5], Step [50/1646], Loss: 0.3369\n",
      "Epoch [4/5], Step [60/1646], Loss: 0.1562\n",
      "Epoch [4/5], Step [70/1646], Loss: 0.1205\n",
      "Epoch [4/5], Step [80/1646], Loss: 0.2309\n",
      "Epoch [4/5], Step [90/1646], Loss: 0.4374\n",
      "Epoch [4/5], Step [100/1646], Loss: 0.1664\n",
      "Epoch [4/5], Step [110/1646], Loss: 0.1162\n",
      "Epoch [4/5], Step [120/1646], Loss: 0.4646\n",
      "Epoch [4/5], Step [130/1646], Loss: 0.0502\n",
      "Epoch [4/5], Step [140/1646], Loss: 0.2055\n",
      "Epoch [4/5], Step [150/1646], Loss: 0.4249\n",
      "Epoch [4/5], Step [160/1646], Loss: 0.4159\n",
      "Epoch [4/5], Step [170/1646], Loss: 0.2950\n",
      "Epoch [4/5], Step [180/1646], Loss: 0.2220\n",
      "Epoch [4/5], Step [190/1646], Loss: 0.1514\n",
      "Epoch [4/5], Step [200/1646], Loss: 0.1079\n",
      "Epoch [4/5], Step [210/1646], Loss: 0.2977\n",
      "Epoch [4/5], Step [220/1646], Loss: 0.1214\n",
      "Epoch [4/5], Step [230/1646], Loss: 0.0695\n",
      "Epoch [4/5], Step [240/1646], Loss: 0.3400\n",
      "Epoch [4/5], Step [250/1646], Loss: 0.0216\n",
      "Epoch [4/5], Step [260/1646], Loss: 0.0815\n",
      "Epoch [4/5], Step [270/1646], Loss: 0.1018\n",
      "Epoch [4/5], Step [280/1646], Loss: 0.1042\n",
      "Epoch [4/5], Step [290/1646], Loss: 0.2055\n",
      "Epoch [4/5], Step [300/1646], Loss: 0.0366\n",
      "Epoch [4/5], Step [310/1646], Loss: 0.2308\n",
      "Epoch [4/5], Step [320/1646], Loss: 0.3169\n",
      "Epoch [4/5], Step [330/1646], Loss: 0.0112\n",
      "Epoch [4/5], Step [340/1646], Loss: 0.2567\n",
      "Epoch [4/5], Step [350/1646], Loss: 0.0241\n",
      "Epoch [4/5], Step [360/1646], Loss: 0.0759\n",
      "Epoch [4/5], Step [370/1646], Loss: 0.1732\n",
      "Epoch [4/5], Step [380/1646], Loss: 0.4352\n",
      "Epoch [4/5], Step [390/1646], Loss: 0.9716\n",
      "Epoch [4/5], Step [400/1646], Loss: 0.1672\n",
      "Epoch [4/5], Step [410/1646], Loss: 0.4606\n",
      "Epoch [4/5], Step [420/1646], Loss: 0.0995\n",
      "Epoch [4/5], Step [430/1646], Loss: 0.0103\n",
      "Epoch [4/5], Step [440/1646], Loss: 0.0421\n",
      "Epoch [4/5], Step [450/1646], Loss: 0.1589\n",
      "Epoch [4/5], Step [460/1646], Loss: 0.0984\n",
      "Epoch [4/5], Step [470/1646], Loss: 0.1683\n",
      "Epoch [4/5], Step [480/1646], Loss: 0.0768\n",
      "Epoch [4/5], Step [490/1646], Loss: 0.1181\n",
      "Epoch [4/5], Step [500/1646], Loss: 0.1864\n",
      "Epoch [4/5], Step [510/1646], Loss: 0.3583\n",
      "Epoch [4/5], Step [520/1646], Loss: 0.1954\n",
      "Epoch [4/5], Step [530/1646], Loss: 0.0586\n",
      "Epoch [4/5], Step [540/1646], Loss: 0.5723\n",
      "Epoch [4/5], Step [550/1646], Loss: 0.1858\n",
      "Epoch [4/5], Step [560/1646], Loss: 0.3313\n",
      "Epoch [4/5], Step [570/1646], Loss: 0.1590\n",
      "Epoch [4/5], Step [580/1646], Loss: 0.1724\n",
      "Epoch [4/5], Step [590/1646], Loss: 0.9633\n",
      "Epoch [4/5], Step [600/1646], Loss: 0.0444\n",
      "Epoch [4/5], Step [610/1646], Loss: 0.0898\n",
      "Epoch [4/5], Step [620/1646], Loss: 0.8247\n",
      "Epoch [4/5], Step [630/1646], Loss: 0.5024\n",
      "Epoch [4/5], Step [640/1646], Loss: 0.3849\n",
      "Epoch [4/5], Step [650/1646], Loss: 0.2706\n",
      "Epoch [4/5], Step [660/1646], Loss: 0.0738\n",
      "Epoch [4/5], Step [670/1646], Loss: 0.0556\n",
      "Epoch [4/5], Step [680/1646], Loss: 0.1422\n",
      "Epoch [4/5], Step [690/1646], Loss: 0.1020\n",
      "Epoch [4/5], Step [700/1646], Loss: 0.3247\n",
      "Epoch [4/5], Step [710/1646], Loss: 0.2690\n",
      "Epoch [4/5], Step [720/1646], Loss: 0.0088\n",
      "Epoch [4/5], Step [730/1646], Loss: 0.0617\n",
      "Epoch [4/5], Step [740/1646], Loss: 0.4943\n",
      "Epoch [4/5], Step [750/1646], Loss: 0.2338\n",
      "Epoch [4/5], Step [760/1646], Loss: 0.5315\n",
      "Epoch [4/5], Step [770/1646], Loss: 0.3550\n",
      "Epoch [4/5], Step [780/1646], Loss: 0.7872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [790/1646], Loss: 0.5168\n",
      "Epoch [4/5], Step [800/1646], Loss: 0.3201\n",
      "Epoch [4/5], Step [810/1646], Loss: 0.0411\n",
      "Epoch [4/5], Step [820/1646], Loss: 0.0164\n",
      "Epoch [4/5], Step [830/1646], Loss: 0.3245\n",
      "Epoch [4/5], Step [840/1646], Loss: 0.3043\n",
      "Epoch [4/5], Step [850/1646], Loss: 0.0829\n",
      "Epoch [4/5], Step [860/1646], Loss: 0.3927\n",
      "Epoch [4/5], Step [870/1646], Loss: 0.9157\n",
      "Epoch [4/5], Step [880/1646], Loss: 0.3303\n",
      "Epoch [4/5], Step [890/1646], Loss: 0.0751\n",
      "Epoch [4/5], Step [900/1646], Loss: 0.0534\n",
      "Epoch [4/5], Step [910/1646], Loss: 0.0189\n",
      "Epoch [4/5], Step [920/1646], Loss: 0.1399\n",
      "Epoch [4/5], Step [930/1646], Loss: 0.0706\n",
      "Epoch [4/5], Step [940/1646], Loss: 0.8518\n",
      "Epoch [4/5], Step [950/1646], Loss: 0.2910\n",
      "Epoch [4/5], Step [960/1646], Loss: 0.2709\n",
      "Epoch [4/5], Step [970/1646], Loss: 0.2780\n",
      "Epoch [4/5], Step [980/1646], Loss: 0.0467\n",
      "Epoch [4/5], Step [990/1646], Loss: 0.0092\n",
      "Epoch [4/5], Step [1000/1646], Loss: 0.1439\n",
      "Epoch [4/5], Step [1010/1646], Loss: 0.0360\n",
      "Epoch [4/5], Step [1020/1646], Loss: 0.1854\n",
      "Epoch [4/5], Step [1030/1646], Loss: 0.1756\n",
      "Epoch [4/5], Step [1040/1646], Loss: 0.4786\n",
      "Epoch [4/5], Step [1050/1646], Loss: 0.6533\n",
      "Epoch [4/5], Step [1060/1646], Loss: 0.4393\n",
      "Epoch [4/5], Step [1070/1646], Loss: 0.3673\n",
      "Epoch [4/5], Step [1080/1646], Loss: 0.1983\n",
      "Epoch [4/5], Step [1090/1646], Loss: 0.3326\n",
      "Epoch [4/5], Step [1100/1646], Loss: 0.7168\n",
      "Epoch [4/5], Step [1110/1646], Loss: 0.1270\n",
      "Epoch [4/5], Step [1120/1646], Loss: 0.3421\n",
      "Epoch [4/5], Step [1130/1646], Loss: 0.0765\n",
      "Epoch [4/5], Step [1140/1646], Loss: 0.1153\n",
      "Epoch [4/5], Step [1150/1646], Loss: 0.1766\n",
      "Epoch [4/5], Step [1160/1646], Loss: 0.0483\n",
      "Epoch [4/5], Step [1170/1646], Loss: 0.3220\n",
      "Epoch [4/5], Step [1180/1646], Loss: 0.1246\n",
      "Epoch [4/5], Step [1190/1646], Loss: 0.0938\n",
      "Epoch [4/5], Step [1200/1646], Loss: 0.4219\n",
      "Epoch [4/5], Step [1210/1646], Loss: 0.3849\n",
      "Epoch [4/5], Step [1220/1646], Loss: 1.3517\n",
      "Epoch [4/5], Step [1230/1646], Loss: 0.4114\n",
      "Epoch [4/5], Step [1240/1646], Loss: 0.0923\n",
      "Epoch [4/5], Step [1250/1646], Loss: 0.1364\n",
      "Epoch [4/5], Step [1260/1646], Loss: 0.5222\n",
      "Epoch [4/5], Step [1270/1646], Loss: 0.3279\n",
      "Epoch [4/5], Step [1280/1646], Loss: 0.4374\n",
      "Epoch [4/5], Step [1290/1646], Loss: 0.2864\n",
      "Epoch [4/5], Step [1300/1646], Loss: 0.5784\n",
      "Epoch [4/5], Step [1310/1646], Loss: 0.1410\n",
      "Epoch [4/5], Step [1320/1646], Loss: 0.6114\n",
      "Epoch [4/5], Step [1330/1646], Loss: 0.0716\n",
      "Epoch [4/5], Step [1340/1646], Loss: 0.1269\n",
      "Epoch [4/5], Step [1350/1646], Loss: 0.0839\n",
      "Epoch [4/5], Step [1360/1646], Loss: 0.3497\n",
      "Epoch [4/5], Step [1370/1646], Loss: 0.1553\n",
      "Epoch [4/5], Step [1380/1646], Loss: 0.2458\n",
      "Epoch [4/5], Step [1390/1646], Loss: 0.2146\n",
      "Epoch [4/5], Step [1400/1646], Loss: 0.1250\n",
      "Epoch [4/5], Step [1410/1646], Loss: 0.0886\n",
      "Epoch [4/5], Step [1420/1646], Loss: 0.3271\n",
      "Epoch [4/5], Step [1430/1646], Loss: 0.4659\n",
      "Epoch [4/5], Step [1440/1646], Loss: 0.2826\n",
      "Epoch [4/5], Step [1450/1646], Loss: 0.2511\n",
      "Epoch [4/5], Step [1460/1646], Loss: 0.1394\n",
      "Epoch [4/5], Step [1470/1646], Loss: 0.0593\n",
      "Epoch [4/5], Step [1480/1646], Loss: 0.0254\n",
      "Epoch [4/5], Step [1490/1646], Loss: 0.1021\n",
      "Epoch [4/5], Step [1500/1646], Loss: 1.4676\n",
      "Epoch [4/5], Step [1510/1646], Loss: 0.2751\n",
      "Epoch [4/5], Step [1520/1646], Loss: 0.1534\n",
      "Epoch [4/5], Step [1530/1646], Loss: 0.0306\n",
      "Epoch [4/5], Step [1540/1646], Loss: 0.0153\n",
      "Epoch [4/5], Step [1550/1646], Loss: 0.3009\n",
      "Epoch [4/5], Step [1560/1646], Loss: 0.0408\n",
      "Epoch [4/5], Step [1570/1646], Loss: 0.3068\n",
      "Epoch [4/5], Step [1580/1646], Loss: 0.1528\n",
      "Epoch [4/5], Step [1590/1646], Loss: 0.0813\n",
      "Epoch [4/5], Step [1600/1646], Loss: 0.2426\n",
      "Epoch [4/5], Step [1610/1646], Loss: 0.1610\n",
      "Epoch [4/5], Step [1620/1646], Loss: 0.1328\n",
      "Epoch [4/5], Step [1630/1646], Loss: 0.0321\n",
      "Epoch [4/5], Step [1640/1646], Loss: 0.1764\n",
      "Epoch [5/5], Step [10/1646], Loss: 0.1935\n",
      "Epoch [5/5], Step [20/1646], Loss: 0.1765\n",
      "Epoch [5/5], Step [30/1646], Loss: 0.5180\n",
      "Epoch [5/5], Step [40/1646], Loss: 0.3211\n",
      "Epoch [5/5], Step [50/1646], Loss: 0.2070\n",
      "Epoch [5/5], Step [60/1646], Loss: 0.1531\n",
      "Epoch [5/5], Step [70/1646], Loss: 0.8157\n",
      "Epoch [5/5], Step [80/1646], Loss: 0.1173\n",
      "Epoch [5/5], Step [90/1646], Loss: 0.0272\n",
      "Epoch [5/5], Step [100/1646], Loss: 0.0514\n",
      "Epoch [5/5], Step [110/1646], Loss: 0.4232\n",
      "Epoch [5/5], Step [120/1646], Loss: 0.7923\n",
      "Epoch [5/5], Step [130/1646], Loss: 0.5629\n",
      "Epoch [5/5], Step [140/1646], Loss: 0.0271\n",
      "Epoch [5/5], Step [150/1646], Loss: 0.1013\n",
      "Epoch [5/5], Step [160/1646], Loss: 0.0339\n",
      "Epoch [5/5], Step [170/1646], Loss: 0.0137\n",
      "Epoch [5/5], Step [180/1646], Loss: 0.0171\n",
      "Epoch [5/5], Step [190/1646], Loss: 0.2483\n",
      "Epoch [5/5], Step [200/1646], Loss: 0.1460\n",
      "Epoch [5/5], Step [210/1646], Loss: 0.5299\n",
      "Epoch [5/5], Step [220/1646], Loss: 0.0794\n",
      "Epoch [5/5], Step [230/1646], Loss: 0.4059\n",
      "Epoch [5/5], Step [240/1646], Loss: 0.0566\n",
      "Epoch [5/5], Step [250/1646], Loss: 0.3243\n",
      "Epoch [5/5], Step [260/1646], Loss: 0.0823\n",
      "Epoch [5/5], Step [270/1646], Loss: 0.2962\n",
      "Epoch [5/5], Step [280/1646], Loss: 0.1781\n",
      "Epoch [5/5], Step [290/1646], Loss: 0.3019\n",
      "Epoch [5/5], Step [300/1646], Loss: 0.0838\n",
      "Epoch [5/5], Step [310/1646], Loss: 0.0101\n",
      "Epoch [5/5], Step [320/1646], Loss: 0.0610\n",
      "Epoch [5/5], Step [330/1646], Loss: 0.1267\n",
      "Epoch [5/5], Step [340/1646], Loss: 0.0126\n",
      "Epoch [5/5], Step [350/1646], Loss: 0.1269\n",
      "Epoch [5/5], Step [360/1646], Loss: 0.0334\n",
      "Epoch [5/5], Step [370/1646], Loss: 0.2407\n",
      "Epoch [5/5], Step [380/1646], Loss: 0.1023\n",
      "Epoch [5/5], Step [390/1646], Loss: 0.0504\n",
      "Epoch [5/5], Step [400/1646], Loss: 0.1210\n",
      "Epoch [5/5], Step [410/1646], Loss: 0.0139\n",
      "Epoch [5/5], Step [420/1646], Loss: 0.4607\n",
      "Epoch [5/5], Step [430/1646], Loss: 0.7711\n",
      "Epoch [5/5], Step [440/1646], Loss: 0.4298\n",
      "Epoch [5/5], Step [450/1646], Loss: 0.2311\n",
      "Epoch [5/5], Step [460/1646], Loss: 0.7775\n",
      "Epoch [5/5], Step [470/1646], Loss: 0.2183\n",
      "Epoch [5/5], Step [480/1646], Loss: 0.1438\n",
      "Epoch [5/5], Step [490/1646], Loss: 0.4432\n",
      "Epoch [5/5], Step [500/1646], Loss: 0.3230\n",
      "Epoch [5/5], Step [510/1646], Loss: 0.0920\n",
      "Epoch [5/5], Step [520/1646], Loss: 0.0352\n",
      "Epoch [5/5], Step [530/1646], Loss: 1.0808\n",
      "Epoch [5/5], Step [540/1646], Loss: 0.1109\n",
      "Epoch [5/5], Step [550/1646], Loss: 0.1430\n",
      "Epoch [5/5], Step [560/1646], Loss: 0.0202\n",
      "Epoch [5/5], Step [570/1646], Loss: 0.1159\n",
      "Epoch [5/5], Step [580/1646], Loss: 0.0706\n",
      "Epoch [5/5], Step [590/1646], Loss: 0.1824\n",
      "Epoch [5/5], Step [600/1646], Loss: 0.0989\n",
      "Epoch [5/5], Step [610/1646], Loss: 0.0498\n",
      "Epoch [5/5], Step [620/1646], Loss: 0.0656\n",
      "Epoch [5/5], Step [630/1646], Loss: 0.0428\n",
      "Epoch [5/5], Step [640/1646], Loss: 0.0852\n",
      "Epoch [5/5], Step [650/1646], Loss: 0.1284\n",
      "Epoch [5/5], Step [660/1646], Loss: 0.0781\n",
      "Epoch [5/5], Step [670/1646], Loss: 0.2620\n",
      "Epoch [5/5], Step [680/1646], Loss: 0.4172\n",
      "Epoch [5/5], Step [690/1646], Loss: 0.1362\n",
      "Epoch [5/5], Step [700/1646], Loss: 0.0307\n",
      "Epoch [5/5], Step [710/1646], Loss: 0.3184\n",
      "Epoch [5/5], Step [720/1646], Loss: 0.1570\n",
      "Epoch [5/5], Step [730/1646], Loss: 0.1776\n",
      "Epoch [5/5], Step [740/1646], Loss: 0.0392\n",
      "Epoch [5/5], Step [750/1646], Loss: 0.2877\n",
      "Epoch [5/5], Step [760/1646], Loss: 0.1100\n",
      "Epoch [5/5], Step [770/1646], Loss: 0.5879\n",
      "Epoch [5/5], Step [780/1646], Loss: 0.1799\n",
      "Epoch [5/5], Step [790/1646], Loss: 0.5572\n",
      "Epoch [5/5], Step [800/1646], Loss: 0.2886\n",
      "Epoch [5/5], Step [810/1646], Loss: 0.0597\n",
      "Epoch [5/5], Step [820/1646], Loss: 0.0391\n",
      "Epoch [5/5], Step [830/1646], Loss: 0.3151\n",
      "Epoch [5/5], Step [840/1646], Loss: 0.0497\n",
      "Epoch [5/5], Step [850/1646], Loss: 0.2591\n",
      "Epoch [5/5], Step [860/1646], Loss: 0.0896\n",
      "Epoch [5/5], Step [870/1646], Loss: 0.0162\n",
      "Epoch [5/5], Step [880/1646], Loss: 0.4261\n",
      "Epoch [5/5], Step [890/1646], Loss: 0.1051\n",
      "Epoch [5/5], Step [900/1646], Loss: 0.4380\n",
      "Epoch [5/5], Step [910/1646], Loss: 0.1247\n",
      "Epoch [5/5], Step [920/1646], Loss: 0.4892\n",
      "Epoch [5/5], Step [930/1646], Loss: 0.5028\n",
      "Epoch [5/5], Step [940/1646], Loss: 0.3351\n",
      "Epoch [5/5], Step [950/1646], Loss: 1.1421\n",
      "Epoch [5/5], Step [960/1646], Loss: 0.1560\n",
      "Epoch [5/5], Step [970/1646], Loss: 0.0685\n",
      "Epoch [5/5], Step [980/1646], Loss: 0.0497\n",
      "Epoch [5/5], Step [990/1646], Loss: 0.2980\n",
      "Epoch [5/5], Step [1000/1646], Loss: 0.1100\n",
      "Epoch [5/5], Step [1010/1646], Loss: 0.4765\n",
      "Epoch [5/5], Step [1020/1646], Loss: 0.0452\n",
      "Epoch [5/5], Step [1030/1646], Loss: 0.0740\n",
      "Epoch [5/5], Step [1040/1646], Loss: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [1050/1646], Loss: 0.2525\n",
      "Epoch [5/5], Step [1060/1646], Loss: 0.0960\n",
      "Epoch [5/5], Step [1070/1646], Loss: 0.6714\n",
      "Epoch [5/5], Step [1080/1646], Loss: 0.9826\n",
      "Epoch [5/5], Step [1090/1646], Loss: 0.0173\n",
      "Epoch [5/5], Step [1100/1646], Loss: 0.1394\n",
      "Epoch [5/5], Step [1110/1646], Loss: 0.4217\n",
      "Epoch [5/5], Step [1120/1646], Loss: 0.3618\n",
      "Epoch [5/5], Step [1130/1646], Loss: 0.3011\n",
      "Epoch [5/5], Step [1140/1646], Loss: 0.1020\n",
      "Epoch [5/5], Step [1150/1646], Loss: 0.2073\n",
      "Epoch [5/5], Step [1160/1646], Loss: 0.0297\n",
      "Epoch [5/5], Step [1170/1646], Loss: 0.0425\n",
      "Epoch [5/5], Step [1180/1646], Loss: 0.1689\n",
      "Epoch [5/5], Step [1190/1646], Loss: 0.6390\n",
      "Epoch [5/5], Step [1200/1646], Loss: 0.3350\n",
      "Epoch [5/5], Step [1210/1646], Loss: 0.0492\n",
      "Epoch [5/5], Step [1220/1646], Loss: 0.0235\n",
      "Epoch [5/5], Step [1230/1646], Loss: 0.0228\n",
      "Epoch [5/5], Step [1240/1646], Loss: 0.3815\n",
      "Epoch [5/5], Step [1250/1646], Loss: 0.3369\n",
      "Epoch [5/5], Step [1260/1646], Loss: 0.1084\n",
      "Epoch [5/5], Step [1270/1646], Loss: 0.2815\n",
      "Epoch [5/5], Step [1280/1646], Loss: 0.1039\n",
      "Epoch [5/5], Step [1290/1646], Loss: 0.3032\n",
      "Epoch [5/5], Step [1300/1646], Loss: 0.0446\n",
      "Epoch [5/5], Step [1310/1646], Loss: 0.0665\n",
      "Epoch [5/5], Step [1320/1646], Loss: 0.0743\n",
      "Epoch [5/5], Step [1330/1646], Loss: 0.1151\n",
      "Epoch [5/5], Step [1340/1646], Loss: 0.2794\n",
      "Epoch [5/5], Step [1350/1646], Loss: 0.8119\n",
      "Epoch [5/5], Step [1360/1646], Loss: 0.2592\n",
      "Epoch [5/5], Step [1370/1646], Loss: 0.4242\n",
      "Epoch [5/5], Step [1380/1646], Loss: 0.3610\n",
      "Epoch [5/5], Step [1390/1646], Loss: 0.0456\n",
      "Epoch [5/5], Step [1400/1646], Loss: 0.0596\n",
      "Epoch [5/5], Step [1410/1646], Loss: 0.3138\n",
      "Epoch [5/5], Step [1420/1646], Loss: 0.1650\n",
      "Epoch [5/5], Step [1430/1646], Loss: 0.0212\n",
      "Epoch [5/5], Step [1440/1646], Loss: 0.0901\n",
      "Epoch [5/5], Step [1450/1646], Loss: 0.0239\n",
      "Epoch [5/5], Step [1460/1646], Loss: 0.0198\n",
      "Epoch [5/5], Step [1470/1646], Loss: 0.0943\n",
      "Epoch [5/5], Step [1480/1646], Loss: 0.0475\n",
      "Epoch [5/5], Step [1490/1646], Loss: 0.0086\n",
      "Epoch [5/5], Step [1500/1646], Loss: 0.0515\n",
      "Epoch [5/5], Step [1510/1646], Loss: 0.1967\n",
      "Epoch [5/5], Step [1520/1646], Loss: 0.1546\n",
      "Epoch [5/5], Step [1530/1646], Loss: 0.6056\n",
      "Epoch [5/5], Step [1540/1646], Loss: 0.3117\n",
      "Epoch [5/5], Step [1550/1646], Loss: 0.1285\n",
      "Epoch [5/5], Step [1560/1646], Loss: 0.0447\n",
      "Epoch [5/5], Step [1570/1646], Loss: 0.2794\n",
      "Epoch [5/5], Step [1580/1646], Loss: 0.2937\n",
      "Epoch [5/5], Step [1590/1646], Loss: 0.3904\n",
      "Epoch [5/5], Step [1600/1646], Loss: 0.1346\n",
      "Epoch [5/5], Step [1610/1646], Loss: 0.0527\n",
      "Epoch [5/5], Step [1620/1646], Loss: 0.1139\n",
      "Epoch [5/5], Step [1630/1646], Loss: 0.6210\n",
      "Epoch [5/5], Step [1640/1646], Loss: 0.1751\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkpD/D9wJVjkWN7y5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9b6b9035d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14h 52min 1s, sys: 31min 48s, total: 15h 23min 49s\n",
      "Wall time: 9h 34min 24s\n"
     ]
    }
   ],
   "source": [
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(resnet18.state_dict(), 'resnet18_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "For applying detection, use a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "Take some windows of varying size and aspect ratios and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value. There is a similar approach used in the paper -Faster RCNN by Ross Girshick, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide. You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_window(image, stepSize=15, windowSize=(45,45)):\n",
    "    # Begin\n",
    "    arr = []\n",
    "    for y in range(0, image.shape[0]-windowSize[0], stepSize):\n",
    "        for x in range(0, image.shape[1]-windowSize[1], stepSize):\n",
    "            arr.append((x, y, x + windowSize[0]-1, y + windowSize[1]-1, \n",
    "                        image[y:y + windowSize[1], x:x + windowSize[0]]))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply non_maximum_supression to reduce the number of boxes. You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_maximum_supression(boxes, threshold=0.3):\n",
    "    # Begin\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    " \n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "\n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "    \n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list, add the index\n",
    "        # value to the list of picked indexes, then initialize\n",
    "        # the suppression list (i.e. indexes that will be deleted)\n",
    "        # using the last index\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        suppress = [last]\n",
    "        \n",
    "        # loop over all indexes in the indexes list\n",
    "        for pos in xrange(0, last):\n",
    "            # grab the current index\n",
    "            j = idxs[pos]\n",
    "\n",
    "            # find the largest (x, y) coordinates for the start of\n",
    "            # the bounding box and the smallest (x, y) coordinates\n",
    "            # for the end of the bounding box\n",
    "            xx1 = max(x1[i], x1[j])\n",
    "            yy1 = max(y1[i], y1[j])\n",
    "            xx2 = min(x2[i], x2[j])\n",
    "            yy2 = min(y2[i], y2[j])\n",
    "\n",
    "            # compute the width and height of the bounding box\n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    "\n",
    "            # compute the ratio of overlap between the computed\n",
    "            # bounding box and the bounding box in the area list\n",
    "            overlap = float(w * h) / area[j]\n",
    "\n",
    "            # if there is sufficient overlap, suppress the\n",
    "            # current bounding box\n",
    "            if overlap > threshold:\n",
    "                suppress.append(pos)\n",
    "\n",
    "        # delete all indexes from the index list that are in the\n",
    "        # suppression list\n",
    "        idxs = np.delete(idxs, suppress)\n",
    "\n",
    "    # return only the bounding boxes that were picked\n",
    "    return boxes[pick]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(resnet18):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # Also print out the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time test(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
