{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Networks\n",
    "In this notebook you have to create a custom network whose architecture has been given, and use the dataset you created earlier to train and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "#\n",
    "# Several of the imports you will need have been added but you will need to provide the\n",
    "# rest yourself; you should be able to figure out most of the imports as you go through\n",
    "# the notebook since without proper imports your code will fail to run\n",
    "#\n",
    "# All import statements go in this block\n",
    "\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import sys\n",
    "sys.path.append('/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/')\n",
    "sys.path.append('/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg')\n",
    "# print(sys.path)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as tnf\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All hyper parameters go in the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_gpu = True and torch.cuda.is_available()\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0005\n",
    "# 5, 0.001: 92%\n",
    "# 5, 0.005: 92%\n",
    "# 5, 0.01 : 91%\n",
    "# 5, 0.05 : 88%\n",
    "# 5, 0.1  : 10%\n",
    "# 5, 0.5  : 10%\n",
    "# 10, 0.0002: 92%\n",
    "# 10, 0.0005: 92%\n",
    "# 10, 0.001: 92%\n",
    "# 10, 0.005: 91%\n",
    "# 10, 0.01 : 91%\n",
    "# 10, 0.05 : 89%\n",
    "# 10, 0.1  : 86%\n",
    "# 10, 0.5  : 10%\n",
    "# 20, 0.0005: 91%\n",
    "# 20, 0.001: 89%\n",
    "# 20, 0.005: 91%\n",
    "# 20, 0.01 : 90%\n",
    "# 20, 0.05 : 89%\n",
    "# 20, 0.1  : 10%\n",
    "# 50, 0.0005: 91%\n",
    "# 50, 0.001: 90%\n",
    "# 50, 0.005: 88%\n",
    "# 50, 0.01 : 90%\n",
    "# 50, 0.05 : 89%\n",
    "# 50, 0.1  : 84%\n",
    "# 50, 0.5  : 10%\n",
    "# 100,0.0005: 91%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Dataset and Loader\n",
    "This is the same as part 1. Simply use the same code to create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CDATA(torch.utils.data.Dataset):\n",
    "    # Copy the code from part 1\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # root_dir  - the root directory of the dataset\n",
    "        # train     - a boolean parameter representing whether to return the training set or the test set\n",
    "        # transform - the transforms to be applied on the images before returning them\n",
    "        #\n",
    "        # In this function store the parameters in instance variables and make a mapping\n",
    "        # from images to labels and keep it as an instance variable. Make sure to check which\n",
    "        # dataset is required; train or test; and create the mapping accordingly.\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        if train:\n",
    "            root_dir += 'train/'\n",
    "        else:\n",
    "            root_dir += 'test/'\n",
    "        self.imagefolder_dataset = datasets.ImageFolder(root=root_dir,\n",
    "                                                       transform=transform)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return the size of the dataset (total number of images) as an integer\n",
    "        # this should be rather easy if you created a mapping in __init__\n",
    "        \n",
    "        return self.imagefolder_dataset.__len__()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # idx - the index of the sample requested\n",
    "        #\n",
    "        # Open the image correspoding to idx, apply transforms on it and return a tuple (image, label)\n",
    "        # where label is an integer from 0-9 (since notMNIST has 10 classes)\n",
    "        image, label = self.imagefolder_dataset.__getitem__(idx)\n",
    "        return (image, label)\n",
    "    \n",
    "composed_transform = transforms.Compose([transforms.Scale((32,32)),transforms.ToTensor()])\n",
    "train_dataset = CDATA(root_dir='../custom_dataset/', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = CDATA(root_dir='../custom_dataset/', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Network\n",
    "It's time to create a new custom network. This network is based on Resnet (indeed it is a resnet since it uses skip connections). The architecture of the network is provided in the diagram. It specifies the layer names, layer types as well as their parameters.\n",
    "<img src=\"../architecture/architecture.png\" width=100>\n",
    "[Full size image](../architecture/architecture.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomResnet(nn.Module): # Extend PyTorch's Module class\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(CustomResnet, self).__init__() # Must call super __init__()\n",
    "        \n",
    "        # Define the layers of the network here\n",
    "        # There should be 17 total layers as evident from the diagram\n",
    "        # The parameters and names for the layers are provided in the diagram\n",
    "        # The variable names have to be the same as the ones in the diagram\n",
    "        # Otherwise, the weights will not load\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.lyr1conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.lyr1bn1 = nn.BatchNorm2d(64)\n",
    "        self.lyr1relu1 = nn.ReLU(inplace=True)\n",
    "        self.lyr1conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.lyr1bn2 = nn.BatchNorm2d(64)\n",
    "        self.lyr1relu2 = nn.ReLU(inplace=True)\n",
    "        self.lyr2conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.lyr2bn1 = nn.BatchNorm2d(64)\n",
    "        self.lyr2relu1 = nn.ReLU(inplace=True)\n",
    "        self.lyr2conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.lyr2bn2 = nn.BatchNorm2d(64)\n",
    "        self.lyr2relu2 = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(4096, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Here you have to define the forward pass\n",
    "        # Make sure you take care of the skip connections\n",
    "        # print(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x1 = self.lyr1conv1(x)\n",
    "        x1 = self.lyr1bn1(x1)\n",
    "        x1 = self.lyr1relu1(x1)\n",
    "        x1 = self.lyr1conv2(x1)\n",
    "        x1 = self.lyr1bn2(x1)\n",
    "        x1 = self.lyr1relu2(x+x1)\n",
    "        x = self.lyr2conv1(x1)\n",
    "        x = self.lyr2bn1(x)\n",
    "        x = self.lyr2relu1(x)\n",
    "        x = self.lyr2conv2(x)\n",
    "        x = self.lyr2bn2(x)\n",
    "        x = self.lyr2relu2(x+x1)\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "        return x;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune on pre-trained CIFAR-100 weights\n",
    "We shall now finetune our model using pretrained CIFAR-100 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CustomResnet(num_classes=100) # 100 classes since CIFAR-100 has 100 classes\n",
    "\n",
    "# Load CIFAR-100 weights. (Download them from assignment page)\n",
    "# If network was properly implemented, weights should load without any problems\n",
    "model.load_state_dict(torch.load('../CIFAR-100_weights')) # Supply the path to the weight file\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional\n",
    "As a sanity check you may load the CIFAR-100 test dataset and test the above model. You should get an accuracy of ~41%. This part is optional and is meant for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Accuracy of the network on the 10000 test images: 41 %\n",
      "CPU times: user 2.04 s, sys: 286 ms, total: 2.33 s\n",
      "Wall time: 2.32 s\n"
     ]
    }
   ],
   "source": [
    "# Block for optionally running the model on CIFAR-100 test set\n",
    "ctest_dataset = datasets.CIFAR100(root='../cifar_dataset', \n",
    "                            train=False, \n",
    "                            transform=transforms.ToTensor(),  \n",
    "                            download=True)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "ctest_loader = torch.utils.data.DataLoader(dataset=ctest_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n",
    "\n",
    "def ctest():\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in ctest_loader:\n",
    "        images = Variable(images)\n",
    "        \n",
    "        if (use_gpu):\n",
    "            images = images.cuda()\n",
    "            \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "    print('Accuracy of the network on the %d test images: %d %%' % (total, 100 * correct / total))\n",
    "\n",
    "    \n",
    "%time ctest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finetune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/168], Loss: 0.2194\n",
      "Epoch [2/5], Step [100/168], Loss: 0.1169\n",
      "Epoch [3/5], Step [100/168], Loss: 0.2301\n",
      "Epoch [4/5], Step [100/168], Loss: 0.1944\n",
      "Epoch [5/5], Step [100/168], Loss: 0.1318\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f507c070750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 33s, sys: 18.5 s, total: 1min 51s\n",
      "Wall time: 1min 51s\n",
      "Accuracy of the network on the 1870 test images: 91 %\n",
      "CPU times: user 923 ms, sys: 245 ms, total: 1.17 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "# Change last layer to output 10 classes since our dataset has 10 classes\n",
    "# Complete this statement. It is similar to the resnet18 case\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# Loss function and optimizers\n",
    "# Define cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Use Adam optimizer, use learning_rate hyper parameter\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train():\n",
    "    # Code for training the model\n",
    "    # Make sure to output a matplotlib graph of training losses\n",
    "    loss_store = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if (use_gpu):\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            \n",
    "            running_loss += loss.data[0]\n",
    "            \n",
    "            if (i+1)%100 == 0:\n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "        loss_store += [running_loss]\n",
    "\n",
    "    plt.plot(loss_store)\n",
    "    plt.show()\n",
    "\n",
    "%time train()\n",
    "%time test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1870 test images: 91 %\n",
      "CPU times: user 943 ms, sys: 145 ms, total: 1.09 s\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images = Variable(images)\n",
    "        \n",
    "        if (use_gpu):\n",
    "            images = images.cuda()\n",
    "            \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "    print('Accuracy of the network on the %d test images: %d %%' % (total, 100 * correct / total))\n",
    "    \n",
    "%time test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training from scratch\n",
    "Now we shall try training the model from scratch and observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reinstantiate the model and optimizer\n",
    "model = CustomResnet(num_classes = 10)\n",
    "# Use Adam optimizer, use learning_rate hyper parameter\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train\n",
    "%time train()\n",
    "\n",
    "# Test\n",
    "%time test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of Assignment 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
