{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating Custom Datasets and Finetuning Pre-trained Networks\n",
    "# In this notebook you have to create custom datasets for PyTorch and use this dataset to finetune certain pre-trained neural networks and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "#\n",
    "# Several of the imports you will need have been added but you will need to provide the\n",
    "# rest yourself; you should be able to figure out most of the imports as you go through\n",
    "# the notebook since without proper imports your code will fail to run\n",
    "#\n",
    "# All import statements go in this block\n",
    "\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import sys\n",
    "sys.path.append('/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/')\n",
    "sys.path.append('/home/tulsyan/apps/miniconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg')\n",
    "# print(sys.path)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All hyper parameters go in the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_gpu = False and torch.cuda.is_available()\n",
    "    \n",
    "num_classes = 10\n",
    "batch_size = 20\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Datasets\n",
    "Your first task is to create a pipeline for the custom dataset so that you can load it using a dataloader. Download the dataset provided in the assignment webpage and complete the following block of code so that you can load it as if it was a standard dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CDATA(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # root_dir  - the root directory of the dataset\n",
    "        # train     - a boolean parameter representing whether to return the training set or the test set\n",
    "        # transform - the transforms to be applied on the images before returning them\n",
    "        #\n",
    "        # In this function store the parameters in instance variables and make a mapping\n",
    "        # from images to labels and keep it as an instance variable. Make sure to check which\n",
    "        # dataset is required; train or test; and create the mapping accordingly.\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        if train:\n",
    "            root_dir += 'train/'\n",
    "        else:\n",
    "            root_dir += 'test/'\n",
    "        self.imagefolder_dataset = datasets.ImageFolder(root=root_dir,\n",
    "                                                       transform=transform)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return the size of the dataset (total number of images) as an integer\n",
    "        # this should be rather easy if you created a mapping in __init__\n",
    "        \n",
    "        return self.imagefolder_dataset.__len__()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # idx - the index of the sample requested\n",
    "        #\n",
    "        # Open the image correspoding to idx, apply transforms on it and return a tuple (image, label)\n",
    "        # where label is an integer from 0-9 (since notMNIST has 10 classes)\n",
    "        image, label = self.imagefolder_dataset.__getitem__(idx)\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now load the dataset. You just need to supply the `root_dir` in the block below and if you implemented the above block correctly, it should work without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 16854\n",
      "Size of test dataset: 1870\n",
      "<class 'torch.FloatTensor'> torch.Size([20, 3, 224, 224])\n",
      "Train images\n",
      "Test images\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAChCAYAAADJLnTIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njY2NzQbn/wCeV4FO/b+pzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4b0bb3f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((224,224)),transforms.ToTensor()])\n",
    "train_dataset = CDATA(root_dir='../dataset/', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = CDATA(root_dir='../dataset/', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "# Let's check the size of the datasets, if implemented correctly they should be 16854 and 1870 respectively\n",
    "print('Size of train dataset: %d' % len(train_dataset))\n",
    "print('Size of test dataset: %d' % len(test_dataset))\n",
    "\n",
    "# Create loaders for the dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Let's look at one batch of train and test images\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "train_dataiter = iter(train_loader)\n",
    "train_images, train_labels = train_dataiter.next()\n",
    "print(type(train_images), train_images.shape)\n",
    "print(\"Train images\")\n",
    "imshow(torchvision.utils.make_grid(train_images))\n",
    "\n",
    "test_dataiter = iter(test_loader)\n",
    "test_images, test_labels = test_dataiter.next()\n",
    "print(\"Test images\")\n",
    "imshow(torchvision.utils.make_grid(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 and Resnet-18\n",
    "Now that you have created the dataset we can use it for training and testing neural networks. VGG-16 and Resnet-18 are both well-known deep-net architectures. VGG-16 is named as such since it has 16 layers in total (13 convolution and 3 fully-connected). Resnet-18 on the other hand is a Resnet architecture that uses skip-connections. PyTorch provides pre-trained models of both these architectures and we shall be using them directly. If you are interested in knowing how they have been defined do take a look at the source, [VGG](https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py), [Resnet](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Code to change the last layers so that they only have 10 classes as output\n",
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(512 * 7 * 7, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 10),\n",
    ")\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code for using CUDA here if it is available\n",
    "if(torch.cuda.is_available() and use_gpu):\n",
    "    vgg16.cuda()\n",
    "    resnet18.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss functions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define cross-entropy loss\n",
    "optimizer_vgg16 = torch.optim.Adam(vgg16.parameters(), lr=learning_rate)\n",
    "# Use Adam optimizer, use learning_rate hyper parameter\n",
    "optimizer_resnet18 = torch.optim.Adam(resnet18.parameters(), lr=learning_rate)\n",
    "# Use Adam optimizer, use learning_rate hyper parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning\n",
    "Finetuning is nothing but training models after their weights have been loaded. This allows us to start at a better position than training from scratch. Since the models created already have weights loaded, you simply need to write a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_vgg16():\n",
    "    # Write loops so as to train the model for N epochs, use num_epochs hyper parameter\n",
    "    # Train/finetune the VGG-16 network\n",
    "    # Store the losses for every epoch and generate a graph using matplotlib\n",
    "    loss_store = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if (use_gpu):\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer_vgg16.zero_grad()  # zero the gradient buffer\n",
    "            outputs = vgg16(images)\n",
    "            print(outputs.dtype)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_vgg16.step()    \n",
    "            \n",
    "            running_loss += loss.data[0]\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0])) \n",
    "    loss_store += [running_loss]\n",
    "    plt.plot(loss_store)\n",
    "    plt.show()\n",
    "    \n",
    "def train_resnet18():\n",
    "    # Same as above except now using the Resnet-18 network\n",
    "\n",
    "    loss_store = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if (use_gpu):\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer_resnet18.zero_grad()  # zero the gradient buffer\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_resnet18.step()    \n",
    "            \n",
    "            running_loss += loss.data[0]\n",
    "            \n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "        loss_store += [running_loss]\n",
    "\n",
    "    plt.plot(loss_store)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start the training/finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/842], Loss: 2.5009\n",
      "Epoch [1/5], Step [2/842], Loss: 1.9915\n",
      "Epoch [1/5], Step [3/842], Loss: 1.8979\n",
      "Epoch [1/5], Step [4/842], Loss: 1.7519\n",
      "Epoch [1/5], Step [5/842], Loss: 1.4071\n",
      "Epoch [1/5], Step [6/842], Loss: 1.1830\n",
      "Epoch [1/5], Step [7/842], Loss: 1.2088\n",
      "Epoch [1/5], Step [8/842], Loss: 0.9061\n",
      "Epoch [1/5], Step [9/842], Loss: 0.8537\n",
      "Epoch [1/5], Step [10/842], Loss: 1.0340\n",
      "Epoch [1/5], Step [11/842], Loss: 1.0616\n",
      "Epoch [1/5], Step [12/842], Loss: 0.9238\n",
      "Epoch [1/5], Step [13/842], Loss: 0.7575\n",
      "Epoch [1/5], Step [14/842], Loss: 0.7993\n",
      "Epoch [1/5], Step [15/842], Loss: 0.4692\n",
      "Epoch [1/5], Step [16/842], Loss: 0.7674\n",
      "Epoch [1/5], Step [17/842], Loss: 0.5638\n",
      "Epoch [1/5], Step [18/842], Loss: 0.7326\n",
      "Epoch [1/5], Step [19/842], Loss: 0.8194\n",
      "Epoch [1/5], Step [20/842], Loss: 0.4195\n",
      "Epoch [1/5], Step [21/842], Loss: 1.1799\n",
      "Epoch [1/5], Step [22/842], Loss: 0.6664\n",
      "Epoch [1/5], Step [23/842], Loss: 0.4657\n",
      "Epoch [1/5], Step [24/842], Loss: 1.0137\n",
      "Epoch [1/5], Step [25/842], Loss: 0.3280\n",
      "Epoch [1/5], Step [26/842], Loss: 0.7898\n",
      "Epoch [1/5], Step [27/842], Loss: 0.3843\n",
      "Epoch [1/5], Step [28/842], Loss: 0.2880\n",
      "Epoch [1/5], Step [29/842], Loss: 0.4080\n",
      "Epoch [1/5], Step [30/842], Loss: 0.8912\n",
      "Epoch [1/5], Step [31/842], Loss: 0.6423\n",
      "Epoch [1/5], Step [32/842], Loss: 0.5648\n",
      "Epoch [1/5], Step [33/842], Loss: 0.7807\n",
      "Epoch [1/5], Step [34/842], Loss: 0.4767\n",
      "Epoch [1/5], Step [35/842], Loss: 0.2888\n",
      "Epoch [1/5], Step [36/842], Loss: 0.5352\n",
      "Epoch [1/5], Step [37/842], Loss: 0.4166\n",
      "Epoch [1/5], Step [38/842], Loss: 0.3662\n",
      "Epoch [1/5], Step [39/842], Loss: 0.6167\n",
      "Epoch [1/5], Step [40/842], Loss: 0.4930\n",
      "Epoch [1/5], Step [41/842], Loss: 0.7966\n",
      "Epoch [1/5], Step [42/842], Loss: 0.4190\n",
      "Epoch [1/5], Step [43/842], Loss: 0.7778\n",
      "Epoch [1/5], Step [44/842], Loss: 0.0925\n",
      "Epoch [1/5], Step [45/842], Loss: 0.6259\n",
      "Epoch [1/5], Step [46/842], Loss: 0.5584\n",
      "Epoch [1/5], Step [47/842], Loss: 0.3107\n",
      "Epoch [1/5], Step [48/842], Loss: 0.4009\n",
      "Epoch [1/5], Step [49/842], Loss: 0.1181\n",
      "Epoch [1/5], Step [50/842], Loss: 0.4146\n",
      "Epoch [1/5], Step [51/842], Loss: 0.2538\n",
      "Epoch [1/5], Step [52/842], Loss: 0.2670\n",
      "Epoch [1/5], Step [53/842], Loss: 0.3693\n",
      "Epoch [1/5], Step [54/842], Loss: 0.1864\n",
      "Epoch [1/5], Step [55/842], Loss: 0.3190\n",
      "Epoch [1/5], Step [56/842], Loss: 0.1453\n",
      "Epoch [1/5], Step [57/842], Loss: 0.3191\n",
      "Epoch [1/5], Step [58/842], Loss: 0.3864\n",
      "Epoch [1/5], Step [59/842], Loss: 0.2372\n",
      "Epoch [1/5], Step [60/842], Loss: 0.4749\n",
      "Epoch [1/5], Step [61/842], Loss: 0.2631\n",
      "Epoch [1/5], Step [62/842], Loss: 0.7928\n",
      "Epoch [1/5], Step [63/842], Loss: 0.7186\n",
      "Epoch [1/5], Step [64/842], Loss: 0.2348\n",
      "Epoch [1/5], Step [65/842], Loss: 0.4893\n",
      "Epoch [1/5], Step [66/842], Loss: 0.7007\n",
      "Epoch [1/5], Step [67/842], Loss: 0.5325\n",
      "Epoch [1/5], Step [68/842], Loss: 0.6607\n",
      "Epoch [1/5], Step [69/842], Loss: 0.4145\n",
      "Epoch [1/5], Step [70/842], Loss: 0.2396\n",
      "Epoch [1/5], Step [71/842], Loss: 0.1294\n",
      "Epoch [1/5], Step [72/842], Loss: 0.2802\n",
      "Epoch [1/5], Step [73/842], Loss: 0.3448\n",
      "Epoch [1/5], Step [74/842], Loss: 0.3913\n",
      "Epoch [1/5], Step [75/842], Loss: 0.2758\n",
      "Epoch [1/5], Step [76/842], Loss: 0.3886\n",
      "Epoch [1/5], Step [77/842], Loss: 0.1135\n",
      "Epoch [1/5], Step [78/842], Loss: 1.0893\n",
      "Epoch [1/5], Step [79/842], Loss: 0.4257\n",
      "Epoch [1/5], Step [80/842], Loss: 0.0536\n",
      "Epoch [1/5], Step [81/842], Loss: 0.3267\n",
      "Epoch [1/5], Step [82/842], Loss: 0.2351\n",
      "Epoch [1/5], Step [83/842], Loss: 0.4156\n",
      "Epoch [1/5], Step [84/842], Loss: 0.3010\n",
      "Epoch [1/5], Step [85/842], Loss: 0.3743\n",
      "Epoch [1/5], Step [86/842], Loss: 0.1810\n",
      "Epoch [1/5], Step [87/842], Loss: 0.1552\n",
      "Epoch [1/5], Step [88/842], Loss: 0.5449\n",
      "Epoch [1/5], Step [89/842], Loss: 0.3597\n",
      "Epoch [1/5], Step [90/842], Loss: 0.3646\n",
      "Epoch [1/5], Step [91/842], Loss: 0.7369\n",
      "Epoch [1/5], Step [92/842], Loss: 0.3868\n",
      "Epoch [1/5], Step [93/842], Loss: 0.2709\n",
      "Epoch [1/5], Step [94/842], Loss: 0.3067\n",
      "Epoch [1/5], Step [95/842], Loss: 0.3859\n",
      "Epoch [1/5], Step [96/842], Loss: 0.2489\n",
      "Epoch [1/5], Step [97/842], Loss: 0.0811\n",
      "Epoch [1/5], Step [98/842], Loss: 0.1662\n",
      "Epoch [1/5], Step [99/842], Loss: 0.1532\n",
      "Epoch [1/5], Step [100/842], Loss: 0.3501\n",
      "Epoch [1/5], Step [101/842], Loss: 0.4655\n",
      "Epoch [1/5], Step [102/842], Loss: 0.5050\n",
      "Epoch [1/5], Step [103/842], Loss: 0.3363\n",
      "Epoch [1/5], Step [104/842], Loss: 0.4028\n",
      "Epoch [1/5], Step [105/842], Loss: 0.1688\n",
      "Epoch [1/5], Step [106/842], Loss: 0.0798\n",
      "Epoch [1/5], Step [107/842], Loss: 0.5476\n",
      "Epoch [1/5], Step [108/842], Loss: 0.1638\n",
      "Epoch [1/5], Step [109/842], Loss: 0.1641\n",
      "Epoch [1/5], Step [110/842], Loss: 0.5540\n",
      "Epoch [1/5], Step [111/842], Loss: 0.2804\n",
      "Epoch [1/5], Step [112/842], Loss: 0.2852\n",
      "Epoch [1/5], Step [113/842], Loss: 0.2106\n",
      "Epoch [1/5], Step [114/842], Loss: 0.1678\n",
      "Epoch [1/5], Step [115/842], Loss: 0.3027\n",
      "Epoch [1/5], Step [116/842], Loss: 0.3471\n",
      "Epoch [1/5], Step [117/842], Loss: 0.3828\n",
      "Epoch [1/5], Step [118/842], Loss: 0.0452\n",
      "Epoch [1/5], Step [119/842], Loss: 0.3625\n",
      "Epoch [1/5], Step [120/842], Loss: 0.6049\n",
      "Epoch [1/5], Step [121/842], Loss: 0.2349\n",
      "Epoch [1/5], Step [122/842], Loss: 0.0978\n",
      "Epoch [1/5], Step [123/842], Loss: 0.3197\n",
      "Epoch [1/5], Step [124/842], Loss: 0.0704\n",
      "Epoch [1/5], Step [125/842], Loss: 0.2517\n",
      "Epoch [1/5], Step [126/842], Loss: 0.5086\n",
      "Epoch [1/5], Step [127/842], Loss: 0.0908\n",
      "Epoch [1/5], Step [128/842], Loss: 0.0390\n",
      "Epoch [1/5], Step [129/842], Loss: 0.3054\n",
      "Epoch [1/5], Step [130/842], Loss: 0.3582\n",
      "Epoch [1/5], Step [131/842], Loss: 0.2219\n",
      "Epoch [1/5], Step [132/842], Loss: 0.2548\n",
      "Epoch [1/5], Step [133/842], Loss: 0.5360\n",
      "Epoch [1/5], Step [134/842], Loss: 0.2997\n",
      "Epoch [1/5], Step [135/842], Loss: 0.3843\n",
      "Epoch [1/5], Step [136/842], Loss: 0.2671\n",
      "Epoch [1/5], Step [137/842], Loss: 0.2402\n",
      "Epoch [1/5], Step [138/842], Loss: 0.4217\n",
      "Epoch [1/5], Step [139/842], Loss: 0.1908\n",
      "Epoch [1/5], Step [140/842], Loss: 0.1546\n",
      "Epoch [1/5], Step [141/842], Loss: 0.5882\n",
      "Epoch [1/5], Step [142/842], Loss: 0.2848\n",
      "Epoch [1/5], Step [143/842], Loss: 0.1553\n",
      "Epoch [1/5], Step [144/842], Loss: 0.3286\n",
      "Epoch [1/5], Step [145/842], Loss: 0.7162\n",
      "Epoch [1/5], Step [146/842], Loss: 0.5647\n",
      "Epoch [1/5], Step [147/842], Loss: 0.1657\n",
      "Epoch [1/5], Step [148/842], Loss: 0.6459\n",
      "Epoch [1/5], Step [149/842], Loss: 0.3199\n",
      "Epoch [1/5], Step [150/842], Loss: 0.2789\n",
      "Epoch [1/5], Step [151/842], Loss: 0.2201\n",
      "Epoch [1/5], Step [152/842], Loss: 0.3960\n",
      "Epoch [1/5], Step [153/842], Loss: 0.1734\n",
      "Epoch [1/5], Step [154/842], Loss: 0.0766\n",
      "Epoch [1/5], Step [155/842], Loss: 0.5051\n",
      "Epoch [1/5], Step [156/842], Loss: 0.0773\n",
      "Epoch [1/5], Step [157/842], Loss: 0.4341\n",
      "Epoch [1/5], Step [158/842], Loss: 0.2750\n",
      "Epoch [1/5], Step [159/842], Loss: 0.4216\n",
      "Epoch [1/5], Step [160/842], Loss: 0.1451\n",
      "Epoch [1/5], Step [161/842], Loss: 0.0483\n",
      "Epoch [1/5], Step [162/842], Loss: 0.1283\n",
      "Epoch [1/5], Step [163/842], Loss: 0.3615\n",
      "Epoch [1/5], Step [164/842], Loss: 0.2374\n",
      "Epoch [1/5], Step [165/842], Loss: 0.4878\n",
      "Epoch [1/5], Step [166/842], Loss: 0.0707\n",
      "Epoch [1/5], Step [167/842], Loss: 0.2650\n",
      "Epoch [1/5], Step [168/842], Loss: 0.3785\n",
      "Epoch [1/5], Step [169/842], Loss: 0.2765\n",
      "Epoch [1/5], Step [170/842], Loss: 0.2509\n",
      "Epoch [1/5], Step [171/842], Loss: 0.2444\n",
      "Epoch [1/5], Step [172/842], Loss: 0.6651\n",
      "Epoch [1/5], Step [173/842], Loss: 0.2150\n",
      "Epoch [1/5], Step [174/842], Loss: 0.3848\n",
      "Epoch [1/5], Step [175/842], Loss: 0.1649\n",
      "Epoch [1/5], Step [176/842], Loss: 0.3571\n",
      "Epoch [1/5], Step [177/842], Loss: 0.0506\n",
      "Epoch [1/5], Step [178/842], Loss: 0.1581\n",
      "Epoch [1/5], Step [179/842], Loss: 0.4546\n",
      "Epoch [1/5], Step [180/842], Loss: 0.8519\n",
      "Epoch [1/5], Step [181/842], Loss: 0.1563\n",
      "Epoch [1/5], Step [182/842], Loss: 0.6538\n",
      "Epoch [1/5], Step [183/842], Loss: 0.5431\n",
      "Epoch [1/5], Step [184/842], Loss: 0.5177\n",
      "Epoch [1/5], Step [185/842], Loss: 0.0875\n",
      "Epoch [1/5], Step [186/842], Loss: 0.1008\n",
      "Epoch [1/5], Step [187/842], Loss: 0.0631\n",
      "Epoch [1/5], Step [188/842], Loss: 0.2806\n",
      "Epoch [1/5], Step [189/842], Loss: 0.6605\n",
      "Epoch [1/5], Step [190/842], Loss: 0.2125\n",
      "Epoch [1/5], Step [191/842], Loss: 0.2149\n",
      "Epoch [1/5], Step [192/842], Loss: 0.3198\n",
      "Epoch [1/5], Step [193/842], Loss: 0.0732\n",
      "Epoch [1/5], Step [194/842], Loss: 0.1293\n",
      "Epoch [1/5], Step [195/842], Loss: 0.3821\n",
      "Epoch [1/5], Step [196/842], Loss: 1.0162\n",
      "Epoch [1/5], Step [197/842], Loss: 0.2865\n",
      "Epoch [1/5], Step [198/842], Loss: 0.3707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [199/842], Loss: 0.2741\n",
      "Epoch [1/5], Step [200/842], Loss: 0.5439\n",
      "Epoch [1/5], Step [201/842], Loss: 0.4644\n",
      "Epoch [1/5], Step [202/842], Loss: 0.1325\n",
      "Epoch [1/5], Step [203/842], Loss: 0.4391\n",
      "Epoch [1/5], Step [204/842], Loss: 0.5944\n",
      "Epoch [1/5], Step [205/842], Loss: 0.2563\n",
      "Epoch [1/5], Step [206/842], Loss: 0.2896\n",
      "Epoch [1/5], Step [207/842], Loss: 0.2058\n",
      "Epoch [1/5], Step [208/842], Loss: 0.1731\n",
      "Epoch [1/5], Step [209/842], Loss: 0.4148\n",
      "Epoch [1/5], Step [210/842], Loss: 0.1195\n",
      "Epoch [1/5], Step [211/842], Loss: 0.1611\n",
      "Epoch [1/5], Step [212/842], Loss: 0.4790\n",
      "Epoch [1/5], Step [213/842], Loss: 0.5202\n",
      "Epoch [1/5], Step [214/842], Loss: 0.1884\n",
      "Epoch [1/5], Step [215/842], Loss: 0.4506\n",
      "Epoch [1/5], Step [216/842], Loss: 0.2645\n",
      "Epoch [1/5], Step [217/842], Loss: 0.1247\n",
      "Epoch [1/5], Step [218/842], Loss: 0.4335\n",
      "Epoch [1/5], Step [219/842], Loss: 0.3777\n",
      "Epoch [1/5], Step [220/842], Loss: 0.1602\n",
      "Epoch [1/5], Step [221/842], Loss: 0.1731\n",
      "Epoch [1/5], Step [222/842], Loss: 0.0494\n",
      "Epoch [1/5], Step [223/842], Loss: 0.1250\n",
      "Epoch [1/5], Step [224/842], Loss: 0.1748\n",
      "Epoch [1/5], Step [225/842], Loss: 0.0444\n",
      "Epoch [1/5], Step [226/842], Loss: 0.2139\n",
      "Epoch [1/5], Step [227/842], Loss: 0.3536\n",
      "Epoch [1/5], Step [228/842], Loss: 0.1591\n",
      "Epoch [1/5], Step [229/842], Loss: 0.3055\n",
      "Epoch [1/5], Step [230/842], Loss: 0.2980\n",
      "Epoch [1/5], Step [231/842], Loss: 0.1433\n",
      "Epoch [1/5], Step [232/842], Loss: 0.0635\n",
      "Epoch [1/5], Step [233/842], Loss: 0.1292\n",
      "Epoch [1/5], Step [234/842], Loss: 0.3292\n",
      "Epoch [1/5], Step [235/842], Loss: 0.0746\n",
      "Epoch [1/5], Step [236/842], Loss: 0.3796\n",
      "Epoch [1/5], Step [237/842], Loss: 0.3055\n",
      "Epoch [1/5], Step [238/842], Loss: 0.2987\n",
      "Epoch [1/5], Step [239/842], Loss: 0.2252\n",
      "Epoch [1/5], Step [240/842], Loss: 0.1931\n",
      "Epoch [1/5], Step [241/842], Loss: 0.2611\n",
      "Epoch [1/5], Step [242/842], Loss: 0.1239\n",
      "Epoch [1/5], Step [243/842], Loss: 0.1533\n",
      "Epoch [1/5], Step [244/842], Loss: 0.1647\n",
      "Epoch [1/5], Step [245/842], Loss: 0.1060\n",
      "Epoch [1/5], Step [246/842], Loss: 0.1935\n",
      "Epoch [1/5], Step [247/842], Loss: 0.0809\n",
      "Epoch [1/5], Step [248/842], Loss: 0.2123\n",
      "Epoch [1/5], Step [249/842], Loss: 0.2599\n",
      "Epoch [1/5], Step [250/842], Loss: 0.1945\n",
      "Epoch [1/5], Step [251/842], Loss: 0.1417\n",
      "Epoch [1/5], Step [252/842], Loss: 0.6220\n",
      "Epoch [1/5], Step [253/842], Loss: 0.1572\n",
      "Epoch [1/5], Step [254/842], Loss: 0.3289\n",
      "Epoch [1/5], Step [255/842], Loss: 0.3504\n",
      "Epoch [1/5], Step [256/842], Loss: 0.1670\n",
      "Epoch [1/5], Step [257/842], Loss: 0.5525\n",
      "Epoch [1/5], Step [258/842], Loss: 0.0394\n",
      "Epoch [1/5], Step [259/842], Loss: 0.5901\n",
      "Epoch [1/5], Step [260/842], Loss: 0.3910\n",
      "Epoch [1/5], Step [261/842], Loss: 0.3942\n",
      "Epoch [1/5], Step [262/842], Loss: 0.2311\n",
      "Epoch [1/5], Step [263/842], Loss: 0.2925\n",
      "Epoch [1/5], Step [264/842], Loss: 0.2068\n",
      "Epoch [1/5], Step [265/842], Loss: 0.1541\n",
      "Epoch [1/5], Step [266/842], Loss: 0.1964\n",
      "Epoch [1/5], Step [267/842], Loss: 0.3468\n",
      "Epoch [1/5], Step [268/842], Loss: 0.3823\n",
      "Epoch [1/5], Step [269/842], Loss: 0.1161\n",
      "Epoch [1/5], Step [270/842], Loss: 0.1557\n",
      "Epoch [1/5], Step [271/842], Loss: 0.1383\n",
      "Epoch [1/5], Step [272/842], Loss: 0.2969\n",
      "Epoch [1/5], Step [273/842], Loss: 0.1267\n",
      "Epoch [1/5], Step [274/842], Loss: 0.0561\n",
      "Epoch [1/5], Step [275/842], Loss: 0.2765\n",
      "Epoch [1/5], Step [276/842], Loss: 0.0477\n",
      "Epoch [1/5], Step [277/842], Loss: 0.3296\n",
      "Epoch [1/5], Step [278/842], Loss: 0.0417\n",
      "Epoch [1/5], Step [279/842], Loss: 0.1438\n",
      "Epoch [1/5], Step [280/842], Loss: 0.5163\n",
      "Epoch [1/5], Step [281/842], Loss: 0.0679\n",
      "Epoch [1/5], Step [282/842], Loss: 0.1046\n",
      "Epoch [1/5], Step [283/842], Loss: 0.6115\n",
      "Epoch [1/5], Step [284/842], Loss: 0.1654\n",
      "Epoch [1/5], Step [285/842], Loss: 0.1145\n",
      "Epoch [1/5], Step [286/842], Loss: 0.1191\n",
      "Epoch [1/5], Step [287/842], Loss: 0.5059\n",
      "Epoch [1/5], Step [288/842], Loss: 0.1603\n",
      "Epoch [1/5], Step [289/842], Loss: 0.0837\n",
      "Epoch [1/5], Step [290/842], Loss: 0.4278\n",
      "Epoch [1/5], Step [291/842], Loss: 0.2663\n",
      "Epoch [1/5], Step [292/842], Loss: 0.2186\n",
      "Epoch [1/5], Step [293/842], Loss: 0.4883\n",
      "Epoch [1/5], Step [294/842], Loss: 0.2204\n",
      "Epoch [1/5], Step [295/842], Loss: 0.2439\n",
      "Epoch [1/5], Step [296/842], Loss: 0.3409\n",
      "Epoch [1/5], Step [297/842], Loss: 0.2793\n",
      "Epoch [1/5], Step [298/842], Loss: 0.6636\n",
      "Epoch [1/5], Step [299/842], Loss: 0.0218\n",
      "Epoch [1/5], Step [300/842], Loss: 0.2782\n",
      "Epoch [1/5], Step [301/842], Loss: 0.1416\n",
      "Epoch [1/5], Step [302/842], Loss: 0.1930\n",
      "Epoch [1/5], Step [303/842], Loss: 0.0603\n",
      "Epoch [1/5], Step [304/842], Loss: 0.5760\n",
      "Epoch [1/5], Step [305/842], Loss: 0.3088\n",
      "Epoch [1/5], Step [306/842], Loss: 0.4296\n",
      "Epoch [1/5], Step [307/842], Loss: 0.0740\n",
      "Epoch [1/5], Step [308/842], Loss: 0.2403\n",
      "Epoch [1/5], Step [309/842], Loss: 0.5069\n",
      "Epoch [1/5], Step [310/842], Loss: 0.1926\n",
      "Epoch [1/5], Step [311/842], Loss: 0.0424\n",
      "Epoch [1/5], Step [312/842], Loss: 0.2478\n",
      "Epoch [1/5], Step [313/842], Loss: 0.3340\n",
      "Epoch [1/5], Step [314/842], Loss: 0.1797\n",
      "Epoch [1/5], Step [315/842], Loss: 0.2002\n",
      "Epoch [1/5], Step [316/842], Loss: 0.4155\n",
      "Epoch [1/5], Step [317/842], Loss: 0.0986\n",
      "Epoch [1/5], Step [318/842], Loss: 0.2051\n",
      "Epoch [1/5], Step [319/842], Loss: 0.4803\n",
      "Epoch [1/5], Step [320/842], Loss: 0.2164\n",
      "Epoch [1/5], Step [321/842], Loss: 0.0712\n",
      "Epoch [1/5], Step [322/842], Loss: 0.0808\n",
      "Epoch [1/5], Step [323/842], Loss: 0.2651\n",
      "Epoch [1/5], Step [324/842], Loss: 0.2939\n",
      "Epoch [1/5], Step [325/842], Loss: 0.3176\n",
      "Epoch [1/5], Step [326/842], Loss: 0.0877\n",
      "Epoch [1/5], Step [327/842], Loss: 0.1139\n",
      "Epoch [1/5], Step [328/842], Loss: 0.1208\n",
      "Epoch [1/5], Step [329/842], Loss: 0.5624\n",
      "Epoch [1/5], Step [330/842], Loss: 0.3500\n",
      "Epoch [1/5], Step [331/842], Loss: 0.3115\n",
      "Epoch [1/5], Step [332/842], Loss: 0.2194\n",
      "Epoch [1/5], Step [333/842], Loss: 0.2624\n",
      "Epoch [1/5], Step [334/842], Loss: 0.0418\n",
      "Epoch [1/5], Step [335/842], Loss: 0.2975\n",
      "Epoch [1/5], Step [336/842], Loss: 0.3513\n",
      "Epoch [1/5], Step [337/842], Loss: 0.1876\n",
      "Epoch [1/5], Step [338/842], Loss: 0.3497\n",
      "Epoch [1/5], Step [339/842], Loss: 0.1692\n",
      "Epoch [1/5], Step [340/842], Loss: 0.1047\n",
      "Epoch [1/5], Step [341/842], Loss: 0.1479\n",
      "Epoch [1/5], Step [342/842], Loss: 0.0984\n",
      "Epoch [1/5], Step [343/842], Loss: 0.0754\n",
      "Epoch [1/5], Step [344/842], Loss: 0.2685\n",
      "Epoch [1/5], Step [345/842], Loss: 0.1185\n",
      "Epoch [1/5], Step [346/842], Loss: 0.2860\n",
      "Epoch [1/5], Step [347/842], Loss: 0.2593\n",
      "Epoch [1/5], Step [348/842], Loss: 0.0535\n",
      "Epoch [1/5], Step [349/842], Loss: 0.1858\n",
      "Epoch [1/5], Step [350/842], Loss: 0.0943\n",
      "Epoch [1/5], Step [351/842], Loss: 0.1225\n",
      "Epoch [1/5], Step [352/842], Loss: 0.5137\n",
      "Epoch [1/5], Step [353/842], Loss: 0.2243\n",
      "Epoch [1/5], Step [354/842], Loss: 0.2001\n",
      "Epoch [1/5], Step [355/842], Loss: 0.2269\n",
      "Epoch [1/5], Step [356/842], Loss: 0.0365\n",
      "Epoch [1/5], Step [357/842], Loss: 0.1627\n",
      "Epoch [1/5], Step [358/842], Loss: 0.0293\n",
      "Epoch [1/5], Step [359/842], Loss: 0.1970\n",
      "Epoch [1/5], Step [360/842], Loss: 0.5507\n",
      "Epoch [1/5], Step [361/842], Loss: 0.0950\n",
      "Epoch [1/5], Step [362/842], Loss: 0.0430\n",
      "Epoch [1/5], Step [363/842], Loss: 0.2722\n",
      "Epoch [1/5], Step [364/842], Loss: 0.0289\n",
      "Epoch [1/5], Step [365/842], Loss: 0.0540\n",
      "Epoch [1/5], Step [366/842], Loss: 0.0808\n",
      "Epoch [1/5], Step [367/842], Loss: 0.0721\n",
      "Epoch [1/5], Step [368/842], Loss: 0.0885\n",
      "Epoch [1/5], Step [369/842], Loss: 0.3118\n",
      "Epoch [1/5], Step [370/842], Loss: 0.0107\n",
      "Epoch [1/5], Step [371/842], Loss: 0.2397\n",
      "Epoch [1/5], Step [372/842], Loss: 0.1868\n",
      "Epoch [1/5], Step [373/842], Loss: 0.0606\n",
      "Epoch [1/5], Step [374/842], Loss: 0.1132\n",
      "Epoch [1/5], Step [375/842], Loss: 0.2208\n",
      "Epoch [1/5], Step [376/842], Loss: 0.0231\n",
      "Epoch [1/5], Step [377/842], Loss: 0.0821\n",
      "Epoch [1/5], Step [378/842], Loss: 0.0077\n",
      "Epoch [1/5], Step [379/842], Loss: 0.3052\n",
      "Epoch [1/5], Step [380/842], Loss: 0.2997\n",
      "Epoch [1/5], Step [381/842], Loss: 0.0180\n",
      "Epoch [1/5], Step [382/842], Loss: 0.0687\n",
      "Epoch [1/5], Step [383/842], Loss: 0.6657\n",
      "Epoch [1/5], Step [384/842], Loss: 0.0765\n",
      "Epoch [1/5], Step [385/842], Loss: 0.1992\n",
      "Epoch [1/5], Step [386/842], Loss: 0.0505\n",
      "Epoch [1/5], Step [387/842], Loss: 0.1126\n",
      "Epoch [1/5], Step [388/842], Loss: 0.0206\n",
      "Epoch [1/5], Step [389/842], Loss: 0.0867\n",
      "Epoch [1/5], Step [390/842], Loss: 0.1083\n",
      "Epoch [1/5], Step [391/842], Loss: 0.2030\n",
      "Epoch [1/5], Step [392/842], Loss: 0.1869\n",
      "Epoch [1/5], Step [393/842], Loss: 0.1450\n",
      "Epoch [1/5], Step [394/842], Loss: 0.2889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [395/842], Loss: 0.2322\n",
      "Epoch [1/5], Step [396/842], Loss: 0.2428\n",
      "Epoch [1/5], Step [397/842], Loss: 0.1637\n",
      "Epoch [1/5], Step [398/842], Loss: 0.2249\n",
      "Epoch [1/5], Step [399/842], Loss: 0.1860\n",
      "Epoch [1/5], Step [400/842], Loss: 0.1273\n",
      "Epoch [1/5], Step [401/842], Loss: 0.1395\n",
      "Epoch [1/5], Step [402/842], Loss: 0.3136\n",
      "Epoch [1/5], Step [403/842], Loss: 0.0429\n",
      "Epoch [1/5], Step [404/842], Loss: 0.3453\n",
      "Epoch [1/5], Step [405/842], Loss: 0.4498\n",
      "Epoch [1/5], Step [406/842], Loss: 0.3213\n",
      "Epoch [1/5], Step [407/842], Loss: 0.0555\n",
      "Epoch [1/5], Step [408/842], Loss: 0.2762\n",
      "Epoch [1/5], Step [409/842], Loss: 0.0402\n",
      "Epoch [1/5], Step [410/842], Loss: 0.2209\n",
      "Epoch [1/5], Step [411/842], Loss: 0.1104\n",
      "Epoch [1/5], Step [412/842], Loss: 0.2226\n",
      "Epoch [1/5], Step [413/842], Loss: 0.1761\n",
      "Epoch [1/5], Step [414/842], Loss: 0.2503\n",
      "Epoch [1/5], Step [415/842], Loss: 0.3019\n",
      "Epoch [1/5], Step [416/842], Loss: 0.2500\n",
      "Epoch [1/5], Step [417/842], Loss: 0.5477\n",
      "Epoch [1/5], Step [418/842], Loss: 0.0935\n",
      "Epoch [1/5], Step [419/842], Loss: 0.3422\n",
      "Epoch [1/5], Step [420/842], Loss: 0.0848\n",
      "Epoch [1/5], Step [421/842], Loss: 0.2132\n",
      "Epoch [1/5], Step [422/842], Loss: 0.6035\n",
      "Epoch [1/5], Step [423/842], Loss: 0.0975\n",
      "Epoch [1/5], Step [424/842], Loss: 0.7418\n",
      "Epoch [1/5], Step [425/842], Loss: 0.2673\n",
      "Epoch [1/5], Step [426/842], Loss: 0.1971\n",
      "Epoch [1/5], Step [427/842], Loss: 0.0141\n",
      "Epoch [1/5], Step [428/842], Loss: 0.1689\n",
      "Epoch [1/5], Step [429/842], Loss: 0.0428\n",
      "Epoch [1/5], Step [430/842], Loss: 0.1807\n",
      "Epoch [1/5], Step [431/842], Loss: 0.1816\n",
      "Epoch [1/5], Step [432/842], Loss: 0.2024\n",
      "Epoch [1/5], Step [433/842], Loss: 0.1441\n",
      "Epoch [1/5], Step [434/842], Loss: 0.0316\n",
      "Epoch [1/5], Step [435/842], Loss: 0.2579\n",
      "Epoch [1/5], Step [436/842], Loss: 0.0438\n",
      "Epoch [1/5], Step [437/842], Loss: 0.2467\n",
      "Epoch [1/5], Step [438/842], Loss: 0.0326\n",
      "Epoch [1/5], Step [439/842], Loss: 0.2811\n",
      "Epoch [1/5], Step [440/842], Loss: 0.2537\n",
      "Epoch [1/5], Step [441/842], Loss: 0.4101\n",
      "Epoch [1/5], Step [442/842], Loss: 0.2012\n",
      "Epoch [1/5], Step [443/842], Loss: 0.1310\n",
      "Epoch [1/5], Step [444/842], Loss: 0.3910\n",
      "Epoch [1/5], Step [445/842], Loss: 0.3196\n",
      "Epoch [1/5], Step [446/842], Loss: 0.0617\n",
      "Epoch [1/5], Step [447/842], Loss: 0.0313\n",
      "Epoch [1/5], Step [448/842], Loss: 0.0591\n",
      "Epoch [1/5], Step [449/842], Loss: 0.3330\n",
      "Epoch [1/5], Step [450/842], Loss: 0.0801\n",
      "Epoch [1/5], Step [451/842], Loss: 0.1466\n",
      "Epoch [1/5], Step [452/842], Loss: 0.6789\n",
      "Epoch [1/5], Step [453/842], Loss: 0.0859\n",
      "Epoch [1/5], Step [454/842], Loss: 0.3196\n",
      "Epoch [1/5], Step [455/842], Loss: 0.1344\n",
      "Epoch [1/5], Step [456/842], Loss: 0.3338\n",
      "Epoch [1/5], Step [457/842], Loss: 0.0724\n",
      "Epoch [1/5], Step [458/842], Loss: 0.4702\n",
      "Epoch [1/5], Step [459/842], Loss: 0.1660\n",
      "Epoch [1/5], Step [460/842], Loss: 0.1800\n",
      "Epoch [1/5], Step [461/842], Loss: 0.3620\n",
      "Epoch [1/5], Step [462/842], Loss: 0.2459\n",
      "Epoch [1/5], Step [463/842], Loss: 0.1961\n",
      "Epoch [1/5], Step [464/842], Loss: 0.1351\n",
      "Epoch [1/5], Step [465/842], Loss: 0.1738\n",
      "Epoch [1/5], Step [466/842], Loss: 0.3931\n",
      "Epoch [1/5], Step [467/842], Loss: 0.1062\n",
      "Epoch [1/5], Step [468/842], Loss: 0.4280\n",
      "Epoch [1/5], Step [469/842], Loss: 0.0167\n",
      "Epoch [1/5], Step [470/842], Loss: 0.3981\n",
      "Epoch [1/5], Step [471/842], Loss: 0.4664\n",
      "Epoch [1/5], Step [472/842], Loss: 0.1199\n",
      "Epoch [1/5], Step [473/842], Loss: 0.2676\n",
      "Epoch [1/5], Step [474/842], Loss: 0.2022\n",
      "Epoch [1/5], Step [475/842], Loss: 0.1841\n",
      "Epoch [1/5], Step [476/842], Loss: 0.1755\n",
      "Epoch [1/5], Step [477/842], Loss: 0.4086\n",
      "Epoch [1/5], Step [478/842], Loss: 0.0918\n",
      "Epoch [1/5], Step [479/842], Loss: 0.0156\n",
      "Epoch [1/5], Step [480/842], Loss: 0.3193\n",
      "Epoch [1/5], Step [481/842], Loss: 0.1982\n",
      "Epoch [1/5], Step [482/842], Loss: 0.0328\n",
      "Epoch [1/5], Step [483/842], Loss: 0.2402\n",
      "Epoch [1/5], Step [484/842], Loss: 0.1990\n",
      "Epoch [1/5], Step [485/842], Loss: 0.1982\n",
      "Epoch [1/5], Step [486/842], Loss: 0.1300\n",
      "Epoch [1/5], Step [487/842], Loss: 0.3815\n",
      "Epoch [1/5], Step [488/842], Loss: 0.0260\n",
      "Epoch [1/5], Step [489/842], Loss: 0.0469\n",
      "Epoch [1/5], Step [490/842], Loss: 0.0527\n",
      "Epoch [1/5], Step [491/842], Loss: 0.0600\n",
      "Epoch [1/5], Step [492/842], Loss: 0.4447\n",
      "Epoch [1/5], Step [493/842], Loss: 0.3364\n",
      "Epoch [1/5], Step [494/842], Loss: 0.0384\n",
      "Epoch [1/5], Step [495/842], Loss: 0.2049\n",
      "Epoch [1/5], Step [496/842], Loss: 0.6039\n",
      "Epoch [1/5], Step [497/842], Loss: 0.5174\n",
      "Epoch [1/5], Step [498/842], Loss: 0.0417\n",
      "Epoch [1/5], Step [499/842], Loss: 0.0512\n",
      "Epoch [1/5], Step [500/842], Loss: 0.2059\n",
      "Epoch [1/5], Step [501/842], Loss: 0.0496\n",
      "Epoch [1/5], Step [502/842], Loss: 0.2058\n",
      "Epoch [1/5], Step [503/842], Loss: 0.0138\n",
      "Epoch [1/5], Step [504/842], Loss: 0.0501\n",
      "Epoch [1/5], Step [505/842], Loss: 0.0848\n",
      "Epoch [1/5], Step [506/842], Loss: 0.1721\n",
      "Epoch [1/5], Step [507/842], Loss: 0.0900\n",
      "Epoch [1/5], Step [508/842], Loss: 0.2194\n",
      "Epoch [1/5], Step [509/842], Loss: 0.0281\n",
      "Epoch [1/5], Step [510/842], Loss: 0.0138\n",
      "Epoch [1/5], Step [511/842], Loss: 0.0289\n",
      "Epoch [1/5], Step [512/842], Loss: 0.0813\n",
      "Epoch [1/5], Step [513/842], Loss: 0.5308\n",
      "Epoch [1/5], Step [514/842], Loss: 0.0077\n",
      "Epoch [1/5], Step [515/842], Loss: 0.1285\n",
      "Epoch [1/5], Step [516/842], Loss: 0.0525\n",
      "Epoch [1/5], Step [517/842], Loss: 0.3017\n",
      "Epoch [1/5], Step [518/842], Loss: 0.1485\n",
      "Epoch [1/5], Step [519/842], Loss: 0.1651\n",
      "Epoch [1/5], Step [520/842], Loss: 0.0158\n",
      "Epoch [1/5], Step [521/842], Loss: 0.0965\n",
      "Epoch [1/5], Step [522/842], Loss: 0.1781\n",
      "Epoch [1/5], Step [523/842], Loss: 0.0313\n",
      "Epoch [1/5], Step [524/842], Loss: 0.1578\n",
      "Epoch [1/5], Step [525/842], Loss: 0.2990\n",
      "Epoch [1/5], Step [526/842], Loss: 0.1560\n",
      "Epoch [1/5], Step [527/842], Loss: 0.0997\n",
      "Epoch [1/5], Step [528/842], Loss: 0.0527\n",
      "Epoch [1/5], Step [529/842], Loss: 0.3103\n",
      "Epoch [1/5], Step [530/842], Loss: 0.0474\n",
      "Epoch [1/5], Step [531/842], Loss: 0.1491\n",
      "Epoch [1/5], Step [532/842], Loss: 0.4989\n",
      "Epoch [1/5], Step [533/842], Loss: 0.3099\n",
      "Epoch [1/5], Step [534/842], Loss: 0.0145\n",
      "Epoch [1/5], Step [535/842], Loss: 0.0330\n",
      "Epoch [1/5], Step [536/842], Loss: 0.0565\n",
      "Epoch [1/5], Step [537/842], Loss: 0.1777\n",
      "Epoch [1/5], Step [538/842], Loss: 0.4226\n",
      "Epoch [1/5], Step [539/842], Loss: 0.0134\n",
      "Epoch [1/5], Step [540/842], Loss: 0.0627\n",
      "Epoch [1/5], Step [541/842], Loss: 0.0265\n",
      "Epoch [1/5], Step [542/842], Loss: 0.0997\n",
      "Epoch [1/5], Step [543/842], Loss: 0.3594\n",
      "Epoch [1/5], Step [544/842], Loss: 0.1514\n",
      "Epoch [1/5], Step [545/842], Loss: 0.0820\n",
      "Epoch [1/5], Step [546/842], Loss: 0.3820\n",
      "Epoch [1/5], Step [547/842], Loss: 0.2357\n",
      "Epoch [1/5], Step [548/842], Loss: 0.0156\n",
      "Epoch [1/5], Step [549/842], Loss: 0.0223\n",
      "Epoch [1/5], Step [550/842], Loss: 0.2109\n",
      "Epoch [1/5], Step [551/842], Loss: 0.3510\n",
      "Epoch [1/5], Step [552/842], Loss: 0.0484\n",
      "Epoch [1/5], Step [553/842], Loss: 0.2536\n",
      "Epoch [1/5], Step [554/842], Loss: 0.2026\n",
      "Epoch [1/5], Step [555/842], Loss: 0.2325\n",
      "Epoch [1/5], Step [556/842], Loss: 0.1074\n",
      "Epoch [1/5], Step [557/842], Loss: 0.0875\n",
      "Epoch [1/5], Step [558/842], Loss: 0.4625\n",
      "Epoch [1/5], Step [559/842], Loss: 0.1619\n",
      "Epoch [1/5], Step [560/842], Loss: 0.2505\n",
      "Epoch [1/5], Step [561/842], Loss: 0.3302\n",
      "Epoch [1/5], Step [562/842], Loss: 0.1854\n",
      "Epoch [1/5], Step [563/842], Loss: 0.0565\n",
      "Epoch [1/5], Step [564/842], Loss: 0.4423\n",
      "Epoch [1/5], Step [565/842], Loss: 0.3865\n",
      "Epoch [1/5], Step [566/842], Loss: 0.1363\n",
      "Epoch [1/5], Step [567/842], Loss: 0.1541\n",
      "Epoch [1/5], Step [568/842], Loss: 0.3628\n",
      "Epoch [1/5], Step [569/842], Loss: 0.2686\n",
      "Epoch [1/5], Step [570/842], Loss: 0.3110\n",
      "Epoch [1/5], Step [571/842], Loss: 0.2553\n",
      "Epoch [1/5], Step [572/842], Loss: 0.0500\n",
      "Epoch [1/5], Step [573/842], Loss: 0.2807\n",
      "Epoch [1/5], Step [574/842], Loss: 0.2049\n",
      "Epoch [1/5], Step [575/842], Loss: 0.6222\n",
      "Epoch [1/5], Step [576/842], Loss: 0.0741\n",
      "Epoch [1/5], Step [577/842], Loss: 0.0252\n",
      "Epoch [1/5], Step [578/842], Loss: 0.0319\n",
      "Epoch [1/5], Step [579/842], Loss: 0.2322\n",
      "Epoch [1/5], Step [580/842], Loss: 0.1137\n",
      "Epoch [1/5], Step [581/842], Loss: 0.0975\n",
      "Epoch [1/5], Step [582/842], Loss: 0.0881\n",
      "Epoch [1/5], Step [583/842], Loss: 0.3345\n",
      "Epoch [1/5], Step [584/842], Loss: 0.5810\n",
      "Epoch [1/5], Step [585/842], Loss: 0.0698\n",
      "Epoch [1/5], Step [586/842], Loss: 0.0379\n",
      "Epoch [1/5], Step [587/842], Loss: 0.1276\n",
      "Epoch [1/5], Step [588/842], Loss: 0.0118\n",
      "Epoch [1/5], Step [589/842], Loss: 0.0371\n",
      "Epoch [1/5], Step [590/842], Loss: 0.1504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [591/842], Loss: 0.1648\n",
      "Epoch [1/5], Step [592/842], Loss: 0.3120\n",
      "Epoch [1/5], Step [593/842], Loss: 0.2851\n",
      "Epoch [1/5], Step [594/842], Loss: 0.1563\n",
      "Epoch [1/5], Step [595/842], Loss: 0.3041\n",
      "Epoch [1/5], Step [596/842], Loss: 0.1671\n",
      "Epoch [1/5], Step [597/842], Loss: 0.3826\n",
      "Epoch [1/5], Step [598/842], Loss: 0.2883\n",
      "Epoch [1/5], Step [599/842], Loss: 0.1602\n",
      "Epoch [1/5], Step [600/842], Loss: 0.1421\n",
      "Epoch [1/5], Step [601/842], Loss: 0.4419\n",
      "Epoch [1/5], Step [602/842], Loss: 0.1432\n",
      "Epoch [1/5], Step [603/842], Loss: 0.1907\n",
      "Epoch [1/5], Step [604/842], Loss: 0.0860\n",
      "Epoch [1/5], Step [605/842], Loss: 0.1526\n",
      "Epoch [1/5], Step [606/842], Loss: 0.0679\n",
      "Epoch [1/5], Step [607/842], Loss: 0.1190\n",
      "Epoch [1/5], Step [608/842], Loss: 0.6467\n",
      "Epoch [1/5], Step [609/842], Loss: 0.0571\n",
      "Epoch [1/5], Step [610/842], Loss: 0.0076\n",
      "Epoch [1/5], Step [611/842], Loss: 0.2679\n",
      "Epoch [1/5], Step [612/842], Loss: 0.1471\n",
      "Epoch [1/5], Step [613/842], Loss: 0.1225\n",
      "Epoch [1/5], Step [614/842], Loss: 0.0917\n",
      "Epoch [1/5], Step [615/842], Loss: 0.0601\n",
      "Epoch [1/5], Step [616/842], Loss: 0.1131\n",
      "Epoch [1/5], Step [617/842], Loss: 0.0829\n",
      "Epoch [1/5], Step [618/842], Loss: 0.0904\n",
      "Epoch [1/5], Step [619/842], Loss: 0.0817\n",
      "Epoch [1/5], Step [620/842], Loss: 0.3281\n",
      "Epoch [1/5], Step [621/842], Loss: 0.3267\n",
      "Epoch [1/5], Step [622/842], Loss: 0.4639\n",
      "Epoch [1/5], Step [623/842], Loss: 0.2238\n",
      "Epoch [1/5], Step [624/842], Loss: 0.0718\n",
      "Epoch [1/5], Step [625/842], Loss: 0.1346\n",
      "Epoch [1/5], Step [626/842], Loss: 0.3130\n",
      "Epoch [1/5], Step [627/842], Loss: 0.0201\n",
      "Epoch [1/5], Step [628/842], Loss: 0.0260\n",
      "Epoch [1/5], Step [629/842], Loss: 0.1446\n",
      "Epoch [1/5], Step [630/842], Loss: 0.3810\n",
      "Epoch [1/5], Step [631/842], Loss: 0.4030\n",
      "Epoch [1/5], Step [632/842], Loss: 0.2742\n",
      "Epoch [1/5], Step [633/842], Loss: 0.0395\n",
      "Epoch [1/5], Step [634/842], Loss: 0.2545\n",
      "Epoch [1/5], Step [635/842], Loss: 0.5056\n",
      "Epoch [1/5], Step [636/842], Loss: 0.0475\n",
      "Epoch [1/5], Step [637/842], Loss: 0.2997\n",
      "Epoch [1/5], Step [638/842], Loss: 0.2121\n",
      "Epoch [1/5], Step [639/842], Loss: 0.3731\n",
      "Epoch [1/5], Step [640/842], Loss: 0.2216\n",
      "Epoch [1/5], Step [641/842], Loss: 0.0646\n",
      "Epoch [1/5], Step [642/842], Loss: 0.1905\n",
      "Epoch [1/5], Step [643/842], Loss: 0.0168\n",
      "Epoch [1/5], Step [644/842], Loss: 0.1045\n",
      "Epoch [1/5], Step [645/842], Loss: 0.2188\n",
      "Epoch [1/5], Step [646/842], Loss: 0.1020\n",
      "Epoch [1/5], Step [647/842], Loss: 0.1661\n",
      "Epoch [1/5], Step [648/842], Loss: 0.0465\n",
      "Epoch [1/5], Step [649/842], Loss: 0.0744\n",
      "Epoch [1/5], Step [650/842], Loss: 0.1257\n",
      "Epoch [1/5], Step [651/842], Loss: 0.0748\n",
      "Epoch [1/5], Step [652/842], Loss: 0.4167\n",
      "Epoch [1/5], Step [653/842], Loss: 0.3232\n",
      "Epoch [1/5], Step [654/842], Loss: 0.1618\n",
      "Epoch [1/5], Step [655/842], Loss: 0.0538\n",
      "Epoch [1/5], Step [656/842], Loss: 0.0143\n",
      "Epoch [1/5], Step [657/842], Loss: 0.0157\n",
      "Epoch [1/5], Step [658/842], Loss: 0.0191\n",
      "Epoch [1/5], Step [659/842], Loss: 0.1201\n",
      "Epoch [1/5], Step [660/842], Loss: 0.0452\n",
      "Epoch [1/5], Step [661/842], Loss: 0.1630\n",
      "Epoch [1/5], Step [662/842], Loss: 0.1358\n",
      "Epoch [1/5], Step [663/842], Loss: 0.5225\n",
      "Epoch [1/5], Step [664/842], Loss: 0.0551\n",
      "Epoch [1/5], Step [665/842], Loss: 0.1147\n",
      "Epoch [1/5], Step [666/842], Loss: 0.1921\n",
      "Epoch [1/5], Step [667/842], Loss: 0.2501\n",
      "Epoch [1/5], Step [668/842], Loss: 0.0728\n",
      "Epoch [1/5], Step [669/842], Loss: 0.1156\n",
      "Epoch [1/5], Step [670/842], Loss: 0.3492\n",
      "Epoch [1/5], Step [671/842], Loss: 0.1733\n",
      "Epoch [1/5], Step [672/842], Loss: 0.1583\n",
      "Epoch [1/5], Step [673/842], Loss: 0.2182\n",
      "Epoch [1/5], Step [674/842], Loss: 0.0325\n",
      "Epoch [1/5], Step [675/842], Loss: 0.0505\n",
      "Epoch [1/5], Step [676/842], Loss: 0.0849\n",
      "Epoch [1/5], Step [677/842], Loss: 0.1228\n",
      "Epoch [1/5], Step [678/842], Loss: 0.1145\n",
      "Epoch [1/5], Step [679/842], Loss: 0.0487\n",
      "Epoch [1/5], Step [680/842], Loss: 0.0600\n",
      "Epoch [1/5], Step [681/842], Loss: 0.3425\n",
      "Epoch [1/5], Step [682/842], Loss: 0.3135\n",
      "Epoch [1/5], Step [683/842], Loss: 0.3662\n",
      "Epoch [1/5], Step [684/842], Loss: 0.0959\n",
      "Epoch [1/5], Step [685/842], Loss: 0.2366\n",
      "Epoch [1/5], Step [686/842], Loss: 0.3916\n",
      "Epoch [1/5], Step [687/842], Loss: 0.2509\n",
      "Epoch [1/5], Step [688/842], Loss: 0.1908\n",
      "Epoch [1/5], Step [689/842], Loss: 0.0881\n",
      "Epoch [1/5], Step [690/842], Loss: 0.0579\n",
      "Epoch [1/5], Step [691/842], Loss: 0.0425\n",
      "Epoch [1/5], Step [692/842], Loss: 0.1769\n",
      "Epoch [1/5], Step [693/842], Loss: 0.2739\n",
      "Epoch [1/5], Step [694/842], Loss: 0.3482\n",
      "Epoch [1/5], Step [695/842], Loss: 0.0981\n",
      "Epoch [1/5], Step [696/842], Loss: 0.1815\n",
      "Epoch [1/5], Step [697/842], Loss: 0.0624\n",
      "Epoch [1/5], Step [698/842], Loss: 0.0949\n",
      "Epoch [1/5], Step [699/842], Loss: 0.2784\n",
      "Epoch [1/5], Step [700/842], Loss: 0.5204\n",
      "Epoch [1/5], Step [701/842], Loss: 0.1604\n",
      "Epoch [1/5], Step [702/842], Loss: 0.1818\n",
      "Epoch [1/5], Step [703/842], Loss: 0.2919\n",
      "Epoch [1/5], Step [704/842], Loss: 0.0337\n",
      "Epoch [1/5], Step [705/842], Loss: 0.2014\n",
      "Epoch [1/5], Step [706/842], Loss: 0.0419\n",
      "Epoch [1/5], Step [707/842], Loss: 0.0760\n",
      "Epoch [1/5], Step [708/842], Loss: 0.0617\n",
      "Epoch [1/5], Step [709/842], Loss: 0.0300\n",
      "Epoch [1/5], Step [710/842], Loss: 0.0645\n",
      "Epoch [1/5], Step [711/842], Loss: 0.2246\n",
      "Epoch [1/5], Step [712/842], Loss: 0.2443\n",
      "Epoch [1/5], Step [713/842], Loss: 0.0523\n",
      "Epoch [1/5], Step [714/842], Loss: 0.1817\n",
      "Epoch [1/5], Step [715/842], Loss: 0.2583\n",
      "Epoch [1/5], Step [716/842], Loss: 0.0318\n",
      "Epoch [1/5], Step [717/842], Loss: 0.1029\n",
      "Epoch [1/5], Step [718/842], Loss: 0.0845\n",
      "Epoch [1/5], Step [719/842], Loss: 0.4891\n",
      "Epoch [1/5], Step [720/842], Loss: 0.0787\n",
      "Epoch [1/5], Step [721/842], Loss: 0.0150\n",
      "Epoch [1/5], Step [722/842], Loss: 0.0065\n",
      "Epoch [1/5], Step [723/842], Loss: 0.2045\n",
      "Epoch [1/5], Step [724/842], Loss: 0.1540\n",
      "Epoch [1/5], Step [725/842], Loss: 0.1409\n",
      "Epoch [1/5], Step [726/842], Loss: 0.2175\n",
      "Epoch [1/5], Step [727/842], Loss: 0.1183\n",
      "Epoch [1/5], Step [728/842], Loss: 0.0266\n",
      "Epoch [1/5], Step [729/842], Loss: 0.3777\n",
      "Epoch [1/5], Step [730/842], Loss: 0.1963\n",
      "Epoch [1/5], Step [731/842], Loss: 0.3463\n",
      "Epoch [1/5], Step [732/842], Loss: 0.1300\n",
      "Epoch [1/5], Step [733/842], Loss: 0.0241\n",
      "Epoch [1/5], Step [734/842], Loss: 0.1565\n",
      "Epoch [1/5], Step [735/842], Loss: 0.1048\n",
      "Epoch [1/5], Step [736/842], Loss: 0.4510\n",
      "Epoch [1/5], Step [737/842], Loss: 0.3600\n",
      "Epoch [1/5], Step [738/842], Loss: 0.0664\n",
      "Epoch [1/5], Step [739/842], Loss: 0.0618\n",
      "Epoch [1/5], Step [740/842], Loss: 0.1929\n",
      "Epoch [1/5], Step [741/842], Loss: 0.0407\n",
      "Epoch [1/5], Step [742/842], Loss: 0.1608\n",
      "Epoch [1/5], Step [743/842], Loss: 0.1407\n",
      "Epoch [1/5], Step [744/842], Loss: 0.5342\n",
      "Epoch [1/5], Step [745/842], Loss: 0.0243\n",
      "Epoch [1/5], Step [746/842], Loss: 0.2327\n",
      "Epoch [1/5], Step [747/842], Loss: 0.2470\n",
      "Epoch [1/5], Step [748/842], Loss: 0.1344\n",
      "Epoch [1/5], Step [749/842], Loss: 0.0293\n",
      "Epoch [1/5], Step [750/842], Loss: 0.4446\n",
      "Epoch [1/5], Step [751/842], Loss: 0.2369\n",
      "Epoch [1/5], Step [752/842], Loss: 0.1196\n",
      "Epoch [1/5], Step [753/842], Loss: 0.4231\n",
      "Epoch [1/5], Step [754/842], Loss: 0.0468\n",
      "Epoch [1/5], Step [755/842], Loss: 0.2461\n",
      "Epoch [1/5], Step [756/842], Loss: 0.5756\n",
      "Epoch [1/5], Step [757/842], Loss: 0.1931\n",
      "Epoch [1/5], Step [758/842], Loss: 0.0389\n",
      "Epoch [1/5], Step [759/842], Loss: 0.1032\n",
      "Epoch [1/5], Step [760/842], Loss: 0.1344\n",
      "Epoch [1/5], Step [761/842], Loss: 0.1312\n",
      "Epoch [1/5], Step [762/842], Loss: 0.3599\n",
      "Epoch [1/5], Step [763/842], Loss: 0.0512\n",
      "Epoch [1/5], Step [764/842], Loss: 0.2206\n",
      "Epoch [1/5], Step [765/842], Loss: 0.0230\n",
      "Epoch [1/5], Step [766/842], Loss: 0.2352\n",
      "Epoch [1/5], Step [767/842], Loss: 0.1900\n",
      "Epoch [1/5], Step [768/842], Loss: 0.1230\n",
      "Epoch [1/5], Step [769/842], Loss: 0.3044\n",
      "Epoch [1/5], Step [770/842], Loss: 0.0549\n",
      "Epoch [1/5], Step [771/842], Loss: 0.2982\n",
      "Epoch [1/5], Step [772/842], Loss: 0.1959\n",
      "Epoch [1/5], Step [773/842], Loss: 0.0376\n",
      "Epoch [1/5], Step [774/842], Loss: 0.4148\n",
      "Epoch [1/5], Step [775/842], Loss: 0.1121\n",
      "Epoch [1/5], Step [776/842], Loss: 0.0970\n",
      "Epoch [1/5], Step [777/842], Loss: 0.1276\n",
      "Epoch [1/5], Step [778/842], Loss: 0.0237\n",
      "Epoch [1/5], Step [779/842], Loss: 0.1756\n",
      "Epoch [1/5], Step [780/842], Loss: 0.2414\n",
      "Epoch [1/5], Step [781/842], Loss: 0.0230\n",
      "Epoch [1/5], Step [782/842], Loss: 0.2571\n",
      "Epoch [1/5], Step [783/842], Loss: 0.4736\n",
      "Epoch [1/5], Step [784/842], Loss: 0.1715\n",
      "Epoch [1/5], Step [785/842], Loss: 0.4743\n",
      "Epoch [1/5], Step [786/842], Loss: 0.3781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [787/842], Loss: 0.2659\n",
      "Epoch [1/5], Step [788/842], Loss: 0.1280\n",
      "Epoch [1/5], Step [789/842], Loss: 0.1541\n",
      "Epoch [1/5], Step [790/842], Loss: 0.1233\n",
      "Epoch [1/5], Step [791/842], Loss: 0.1593\n",
      "Epoch [1/5], Step [792/842], Loss: 0.3007\n",
      "Epoch [1/5], Step [793/842], Loss: 0.1522\n",
      "Epoch [1/5], Step [794/842], Loss: 0.2443\n",
      "Epoch [1/5], Step [795/842], Loss: 0.1815\n",
      "Epoch [1/5], Step [796/842], Loss: 0.1594\n",
      "Epoch [1/5], Step [797/842], Loss: 0.2212\n",
      "Epoch [1/5], Step [798/842], Loss: 0.0936\n",
      "Epoch [1/5], Step [799/842], Loss: 0.0305\n",
      "Epoch [1/5], Step [800/842], Loss: 0.1100\n",
      "Epoch [1/5], Step [801/842], Loss: 0.0535\n",
      "Epoch [1/5], Step [802/842], Loss: 0.0158\n",
      "Epoch [1/5], Step [803/842], Loss: 0.1235\n",
      "Epoch [1/5], Step [804/842], Loss: 0.1789\n",
      "Epoch [1/5], Step [805/842], Loss: 0.2418\n",
      "Epoch [1/5], Step [806/842], Loss: 0.1440\n",
      "Epoch [1/5], Step [807/842], Loss: 0.3623\n",
      "Epoch [1/5], Step [808/842], Loss: 0.3363\n",
      "Epoch [1/5], Step [809/842], Loss: 0.3026\n",
      "Epoch [1/5], Step [810/842], Loss: 0.1566\n",
      "Epoch [1/5], Step [811/842], Loss: 0.2272\n",
      "Epoch [1/5], Step [812/842], Loss: 0.0611\n",
      "Epoch [1/5], Step [813/842], Loss: 0.2061\n",
      "Epoch [1/5], Step [814/842], Loss: 0.2899\n",
      "Epoch [1/5], Step [815/842], Loss: 0.1917\n",
      "Epoch [1/5], Step [816/842], Loss: 0.1445\n",
      "Epoch [1/5], Step [817/842], Loss: 0.0500\n",
      "Epoch [1/5], Step [818/842], Loss: 0.1535\n",
      "Epoch [1/5], Step [819/842], Loss: 0.4685\n",
      "Epoch [1/5], Step [820/842], Loss: 0.0205\n",
      "Epoch [1/5], Step [821/842], Loss: 0.0433\n",
      "Epoch [1/5], Step [822/842], Loss: 0.1395\n",
      "Epoch [1/5], Step [823/842], Loss: 0.1048\n",
      "Epoch [1/5], Step [824/842], Loss: 0.2464\n",
      "Epoch [1/5], Step [825/842], Loss: 0.0793\n",
      "Epoch [1/5], Step [826/842], Loss: 0.1723\n",
      "Epoch [1/5], Step [827/842], Loss: 0.0355\n",
      "Epoch [1/5], Step [828/842], Loss: 0.2092\n",
      "Epoch [1/5], Step [829/842], Loss: 0.1480\n",
      "Epoch [1/5], Step [830/842], Loss: 0.2605\n",
      "Epoch [1/5], Step [831/842], Loss: 0.1382\n",
      "Epoch [1/5], Step [832/842], Loss: 0.1752\n",
      "Epoch [1/5], Step [833/842], Loss: 0.0130\n",
      "Epoch [1/5], Step [834/842], Loss: 0.0795\n",
      "Epoch [1/5], Step [835/842], Loss: 0.0378\n",
      "Epoch [1/5], Step [836/842], Loss: 0.2454\n",
      "Epoch [1/5], Step [837/842], Loss: 0.1077\n",
      "Epoch [1/5], Step [838/842], Loss: 0.3257\n",
      "Epoch [1/5], Step [839/842], Loss: 0.1685\n",
      "Epoch [1/5], Step [840/842], Loss: 0.0517\n",
      "Epoch [1/5], Step [841/842], Loss: 0.1566\n",
      "Epoch [1/5], Step [842/842], Loss: 0.1645\n",
      "Epoch [1/5], Step [843/842], Loss: 0.1671\n",
      "Epoch [2/5], Step [1/842], Loss: 0.3362\n",
      "Epoch [2/5], Step [2/842], Loss: 0.1657\n",
      "Epoch [2/5], Step [3/842], Loss: 0.2519\n",
      "Epoch [2/5], Step [4/842], Loss: 0.0445\n",
      "Epoch [2/5], Step [5/842], Loss: 0.2678\n",
      "Epoch [2/5], Step [6/842], Loss: 0.1957\n",
      "Epoch [2/5], Step [7/842], Loss: 0.0186\n",
      "Epoch [2/5], Step [8/842], Loss: 0.1103\n",
      "Epoch [2/5], Step [9/842], Loss: 0.2002\n",
      "Epoch [2/5], Step [10/842], Loss: 0.0121\n",
      "Epoch [2/5], Step [11/842], Loss: 0.1498\n",
      "Epoch [2/5], Step [12/842], Loss: 0.0427\n",
      "Epoch [2/5], Step [13/842], Loss: 0.3062\n",
      "Epoch [2/5], Step [14/842], Loss: 0.0674\n",
      "Epoch [2/5], Step [15/842], Loss: 0.1547\n",
      "Epoch [2/5], Step [16/842], Loss: 0.2145\n",
      "Epoch [2/5], Step [17/842], Loss: 0.1813\n",
      "Epoch [2/5], Step [18/842], Loss: 0.1751\n",
      "Epoch [2/5], Step [19/842], Loss: 0.0192\n",
      "Epoch [2/5], Step [20/842], Loss: 0.1363\n",
      "Epoch [2/5], Step [21/842], Loss: 0.0997\n",
      "Epoch [2/5], Step [22/842], Loss: 0.1992\n",
      "Epoch [2/5], Step [23/842], Loss: 0.0691\n",
      "Epoch [2/5], Step [24/842], Loss: 0.0565\n",
      "Epoch [2/5], Step [25/842], Loss: 0.2527\n",
      "Epoch [2/5], Step [26/842], Loss: 0.0315\n",
      "Epoch [2/5], Step [27/842], Loss: 0.0235\n",
      "Epoch [2/5], Step [28/842], Loss: 0.0030\n",
      "Epoch [2/5], Step [29/842], Loss: 0.0923\n",
      "Epoch [2/5], Step [30/842], Loss: 0.0404\n",
      "Epoch [2/5], Step [31/842], Loss: 0.1055\n",
      "Epoch [2/5], Step [32/842], Loss: 0.0578\n",
      "Epoch [2/5], Step [33/842], Loss: 0.1780\n",
      "Epoch [2/5], Step [34/842], Loss: 0.0751\n",
      "Epoch [2/5], Step [35/842], Loss: 0.2336\n",
      "Epoch [2/5], Step [36/842], Loss: 0.0600\n",
      "Epoch [2/5], Step [37/842], Loss: 0.0152\n",
      "Epoch [2/5], Step [38/842], Loss: 0.0453\n",
      "Epoch [2/5], Step [39/842], Loss: 0.1869\n",
      "Epoch [2/5], Step [40/842], Loss: 0.0523\n",
      "Epoch [2/5], Step [41/842], Loss: 0.1293\n",
      "Epoch [2/5], Step [42/842], Loss: 0.2254\n",
      "Epoch [2/5], Step [43/842], Loss: 0.0320\n",
      "Epoch [2/5], Step [44/842], Loss: 0.3237\n",
      "Epoch [2/5], Step [45/842], Loss: 0.0094\n",
      "Epoch [2/5], Step [46/842], Loss: 0.0239\n",
      "Epoch [2/5], Step [47/842], Loss: 0.2997\n",
      "Epoch [2/5], Step [48/842], Loss: 0.3391\n",
      "Epoch [2/5], Step [49/842], Loss: 0.2020\n",
      "Epoch [2/5], Step [50/842], Loss: 0.0246\n",
      "Epoch [2/5], Step [51/842], Loss: 0.0245\n",
      "Epoch [2/5], Step [52/842], Loss: 0.0981\n",
      "Epoch [2/5], Step [53/842], Loss: 0.2114\n",
      "Epoch [2/5], Step [54/842], Loss: 0.0584\n",
      "Epoch [2/5], Step [55/842], Loss: 0.2512\n",
      "Epoch [2/5], Step [56/842], Loss: 0.0532\n",
      "Epoch [2/5], Step [57/842], Loss: 0.0248\n",
      "Epoch [2/5], Step [58/842], Loss: 0.2522\n",
      "Epoch [2/5], Step [59/842], Loss: 0.2644\n",
      "Epoch [2/5], Step [60/842], Loss: 0.3689\n",
      "Epoch [2/5], Step [61/842], Loss: 0.2379\n",
      "Epoch [2/5], Step [62/842], Loss: 0.0101\n",
      "Epoch [2/5], Step [63/842], Loss: 0.0080\n",
      "Epoch [2/5], Step [64/842], Loss: 0.1147\n",
      "Epoch [2/5], Step [65/842], Loss: 0.6941\n",
      "Epoch [2/5], Step [66/842], Loss: 0.0137\n",
      "Epoch [2/5], Step [67/842], Loss: 0.0305\n",
      "Epoch [2/5], Step [68/842], Loss: 0.1260\n",
      "Epoch [2/5], Step [69/842], Loss: 0.1395\n",
      "Epoch [2/5], Step [70/842], Loss: 0.0049\n",
      "Epoch [2/5], Step [71/842], Loss: 0.1110\n",
      "Epoch [2/5], Step [72/842], Loss: 0.3782\n",
      "Epoch [2/5], Step [73/842], Loss: 0.1084\n",
      "Epoch [2/5], Step [74/842], Loss: 0.0282\n",
      "Epoch [2/5], Step [75/842], Loss: 0.1223\n",
      "Epoch [2/5], Step [76/842], Loss: 0.7379\n",
      "Epoch [2/5], Step [77/842], Loss: 0.0209\n",
      "Epoch [2/5], Step [78/842], Loss: 0.3125\n",
      "Epoch [2/5], Step [79/842], Loss: 0.0496\n",
      "Epoch [2/5], Step [80/842], Loss: 0.2403\n",
      "Epoch [2/5], Step [81/842], Loss: 0.0081\n",
      "Epoch [2/5], Step [82/842], Loss: 0.2192\n",
      "Epoch [2/5], Step [83/842], Loss: 0.1259\n",
      "Epoch [2/5], Step [84/842], Loss: 0.1420\n",
      "Epoch [2/5], Step [85/842], Loss: 0.2163\n",
      "Epoch [2/5], Step [86/842], Loss: 0.0263\n",
      "Epoch [2/5], Step [87/842], Loss: 0.0470\n",
      "Epoch [2/5], Step [88/842], Loss: 0.0500\n",
      "Epoch [2/5], Step [89/842], Loss: 0.0219\n",
      "Epoch [2/5], Step [90/842], Loss: 0.2976\n",
      "Epoch [2/5], Step [91/842], Loss: 0.0854\n",
      "Epoch [2/5], Step [92/842], Loss: 0.0746\n",
      "Epoch [2/5], Step [93/842], Loss: 0.1139\n",
      "Epoch [2/5], Step [94/842], Loss: 0.1140\n",
      "Epoch [2/5], Step [95/842], Loss: 0.4287\n",
      "Epoch [2/5], Step [96/842], Loss: 0.1232\n",
      "Epoch [2/5], Step [97/842], Loss: 0.1725\n",
      "Epoch [2/5], Step [98/842], Loss: 0.0598\n",
      "Epoch [2/5], Step [99/842], Loss: 0.0316\n",
      "Epoch [2/5], Step [100/842], Loss: 0.1970\n",
      "Epoch [2/5], Step [101/842], Loss: 0.0284\n",
      "Epoch [2/5], Step [102/842], Loss: 0.0199\n",
      "Epoch [2/5], Step [103/842], Loss: 0.4289\n",
      "Epoch [2/5], Step [104/842], Loss: 0.2329\n",
      "Epoch [2/5], Step [105/842], Loss: 0.0291\n",
      "Epoch [2/5], Step [106/842], Loss: 0.3343\n",
      "Epoch [2/5], Step [107/842], Loss: 0.4352\n",
      "Epoch [2/5], Step [108/842], Loss: 0.0598\n",
      "Epoch [2/5], Step [109/842], Loss: 0.0882\n",
      "Epoch [2/5], Step [110/842], Loss: 0.2381\n",
      "Epoch [2/5], Step [111/842], Loss: 0.0684\n",
      "Epoch [2/5], Step [112/842], Loss: 0.0147\n",
      "Epoch [2/5], Step [113/842], Loss: 0.0187\n",
      "Epoch [2/5], Step [114/842], Loss: 0.3056\n",
      "Epoch [2/5], Step [115/842], Loss: 0.0685\n",
      "Epoch [2/5], Step [116/842], Loss: 0.2404\n",
      "Epoch [2/5], Step [117/842], Loss: 0.2070\n",
      "Epoch [2/5], Step [118/842], Loss: 0.0899\n",
      "Epoch [2/5], Step [119/842], Loss: 0.0957\n",
      "Epoch [2/5], Step [120/842], Loss: 0.2419\n",
      "Epoch [2/5], Step [121/842], Loss: 0.0093\n",
      "Epoch [2/5], Step [122/842], Loss: 0.0653\n",
      "Epoch [2/5], Step [123/842], Loss: 0.3030\n",
      "Epoch [2/5], Step [124/842], Loss: 0.0782\n",
      "Epoch [2/5], Step [125/842], Loss: 0.2871\n",
      "Epoch [2/5], Step [126/842], Loss: 0.4089\n",
      "Epoch [2/5], Step [127/842], Loss: 0.0707\n",
      "Epoch [2/5], Step [128/842], Loss: 0.1375\n",
      "Epoch [2/5], Step [129/842], Loss: 0.3705\n",
      "Epoch [2/5], Step [130/842], Loss: 0.1342\n",
      "Epoch [2/5], Step [131/842], Loss: 0.2923\n",
      "Epoch [2/5], Step [132/842], Loss: 0.2205\n",
      "Epoch [2/5], Step [133/842], Loss: 0.1550\n",
      "Epoch [2/5], Step [134/842], Loss: 0.0958\n",
      "Epoch [2/5], Step [135/842], Loss: 0.2009\n",
      "Epoch [2/5], Step [136/842], Loss: 0.1629\n",
      "Epoch [2/5], Step [137/842], Loss: 0.3127\n",
      "Epoch [2/5], Step [138/842], Loss: 0.1823\n",
      "Epoch [2/5], Step [139/842], Loss: 0.5720\n",
      "Epoch [2/5], Step [140/842], Loss: 0.2599\n",
      "Epoch [2/5], Step [141/842], Loss: 0.0280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [142/842], Loss: 0.0873\n",
      "Epoch [2/5], Step [143/842], Loss: 0.0812\n",
      "Epoch [2/5], Step [144/842], Loss: 0.3432\n",
      "Epoch [2/5], Step [145/842], Loss: 0.0322\n",
      "Epoch [2/5], Step [146/842], Loss: 0.2147\n",
      "Epoch [2/5], Step [147/842], Loss: 0.0041\n",
      "Epoch [2/5], Step [148/842], Loss: 0.0444\n",
      "Epoch [2/5], Step [149/842], Loss: 0.2519\n",
      "Epoch [2/5], Step [150/842], Loss: 0.4083\n",
      "Epoch [2/5], Step [151/842], Loss: 0.0900\n",
      "Epoch [2/5], Step [152/842], Loss: 0.1216\n",
      "Epoch [2/5], Step [153/842], Loss: 0.2561\n",
      "Epoch [2/5], Step [154/842], Loss: 0.1962\n",
      "Epoch [2/5], Step [155/842], Loss: 0.0054\n",
      "Epoch [2/5], Step [156/842], Loss: 0.0144\n",
      "Epoch [2/5], Step [157/842], Loss: 0.0376\n",
      "Epoch [2/5], Step [158/842], Loss: 0.2477\n",
      "Epoch [2/5], Step [159/842], Loss: 0.1033\n",
      "Epoch [2/5], Step [160/842], Loss: 0.1199\n",
      "Epoch [2/5], Step [161/842], Loss: 0.2587\n",
      "Epoch [2/5], Step [162/842], Loss: 0.0041\n",
      "Epoch [2/5], Step [163/842], Loss: 0.1267\n",
      "Epoch [2/5], Step [164/842], Loss: 0.0063\n",
      "Epoch [2/5], Step [165/842], Loss: 0.1290\n",
      "Epoch [2/5], Step [166/842], Loss: 0.1993\n",
      "Epoch [2/5], Step [167/842], Loss: 0.0091\n",
      "Epoch [2/5], Step [168/842], Loss: 0.5676\n",
      "Epoch [2/5], Step [169/842], Loss: 0.0316\n",
      "Epoch [2/5], Step [170/842], Loss: 0.0639\n",
      "Epoch [2/5], Step [171/842], Loss: 0.2461\n",
      "Epoch [2/5], Step [172/842], Loss: 0.1402\n",
      "Epoch [2/5], Step [173/842], Loss: 0.1877\n",
      "Epoch [2/5], Step [174/842], Loss: 0.0641\n",
      "Epoch [2/5], Step [175/842], Loss: 0.0260\n",
      "Epoch [2/5], Step [176/842], Loss: 0.0218\n",
      "Epoch [2/5], Step [177/842], Loss: 0.1843\n",
      "Epoch [2/5], Step [178/842], Loss: 0.0621\n",
      "Epoch [2/5], Step [179/842], Loss: 0.1368\n",
      "Epoch [2/5], Step [180/842], Loss: 0.0307\n",
      "Epoch [2/5], Step [181/842], Loss: 0.0174\n",
      "Epoch [2/5], Step [182/842], Loss: 0.2705\n",
      "Epoch [2/5], Step [183/842], Loss: 0.0240\n",
      "Epoch [2/5], Step [184/842], Loss: 0.1875\n",
      "Epoch [2/5], Step [185/842], Loss: 0.0135\n",
      "Epoch [2/5], Step [186/842], Loss: 0.0214\n",
      "Epoch [2/5], Step [187/842], Loss: 0.4344\n",
      "Epoch [2/5], Step [188/842], Loss: 0.2446\n",
      "Epoch [2/5], Step [189/842], Loss: 0.0469\n",
      "Epoch [2/5], Step [190/842], Loss: 0.1924\n",
      "Epoch [2/5], Step [191/842], Loss: 0.6276\n",
      "Epoch [2/5], Step [192/842], Loss: 0.2424\n",
      "Epoch [2/5], Step [193/842], Loss: 0.2616\n",
      "Epoch [2/5], Step [194/842], Loss: 0.0322\n",
      "Epoch [2/5], Step [195/842], Loss: 0.0369\n",
      "Epoch [2/5], Step [196/842], Loss: 0.0251\n",
      "Epoch [2/5], Step [197/842], Loss: 0.0057\n",
      "Epoch [2/5], Step [198/842], Loss: 0.3591\n",
      "Epoch [2/5], Step [199/842], Loss: 0.0629\n",
      "Epoch [2/5], Step [200/842], Loss: 0.0925\n",
      "Epoch [2/5], Step [201/842], Loss: 0.3339\n",
      "Epoch [2/5], Step [202/842], Loss: 0.0081\n",
      "Epoch [2/5], Step [203/842], Loss: 0.5235\n",
      "Epoch [2/5], Step [204/842], Loss: 0.0358\n",
      "Epoch [2/5], Step [205/842], Loss: 0.0686\n",
      "Epoch [2/5], Step [206/842], Loss: 0.1902\n",
      "Epoch [2/5], Step [207/842], Loss: 0.0511\n",
      "Epoch [2/5], Step [208/842], Loss: 0.0843\n",
      "Epoch [2/5], Step [209/842], Loss: 0.1923\n",
      "Epoch [2/5], Step [210/842], Loss: 0.1505\n",
      "Epoch [2/5], Step [211/842], Loss: 0.0351\n",
      "Epoch [2/5], Step [212/842], Loss: 0.0154\n",
      "Epoch [2/5], Step [213/842], Loss: 0.0287\n",
      "Epoch [2/5], Step [214/842], Loss: 0.2631\n",
      "Epoch [2/5], Step [215/842], Loss: 0.2983\n",
      "Epoch [2/5], Step [216/842], Loss: 0.1493\n",
      "Epoch [2/5], Step [217/842], Loss: 0.0188\n",
      "Epoch [2/5], Step [218/842], Loss: 0.2849\n",
      "Epoch [2/5], Step [219/842], Loss: 0.1872\n",
      "Epoch [2/5], Step [220/842], Loss: 0.1915\n",
      "Epoch [2/5], Step [221/842], Loss: 0.3511\n",
      "Epoch [2/5], Step [222/842], Loss: 0.0528\n",
      "Epoch [2/5], Step [223/842], Loss: 0.1340\n",
      "Epoch [2/5], Step [224/842], Loss: 0.0900\n",
      "Epoch [2/5], Step [225/842], Loss: 0.0926\n",
      "Epoch [2/5], Step [226/842], Loss: 0.0390\n",
      "Epoch [2/5], Step [227/842], Loss: 0.0378\n",
      "Epoch [2/5], Step [228/842], Loss: 0.1723\n",
      "Epoch [2/5], Step [229/842], Loss: 0.1082\n",
      "Epoch [2/5], Step [230/842], Loss: 0.2141\n",
      "Epoch [2/5], Step [231/842], Loss: 0.0426\n",
      "Epoch [2/5], Step [232/842], Loss: 0.1423\n",
      "Epoch [2/5], Step [233/842], Loss: 0.0141\n",
      "Epoch [2/5], Step [234/842], Loss: 0.3173\n",
      "Epoch [2/5], Step [235/842], Loss: 0.0482\n",
      "Epoch [2/5], Step [236/842], Loss: 0.1202\n",
      "Epoch [2/5], Step [237/842], Loss: 0.0810\n",
      "Epoch [2/5], Step [238/842], Loss: 0.0880\n",
      "Epoch [2/5], Step [239/842], Loss: 0.0269\n",
      "Epoch [2/5], Step [240/842], Loss: 0.3827\n",
      "Epoch [2/5], Step [241/842], Loss: 0.0078\n",
      "Epoch [2/5], Step [242/842], Loss: 0.0089\n",
      "Epoch [2/5], Step [243/842], Loss: 0.0730\n",
      "Epoch [2/5], Step [244/842], Loss: 0.0218\n",
      "Epoch [2/5], Step [245/842], Loss: 0.0275\n",
      "Epoch [2/5], Step [246/842], Loss: 0.0219\n",
      "Epoch [2/5], Step [247/842], Loss: 0.0120\n",
      "Epoch [2/5], Step [248/842], Loss: 0.1468\n",
      "Epoch [2/5], Step [249/842], Loss: 0.0381\n",
      "Epoch [2/5], Step [250/842], Loss: 0.0291\n",
      "Epoch [2/5], Step [251/842], Loss: 0.0289\n",
      "Epoch [2/5], Step [252/842], Loss: 0.0167\n",
      "Epoch [2/5], Step [253/842], Loss: 0.2888\n",
      "Epoch [2/5], Step [254/842], Loss: 0.1770\n",
      "Epoch [2/5], Step [255/842], Loss: 0.2133\n",
      "Epoch [2/5], Step [256/842], Loss: 0.4559\n",
      "Epoch [2/5], Step [257/842], Loss: 0.7087\n",
      "Epoch [2/5], Step [258/842], Loss: 0.0294\n",
      "Epoch [2/5], Step [259/842], Loss: 0.2254\n",
      "Epoch [2/5], Step [260/842], Loss: 0.0860\n",
      "Epoch [2/5], Step [261/842], Loss: 0.0154\n",
      "Epoch [2/5], Step [262/842], Loss: 0.1169\n",
      "Epoch [2/5], Step [263/842], Loss: 0.0949\n",
      "Epoch [2/5], Step [264/842], Loss: 0.0795\n",
      "Epoch [2/5], Step [265/842], Loss: 0.2441\n",
      "Epoch [2/5], Step [266/842], Loss: 0.2579\n",
      "Epoch [2/5], Step [267/842], Loss: 0.2523\n",
      "Epoch [2/5], Step [268/842], Loss: 0.0175\n",
      "Epoch [2/5], Step [269/842], Loss: 0.0086\n",
      "Epoch [2/5], Step [270/842], Loss: 0.0483\n",
      "Epoch [2/5], Step [271/842], Loss: 0.1231\n",
      "Epoch [2/5], Step [272/842], Loss: 0.0754\n",
      "Epoch [2/5], Step [273/842], Loss: 0.1673\n",
      "Epoch [2/5], Step [274/842], Loss: 0.0341\n",
      "Epoch [2/5], Step [275/842], Loss: 0.0785\n",
      "Epoch [2/5], Step [276/842], Loss: 0.0099\n",
      "Epoch [2/5], Step [277/842], Loss: 0.3779\n",
      "Epoch [2/5], Step [278/842], Loss: 0.0777\n",
      "Epoch [2/5], Step [279/842], Loss: 0.0217\n",
      "Epoch [2/5], Step [280/842], Loss: 0.1531\n",
      "Epoch [2/5], Step [281/842], Loss: 0.2215\n",
      "Epoch [2/5], Step [282/842], Loss: 0.0693\n",
      "Epoch [2/5], Step [283/842], Loss: 0.1787\n",
      "Epoch [2/5], Step [284/842], Loss: 0.0564\n",
      "Epoch [2/5], Step [285/842], Loss: 0.2766\n",
      "Epoch [2/5], Step [286/842], Loss: 0.0049\n",
      "Epoch [2/5], Step [287/842], Loss: 0.1946\n",
      "Epoch [2/5], Step [288/842], Loss: 0.0504\n",
      "Epoch [2/5], Step [289/842], Loss: 0.0734\n",
      "Epoch [2/5], Step [290/842], Loss: 0.1981\n",
      "Epoch [2/5], Step [291/842], Loss: 0.0398\n",
      "Epoch [2/5], Step [292/842], Loss: 0.0066\n",
      "Epoch [2/5], Step [293/842], Loss: 0.3219\n",
      "Epoch [2/5], Step [294/842], Loss: 0.2514\n",
      "Epoch [2/5], Step [295/842], Loss: 0.2619\n",
      "Epoch [2/5], Step [296/842], Loss: 0.0047\n",
      "Epoch [2/5], Step [297/842], Loss: 0.1154\n",
      "Epoch [2/5], Step [298/842], Loss: 0.0301\n",
      "Epoch [2/5], Step [299/842], Loss: 0.0963\n",
      "Epoch [2/5], Step [300/842], Loss: 0.0388\n",
      "Epoch [2/5], Step [301/842], Loss: 0.1768\n",
      "Epoch [2/5], Step [302/842], Loss: 0.0947\n",
      "Epoch [2/5], Step [303/842], Loss: 0.1410\n",
      "Epoch [2/5], Step [304/842], Loss: 0.0371\n",
      "Epoch [2/5], Step [305/842], Loss: 0.1582\n",
      "Epoch [2/5], Step [306/842], Loss: 0.3559\n",
      "Epoch [2/5], Step [307/842], Loss: 0.0693\n",
      "Epoch [2/5], Step [308/842], Loss: 0.0217\n",
      "Epoch [2/5], Step [309/842], Loss: 0.3149\n",
      "Epoch [2/5], Step [310/842], Loss: 0.1157\n",
      "Epoch [2/5], Step [311/842], Loss: 0.2427\n",
      "Epoch [2/5], Step [312/842], Loss: 0.0920\n",
      "Epoch [2/5], Step [313/842], Loss: 0.0081\n",
      "Epoch [2/5], Step [314/842], Loss: 0.0064\n",
      "Epoch [2/5], Step [315/842], Loss: 0.4136\n",
      "Epoch [2/5], Step [316/842], Loss: 0.0597\n",
      "Epoch [2/5], Step [317/842], Loss: 0.0393\n",
      "Epoch [2/5], Step [318/842], Loss: 0.0614\n",
      "Epoch [2/5], Step [319/842], Loss: 0.0646\n",
      "Epoch [2/5], Step [320/842], Loss: 0.0628\n",
      "Epoch [2/5], Step [321/842], Loss: 0.1618\n",
      "Epoch [2/5], Step [322/842], Loss: 0.2081\n",
      "Epoch [2/5], Step [323/842], Loss: 0.0185\n",
      "Epoch [2/5], Step [324/842], Loss: 0.1515\n",
      "Epoch [2/5], Step [325/842], Loss: 0.0289\n",
      "Epoch [2/5], Step [326/842], Loss: 0.0899\n",
      "Epoch [2/5], Step [327/842], Loss: 0.0157\n",
      "Epoch [2/5], Step [328/842], Loss: 0.1362\n",
      "Epoch [2/5], Step [329/842], Loss: 0.0376\n",
      "Epoch [2/5], Step [330/842], Loss: 0.0311\n",
      "Epoch [2/5], Step [331/842], Loss: 0.0683\n",
      "Epoch [2/5], Step [332/842], Loss: 0.3418\n",
      "Epoch [2/5], Step [333/842], Loss: 0.0366\n",
      "Epoch [2/5], Step [334/842], Loss: 0.0399\n",
      "Epoch [2/5], Step [335/842], Loss: 0.0368\n",
      "Epoch [2/5], Step [336/842], Loss: 0.2418\n",
      "Epoch [2/5], Step [337/842], Loss: 0.1503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [338/842], Loss: 0.2231\n",
      "Epoch [2/5], Step [339/842], Loss: 0.0892\n",
      "Epoch [2/5], Step [340/842], Loss: 0.1526\n",
      "Epoch [2/5], Step [341/842], Loss: 0.1099\n",
      "Epoch [2/5], Step [342/842], Loss: 0.2660\n",
      "Epoch [2/5], Step [343/842], Loss: 0.0174\n",
      "Epoch [2/5], Step [344/842], Loss: 0.0365\n",
      "Epoch [2/5], Step [345/842], Loss: 0.0213\n",
      "Epoch [2/5], Step [346/842], Loss: 0.2960\n",
      "Epoch [2/5], Step [347/842], Loss: 0.0371\n",
      "Epoch [2/5], Step [348/842], Loss: 0.0168\n",
      "Epoch [2/5], Step [349/842], Loss: 0.0811\n",
      "Epoch [2/5], Step [350/842], Loss: 0.0748\n",
      "Epoch [2/5], Step [351/842], Loss: 0.0467\n",
      "Epoch [2/5], Step [352/842], Loss: 0.0391\n",
      "Epoch [2/5], Step [353/842], Loss: 0.0737\n",
      "Epoch [2/5], Step [354/842], Loss: 0.1727\n",
      "Epoch [2/5], Step [355/842], Loss: 0.0077\n",
      "Epoch [2/5], Step [356/842], Loss: 0.0512\n",
      "Epoch [2/5], Step [357/842], Loss: 0.2490\n",
      "Epoch [2/5], Step [358/842], Loss: 0.1719\n",
      "Epoch [2/5], Step [359/842], Loss: 0.1239\n",
      "Epoch [2/5], Step [360/842], Loss: 0.2942\n",
      "Epoch [2/5], Step [361/842], Loss: 0.1808\n",
      "Epoch [2/5], Step [362/842], Loss: 0.1750\n",
      "Epoch [2/5], Step [363/842], Loss: 0.1219\n",
      "Epoch [2/5], Step [364/842], Loss: 0.1617\n",
      "Epoch [2/5], Step [365/842], Loss: 0.0561\n",
      "Epoch [2/5], Step [366/842], Loss: 0.0142\n",
      "Epoch [2/5], Step [367/842], Loss: 0.2180\n",
      "Epoch [2/5], Step [368/842], Loss: 0.1368\n",
      "Epoch [2/5], Step [369/842], Loss: 0.0672\n",
      "Epoch [2/5], Step [370/842], Loss: 0.7160\n",
      "Epoch [2/5], Step [371/842], Loss: 0.1459\n",
      "Epoch [2/5], Step [372/842], Loss: 0.0073\n",
      "Epoch [2/5], Step [373/842], Loss: 0.1590\n",
      "Epoch [2/5], Step [374/842], Loss: 0.0232\n",
      "Epoch [2/5], Step [375/842], Loss: 0.0525\n",
      "Epoch [2/5], Step [376/842], Loss: 0.0455\n",
      "Epoch [2/5], Step [377/842], Loss: 0.1197\n",
      "Epoch [2/5], Step [378/842], Loss: 0.2684\n",
      "Epoch [2/5], Step [379/842], Loss: 0.0069\n",
      "Epoch [2/5], Step [380/842], Loss: 0.0286\n",
      "Epoch [2/5], Step [381/842], Loss: 0.3033\n",
      "Epoch [2/5], Step [382/842], Loss: 0.0619\n",
      "Epoch [2/5], Step [383/842], Loss: 0.0544\n",
      "Epoch [2/5], Step [384/842], Loss: 0.0739\n",
      "Epoch [2/5], Step [385/842], Loss: 0.0403\n",
      "Epoch [2/5], Step [386/842], Loss: 0.1772\n",
      "Epoch [2/5], Step [387/842], Loss: 0.0083\n",
      "Epoch [2/5], Step [388/842], Loss: 0.1902\n",
      "Epoch [2/5], Step [389/842], Loss: 0.0378\n",
      "Epoch [2/5], Step [390/842], Loss: 0.1501\n",
      "Epoch [2/5], Step [391/842], Loss: 0.3052\n",
      "Epoch [2/5], Step [392/842], Loss: 0.0139\n",
      "Epoch [2/5], Step [393/842], Loss: 0.2144\n",
      "Epoch [2/5], Step [394/842], Loss: 0.0232\n",
      "Epoch [2/5], Step [395/842], Loss: 0.1962\n",
      "Epoch [2/5], Step [396/842], Loss: 0.0091\n",
      "Epoch [2/5], Step [397/842], Loss: 0.0944\n",
      "Epoch [2/5], Step [398/842], Loss: 0.0418\n",
      "Epoch [2/5], Step [399/842], Loss: 0.0288\n",
      "Epoch [2/5], Step [400/842], Loss: 0.0068\n",
      "Epoch [2/5], Step [401/842], Loss: 0.0211\n",
      "Epoch [2/5], Step [402/842], Loss: 0.3105\n",
      "Epoch [2/5], Step [403/842], Loss: 0.3627\n",
      "Epoch [2/5], Step [404/842], Loss: 0.1780\n",
      "Epoch [2/5], Step [405/842], Loss: 0.0087\n",
      "Epoch [2/5], Step [406/842], Loss: 0.1928\n",
      "Epoch [2/5], Step [407/842], Loss: 0.0222\n",
      "Epoch [2/5], Step [408/842], Loss: 0.1066\n",
      "Epoch [2/5], Step [409/842], Loss: 0.1907\n",
      "Epoch [2/5], Step [410/842], Loss: 0.0933\n",
      "Epoch [2/5], Step [411/842], Loss: 0.0292\n",
      "Epoch [2/5], Step [412/842], Loss: 0.0666\n",
      "Epoch [2/5], Step [413/842], Loss: 0.0189\n",
      "Epoch [2/5], Step [414/842], Loss: 0.0395\n",
      "Epoch [2/5], Step [415/842], Loss: 0.0995\n",
      "Epoch [2/5], Step [416/842], Loss: 0.0520\n",
      "Epoch [2/5], Step [417/842], Loss: 0.2984\n",
      "Epoch [2/5], Step [418/842], Loss: 0.0114\n",
      "Epoch [2/5], Step [419/842], Loss: 0.3382\n",
      "Epoch [2/5], Step [420/842], Loss: 0.2325\n",
      "Epoch [2/5], Step [421/842], Loss: 0.1547\n",
      "Epoch [2/5], Step [422/842], Loss: 0.1041\n",
      "Epoch [2/5], Step [423/842], Loss: 0.0402\n",
      "Epoch [2/5], Step [424/842], Loss: 0.0788\n",
      "Epoch [2/5], Step [425/842], Loss: 0.0155\n",
      "Epoch [2/5], Step [426/842], Loss: 0.0731\n",
      "Epoch [2/5], Step [427/842], Loss: 0.1821\n",
      "Epoch [2/5], Step [428/842], Loss: 0.4942\n",
      "Epoch [2/5], Step [429/842], Loss: 0.3558\n",
      "Epoch [2/5], Step [430/842], Loss: 0.1127\n",
      "Epoch [2/5], Step [431/842], Loss: 0.1053\n",
      "Epoch [2/5], Step [432/842], Loss: 0.0368\n",
      "Epoch [2/5], Step [433/842], Loss: 0.1478\n",
      "Epoch [2/5], Step [434/842], Loss: 0.2376\n",
      "Epoch [2/5], Step [435/842], Loss: 0.2791\n",
      "Epoch [2/5], Step [436/842], Loss: 0.0329\n",
      "Epoch [2/5], Step [437/842], Loss: 0.0086\n",
      "Epoch [2/5], Step [438/842], Loss: 0.4046\n",
      "Epoch [2/5], Step [439/842], Loss: 0.0702\n",
      "Epoch [2/5], Step [440/842], Loss: 0.0063\n",
      "Epoch [2/5], Step [441/842], Loss: 0.0796\n",
      "Epoch [2/5], Step [442/842], Loss: 0.4035\n",
      "Epoch [2/5], Step [443/842], Loss: 0.0142\n",
      "Epoch [2/5], Step [444/842], Loss: 0.1782\n",
      "Epoch [2/5], Step [445/842], Loss: 0.1853\n",
      "Epoch [2/5], Step [446/842], Loss: 0.0165\n",
      "Epoch [2/5], Step [447/842], Loss: 0.4027\n",
      "Epoch [2/5], Step [448/842], Loss: 0.0869\n",
      "Epoch [2/5], Step [449/842], Loss: 0.0678\n",
      "Epoch [2/5], Step [450/842], Loss: 0.1441\n",
      "Epoch [2/5], Step [451/842], Loss: 0.1594\n",
      "Epoch [2/5], Step [452/842], Loss: 0.0122\n",
      "Epoch [2/5], Step [453/842], Loss: 0.2342\n",
      "Epoch [2/5], Step [454/842], Loss: 0.0123\n",
      "Epoch [2/5], Step [455/842], Loss: 0.0252\n",
      "Epoch [2/5], Step [456/842], Loss: 0.1172\n",
      "Epoch [2/5], Step [457/842], Loss: 0.0704\n",
      "Epoch [2/5], Step [458/842], Loss: 0.0120\n",
      "Epoch [2/5], Step [459/842], Loss: 0.0363\n",
      "Epoch [2/5], Step [460/842], Loss: 0.3706\n",
      "Epoch [2/5], Step [461/842], Loss: 0.0739\n",
      "Epoch [2/5], Step [462/842], Loss: 0.1250\n",
      "Epoch [2/5], Step [463/842], Loss: 0.2190\n",
      "Epoch [2/5], Step [464/842], Loss: 0.1791\n",
      "Epoch [2/5], Step [465/842], Loss: 0.0113\n",
      "Epoch [2/5], Step [466/842], Loss: 0.0195\n",
      "Epoch [2/5], Step [467/842], Loss: 0.1284\n",
      "Epoch [2/5], Step [468/842], Loss: 0.2513\n",
      "Epoch [2/5], Step [469/842], Loss: 0.1141\n",
      "Epoch [2/5], Step [470/842], Loss: 0.0890\n",
      "Epoch [2/5], Step [471/842], Loss: 0.0150\n",
      "Epoch [2/5], Step [472/842], Loss: 0.0560\n",
      "Epoch [2/5], Step [473/842], Loss: 0.0699\n",
      "Epoch [2/5], Step [474/842], Loss: 0.2545\n",
      "Epoch [2/5], Step [475/842], Loss: 0.0296\n",
      "Epoch [2/5], Step [476/842], Loss: 0.0061\n",
      "Epoch [2/5], Step [477/842], Loss: 0.1116\n",
      "Epoch [2/5], Step [478/842], Loss: 0.0377\n",
      "Epoch [2/5], Step [479/842], Loss: 0.0907\n",
      "Epoch [2/5], Step [480/842], Loss: 0.0629\n",
      "Epoch [2/5], Step [481/842], Loss: 0.0487\n",
      "Epoch [2/5], Step [482/842], Loss: 0.1410\n",
      "Epoch [2/5], Step [483/842], Loss: 0.0367\n",
      "Epoch [2/5], Step [484/842], Loss: 0.0041\n",
      "Epoch [2/5], Step [485/842], Loss: 0.0202\n",
      "Epoch [2/5], Step [486/842], Loss: 0.0206\n",
      "Epoch [2/5], Step [487/842], Loss: 0.0354\n",
      "Epoch [2/5], Step [488/842], Loss: 0.0331\n",
      "Epoch [2/5], Step [489/842], Loss: 0.2009\n",
      "Epoch [2/5], Step [490/842], Loss: 0.0089\n",
      "Epoch [2/5], Step [491/842], Loss: 0.1287\n",
      "Epoch [2/5], Step [492/842], Loss: 0.1314\n",
      "Epoch [2/5], Step [493/842], Loss: 0.0495\n",
      "Epoch [2/5], Step [494/842], Loss: 0.0708\n",
      "Epoch [2/5], Step [495/842], Loss: 0.6932\n",
      "Epoch [2/5], Step [496/842], Loss: 0.2179\n",
      "Epoch [2/5], Step [497/842], Loss: 0.0651\n",
      "Epoch [2/5], Step [498/842], Loss: 0.0409\n",
      "Epoch [2/5], Step [499/842], Loss: 0.1880\n",
      "Epoch [2/5], Step [500/842], Loss: 0.2360\n",
      "Epoch [2/5], Step [501/842], Loss: 0.0513\n",
      "Epoch [2/5], Step [502/842], Loss: 0.2584\n",
      "Epoch [2/5], Step [503/842], Loss: 0.0537\n",
      "Epoch [2/5], Step [504/842], Loss: 0.1639\n",
      "Epoch [2/5], Step [505/842], Loss: 0.0454\n",
      "Epoch [2/5], Step [506/842], Loss: 0.1570\n",
      "Epoch [2/5], Step [507/842], Loss: 0.1782\n",
      "Epoch [2/5], Step [508/842], Loss: 0.0127\n",
      "Epoch [2/5], Step [509/842], Loss: 0.3817\n",
      "Epoch [2/5], Step [510/842], Loss: 0.0184\n",
      "Epoch [2/5], Step [511/842], Loss: 0.0225\n",
      "Epoch [2/5], Step [512/842], Loss: 0.0162\n",
      "Epoch [2/5], Step [513/842], Loss: 0.0599\n",
      "Epoch [2/5], Step [514/842], Loss: 0.0303\n",
      "Epoch [2/5], Step [515/842], Loss: 0.1702\n",
      "Epoch [2/5], Step [516/842], Loss: 0.1504\n",
      "Epoch [2/5], Step [517/842], Loss: 0.2046\n",
      "Epoch [2/5], Step [518/842], Loss: 0.2033\n",
      "Epoch [2/5], Step [519/842], Loss: 0.0212\n",
      "Epoch [2/5], Step [520/842], Loss: 0.0092\n",
      "Epoch [2/5], Step [521/842], Loss: 0.0027\n",
      "Epoch [2/5], Step [522/842], Loss: 0.0689\n",
      "Epoch [2/5], Step [523/842], Loss: 0.0086\n",
      "Epoch [2/5], Step [524/842], Loss: 0.2424\n",
      "Epoch [2/5], Step [525/842], Loss: 0.1111\n",
      "Epoch [2/5], Step [526/842], Loss: 0.3982\n",
      "Epoch [2/5], Step [527/842], Loss: 0.1638\n",
      "Epoch [2/5], Step [528/842], Loss: 0.0052\n",
      "Epoch [2/5], Step [529/842], Loss: 0.2323\n",
      "Epoch [2/5], Step [530/842], Loss: 0.0814\n",
      "Epoch [2/5], Step [531/842], Loss: 0.0079\n",
      "Epoch [2/5], Step [532/842], Loss: 0.1138\n",
      "Epoch [2/5], Step [533/842], Loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [534/842], Loss: 0.0660\n",
      "Epoch [2/5], Step [535/842], Loss: 0.2262\n",
      "Epoch [2/5], Step [536/842], Loss: 0.3355\n",
      "Epoch [2/5], Step [537/842], Loss: 0.2054\n",
      "Epoch [2/5], Step [538/842], Loss: 0.0840\n",
      "Epoch [2/5], Step [539/842], Loss: 0.0024\n",
      "Epoch [2/5], Step [540/842], Loss: 0.4149\n",
      "Epoch [2/5], Step [541/842], Loss: 0.0262\n",
      "Epoch [2/5], Step [542/842], Loss: 0.2421\n",
      "Epoch [2/5], Step [543/842], Loss: 0.1098\n",
      "Epoch [2/5], Step [544/842], Loss: 0.1564\n",
      "Epoch [2/5], Step [545/842], Loss: 0.0220\n",
      "Epoch [2/5], Step [546/842], Loss: 0.1440\n",
      "Epoch [2/5], Step [547/842], Loss: 0.1648\n",
      "Epoch [2/5], Step [548/842], Loss: 0.1800\n",
      "Epoch [2/5], Step [549/842], Loss: 0.2124\n",
      "Epoch [2/5], Step [550/842], Loss: 0.1293\n",
      "Epoch [2/5], Step [551/842], Loss: 0.4466\n",
      "Epoch [2/5], Step [552/842], Loss: 0.0320\n",
      "Epoch [2/5], Step [553/842], Loss: 0.0234\n",
      "Epoch [2/5], Step [554/842], Loss: 0.0189\n",
      "Epoch [2/5], Step [555/842], Loss: 0.0729\n",
      "Epoch [2/5], Step [556/842], Loss: 0.0911\n",
      "Epoch [2/5], Step [557/842], Loss: 0.1644\n",
      "Epoch [2/5], Step [558/842], Loss: 0.0104\n",
      "Epoch [2/5], Step [559/842], Loss: 0.1119\n",
      "Epoch [2/5], Step [560/842], Loss: 0.0404\n",
      "Epoch [2/5], Step [561/842], Loss: 0.0162\n",
      "Epoch [2/5], Step [562/842], Loss: 0.0149\n",
      "Epoch [2/5], Step [563/842], Loss: 0.0122\n",
      "Epoch [2/5], Step [564/842], Loss: 0.1587\n",
      "Epoch [2/5], Step [565/842], Loss: 0.0408\n",
      "Epoch [2/5], Step [566/842], Loss: 0.5000\n",
      "Epoch [2/5], Step [567/842], Loss: 0.1861\n",
      "Epoch [2/5], Step [568/842], Loss: 0.2099\n",
      "Epoch [2/5], Step [569/842], Loss: 0.3931\n",
      "Epoch [2/5], Step [570/842], Loss: 0.2294\n",
      "Epoch [2/5], Step [571/842], Loss: 0.0051\n",
      "Epoch [2/5], Step [572/842], Loss: 0.1791\n",
      "Epoch [2/5], Step [573/842], Loss: 0.0096\n",
      "Epoch [2/5], Step [574/842], Loss: 0.3148\n",
      "Epoch [2/5], Step [575/842], Loss: 0.1641\n",
      "Epoch [2/5], Step [576/842], Loss: 0.3490\n",
      "Epoch [2/5], Step [577/842], Loss: 0.0422\n",
      "Epoch [2/5], Step [578/842], Loss: 0.0573\n",
      "Epoch [2/5], Step [579/842], Loss: 0.2190\n",
      "Epoch [2/5], Step [580/842], Loss: 0.0735\n",
      "Epoch [2/5], Step [581/842], Loss: 0.0515\n",
      "Epoch [2/5], Step [582/842], Loss: 0.1254\n",
      "Epoch [2/5], Step [583/842], Loss: 0.1092\n",
      "Epoch [2/5], Step [584/842], Loss: 0.4443\n",
      "Epoch [2/5], Step [585/842], Loss: 0.0172\n",
      "Epoch [2/5], Step [586/842], Loss: 0.0920\n",
      "Epoch [2/5], Step [587/842], Loss: 0.2084\n",
      "Epoch [2/5], Step [588/842], Loss: 0.0838\n",
      "Epoch [2/5], Step [589/842], Loss: 0.1613\n",
      "Epoch [2/5], Step [590/842], Loss: 0.0611\n",
      "Epoch [2/5], Step [591/842], Loss: 0.1154\n",
      "Epoch [2/5], Step [592/842], Loss: 0.0441\n",
      "Epoch [2/5], Step [593/842], Loss: 0.2298\n",
      "Epoch [2/5], Step [594/842], Loss: 0.2941\n",
      "Epoch [2/5], Step [595/842], Loss: 0.1196\n",
      "Epoch [2/5], Step [596/842], Loss: 0.1101\n",
      "Epoch [2/5], Step [597/842], Loss: 0.0065\n",
      "Epoch [2/5], Step [598/842], Loss: 0.1768\n",
      "Epoch [2/5], Step [599/842], Loss: 0.0039\n",
      "Epoch [2/5], Step [600/842], Loss: 0.2139\n",
      "Epoch [2/5], Step [601/842], Loss: 0.1645\n",
      "Epoch [2/5], Step [602/842], Loss: 0.1464\n",
      "Epoch [2/5], Step [603/842], Loss: 0.1399\n",
      "Epoch [2/5], Step [604/842], Loss: 0.0162\n",
      "Epoch [2/5], Step [605/842], Loss: 0.0643\n",
      "Epoch [2/5], Step [606/842], Loss: 0.1153\n",
      "Epoch [2/5], Step [607/842], Loss: 0.1158\n",
      "Epoch [2/5], Step [608/842], Loss: 0.1490\n",
      "Epoch [2/5], Step [609/842], Loss: 0.0299\n",
      "Epoch [2/5], Step [610/842], Loss: 0.0344\n",
      "Epoch [2/5], Step [611/842], Loss: 0.0492\n",
      "Epoch [2/5], Step [612/842], Loss: 0.0082\n",
      "Epoch [2/5], Step [613/842], Loss: 0.0663\n",
      "Epoch [2/5], Step [614/842], Loss: 0.1363\n",
      "Epoch [2/5], Step [615/842], Loss: 0.2457\n",
      "Epoch [2/5], Step [616/842], Loss: 0.1404\n",
      "Epoch [2/5], Step [617/842], Loss: 0.3053\n",
      "Epoch [2/5], Step [618/842], Loss: 0.0216\n",
      "Epoch [2/5], Step [619/842], Loss: 0.2338\n",
      "Epoch [2/5], Step [620/842], Loss: 0.0146\n",
      "Epoch [2/5], Step [621/842], Loss: 0.0482\n",
      "Epoch [2/5], Step [622/842], Loss: 0.0367\n",
      "Epoch [2/5], Step [623/842], Loss: 0.0632\n",
      "Epoch [2/5], Step [624/842], Loss: 0.0607\n",
      "Epoch [2/5], Step [625/842], Loss: 0.0045\n",
      "Epoch [2/5], Step [626/842], Loss: 0.3512\n",
      "Epoch [2/5], Step [627/842], Loss: 0.0694\n",
      "Epoch [2/5], Step [628/842], Loss: 0.3043\n",
      "Epoch [2/5], Step [629/842], Loss: 0.1410\n",
      "Epoch [2/5], Step [630/842], Loss: 0.0101\n",
      "Epoch [2/5], Step [631/842], Loss: 0.0822\n",
      "Epoch [2/5], Step [632/842], Loss: 0.3594\n",
      "Epoch [2/5], Step [633/842], Loss: 0.1005\n",
      "Epoch [2/5], Step [634/842], Loss: 0.0521\n",
      "Epoch [2/5], Step [635/842], Loss: 0.0957\n",
      "Epoch [2/5], Step [636/842], Loss: 0.0354\n",
      "Epoch [2/5], Step [637/842], Loss: 0.0065\n",
      "Epoch [2/5], Step [638/842], Loss: 0.1038\n",
      "Epoch [2/5], Step [639/842], Loss: 0.0687\n",
      "Epoch [2/5], Step [640/842], Loss: 0.0610\n",
      "Epoch [2/5], Step [641/842], Loss: 0.4046\n",
      "Epoch [2/5], Step [642/842], Loss: 0.0792\n",
      "Epoch [2/5], Step [643/842], Loss: 0.1733\n",
      "Epoch [2/5], Step [644/842], Loss: 0.0739\n",
      "Epoch [2/5], Step [645/842], Loss: 0.0360\n",
      "Epoch [2/5], Step [646/842], Loss: 0.0329\n",
      "Epoch [2/5], Step [647/842], Loss: 0.0707\n",
      "Epoch [2/5], Step [648/842], Loss: 0.0625\n",
      "Epoch [2/5], Step [649/842], Loss: 0.0025\n",
      "Epoch [2/5], Step [650/842], Loss: 0.0655\n",
      "Epoch [2/5], Step [651/842], Loss: 0.3591\n",
      "Epoch [2/5], Step [652/842], Loss: 0.4567\n",
      "Epoch [2/5], Step [653/842], Loss: 0.0586\n",
      "Epoch [2/5], Step [654/842], Loss: 0.0025\n",
      "Epoch [2/5], Step [655/842], Loss: 0.0518\n",
      "Epoch [2/5], Step [656/842], Loss: 0.2610\n",
      "Epoch [2/5], Step [657/842], Loss: 0.1383\n",
      "Epoch [2/5], Step [658/842], Loss: 0.2990\n",
      "Epoch [2/5], Step [659/842], Loss: 0.0178\n",
      "Epoch [2/5], Step [660/842], Loss: 0.0615\n",
      "Epoch [2/5], Step [661/842], Loss: 0.0058\n",
      "Epoch [2/5], Step [662/842], Loss: 0.3294\n",
      "Epoch [2/5], Step [663/842], Loss: 0.3691\n",
      "Epoch [2/5], Step [664/842], Loss: 0.0042\n",
      "Epoch [2/5], Step [665/842], Loss: 0.0129\n",
      "Epoch [2/5], Step [666/842], Loss: 0.2429\n",
      "Epoch [2/5], Step [667/842], Loss: 0.0998\n",
      "Epoch [2/5], Step [668/842], Loss: 0.2294\n",
      "Epoch [2/5], Step [669/842], Loss: 0.0321\n",
      "Epoch [2/5], Step [670/842], Loss: 0.0078\n",
      "Epoch [2/5], Step [671/842], Loss: 0.0427\n",
      "Epoch [2/5], Step [672/842], Loss: 0.0123\n",
      "Epoch [2/5], Step [673/842], Loss: 0.1575\n",
      "Epoch [2/5], Step [674/842], Loss: 0.0144\n",
      "Epoch [2/5], Step [675/842], Loss: 0.1150\n",
      "Epoch [2/5], Step [676/842], Loss: 0.1156\n",
      "Epoch [2/5], Step [677/842], Loss: 0.2487\n",
      "Epoch [2/5], Step [678/842], Loss: 0.6503\n",
      "Epoch [2/5], Step [679/842], Loss: 0.0074\n",
      "Epoch [2/5], Step [680/842], Loss: 0.0270\n",
      "Epoch [2/5], Step [681/842], Loss: 0.0145\n",
      "Epoch [2/5], Step [682/842], Loss: 0.1294\n",
      "Epoch [2/5], Step [683/842], Loss: 0.0163\n",
      "Epoch [2/5], Step [684/842], Loss: 0.0838\n",
      "Epoch [2/5], Step [685/842], Loss: 0.0583\n",
      "Epoch [2/5], Step [686/842], Loss: 0.2165\n",
      "Epoch [2/5], Step [687/842], Loss: 0.2606\n",
      "Epoch [2/5], Step [688/842], Loss: 0.3271\n",
      "Epoch [2/5], Step [689/842], Loss: 0.1799\n",
      "Epoch [2/5], Step [690/842], Loss: 0.0251\n",
      "Epoch [2/5], Step [691/842], Loss: 0.1446\n",
      "Epoch [2/5], Step [692/842], Loss: 0.0481\n",
      "Epoch [2/5], Step [693/842], Loss: 0.0896\n",
      "Epoch [2/5], Step [694/842], Loss: 0.0784\n",
      "Epoch [2/5], Step [695/842], Loss: 0.1410\n",
      "Epoch [2/5], Step [696/842], Loss: 0.0738\n",
      "Epoch [2/5], Step [697/842], Loss: 0.2796\n",
      "Epoch [2/5], Step [698/842], Loss: 0.3348\n",
      "Epoch [2/5], Step [699/842], Loss: 0.0510\n",
      "Epoch [2/5], Step [700/842], Loss: 0.0517\n",
      "Epoch [2/5], Step [701/842], Loss: 0.0343\n",
      "Epoch [2/5], Step [702/842], Loss: 0.0594\n",
      "Epoch [2/5], Step [703/842], Loss: 0.0339\n",
      "Epoch [2/5], Step [704/842], Loss: 0.1918\n",
      "Epoch [2/5], Step [705/842], Loss: 0.2449\n",
      "Epoch [2/5], Step [706/842], Loss: 0.1078\n",
      "Epoch [2/5], Step [707/842], Loss: 0.1148\n",
      "Epoch [2/5], Step [708/842], Loss: 0.0195\n",
      "Epoch [2/5], Step [709/842], Loss: 0.1179\n",
      "Epoch [2/5], Step [710/842], Loss: 0.0406\n",
      "Epoch [2/5], Step [711/842], Loss: 0.2349\n",
      "Epoch [2/5], Step [712/842], Loss: 0.2923\n",
      "Epoch [2/5], Step [713/842], Loss: 0.5686\n",
      "Epoch [2/5], Step [714/842], Loss: 0.3172\n",
      "Epoch [2/5], Step [715/842], Loss: 0.3350\n",
      "Epoch [2/5], Step [716/842], Loss: 0.1664\n",
      "Epoch [2/5], Step [717/842], Loss: 0.0279\n",
      "Epoch [2/5], Step [718/842], Loss: 0.0292\n",
      "Epoch [2/5], Step [719/842], Loss: 0.1382\n",
      "Epoch [2/5], Step [720/842], Loss: 0.5654\n",
      "Epoch [2/5], Step [721/842], Loss: 0.0726\n",
      "Epoch [2/5], Step [722/842], Loss: 0.0462\n",
      "Epoch [2/5], Step [723/842], Loss: 0.3058\n",
      "Epoch [2/5], Step [724/842], Loss: 0.0980\n",
      "Epoch [2/5], Step [725/842], Loss: 0.2054\n",
      "Epoch [2/5], Step [726/842], Loss: 0.0263\n",
      "Epoch [2/5], Step [727/842], Loss: 0.1575\n",
      "Epoch [2/5], Step [728/842], Loss: 0.0700\n",
      "Epoch [2/5], Step [729/842], Loss: 0.0173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [730/842], Loss: 0.2966\n",
      "Epoch [2/5], Step [731/842], Loss: 0.1375\n",
      "Epoch [2/5], Step [732/842], Loss: 0.3337\n",
      "Epoch [2/5], Step [733/842], Loss: 0.1797\n",
      "Epoch [2/5], Step [734/842], Loss: 0.2419\n",
      "Epoch [2/5], Step [735/842], Loss: 0.0609\n",
      "Epoch [2/5], Step [736/842], Loss: 0.2525\n",
      "Epoch [2/5], Step [737/842], Loss: 0.1524\n",
      "Epoch [2/5], Step [738/842], Loss: 0.2440\n",
      "Epoch [2/5], Step [739/842], Loss: 0.0431\n",
      "Epoch [2/5], Step [740/842], Loss: 0.0485\n",
      "Epoch [2/5], Step [741/842], Loss: 0.0286\n",
      "Epoch [2/5], Step [742/842], Loss: 0.4476\n",
      "Epoch [2/5], Step [743/842], Loss: 0.0683\n",
      "Epoch [2/5], Step [744/842], Loss: 0.2501\n",
      "Epoch [2/5], Step [745/842], Loss: 0.1542\n",
      "Epoch [2/5], Step [746/842], Loss: 0.7460\n",
      "Epoch [2/5], Step [747/842], Loss: 0.2111\n",
      "Epoch [2/5], Step [748/842], Loss: 0.2173\n",
      "Epoch [2/5], Step [749/842], Loss: 0.2229\n",
      "Epoch [2/5], Step [750/842], Loss: 0.0871\n",
      "Epoch [2/5], Step [751/842], Loss: 0.1638\n",
      "Epoch [2/5], Step [752/842], Loss: 0.0865\n",
      "Epoch [2/5], Step [753/842], Loss: 0.1196\n",
      "Epoch [2/5], Step [754/842], Loss: 0.0870\n",
      "Epoch [2/5], Step [755/842], Loss: 0.2576\n",
      "Epoch [2/5], Step [756/842], Loss: 0.0703\n",
      "Epoch [2/5], Step [757/842], Loss: 0.2744\n",
      "Epoch [2/5], Step [758/842], Loss: 0.0477\n",
      "Epoch [2/5], Step [759/842], Loss: 0.0418\n",
      "Epoch [2/5], Step [760/842], Loss: 0.0113\n",
      "Epoch [2/5], Step [761/842], Loss: 0.2601\n",
      "Epoch [2/5], Step [762/842], Loss: 0.4996\n",
      "Epoch [2/5], Step [763/842], Loss: 0.0100\n",
      "Epoch [2/5], Step [764/842], Loss: 0.2264\n",
      "Epoch [2/5], Step [765/842], Loss: 0.0304\n",
      "Epoch [2/5], Step [766/842], Loss: 0.1256\n",
      "Epoch [2/5], Step [767/842], Loss: 0.0093\n",
      "Epoch [2/5], Step [768/842], Loss: 0.1699\n",
      "Epoch [2/5], Step [769/842], Loss: 0.0999\n",
      "Epoch [2/5], Step [770/842], Loss: 0.0367\n",
      "Epoch [2/5], Step [771/842], Loss: 0.0340\n",
      "Epoch [2/5], Step [772/842], Loss: 0.0438\n",
      "Epoch [2/5], Step [773/842], Loss: 0.0270\n",
      "Epoch [2/5], Step [774/842], Loss: 0.0831\n",
      "Epoch [2/5], Step [775/842], Loss: 0.0356\n",
      "Epoch [2/5], Step [776/842], Loss: 0.1745\n",
      "Epoch [2/5], Step [777/842], Loss: 0.0997\n",
      "Epoch [2/5], Step [778/842], Loss: 0.0843\n",
      "Epoch [2/5], Step [779/842], Loss: 0.3674\n",
      "Epoch [2/5], Step [780/842], Loss: 0.1891\n",
      "Epoch [2/5], Step [781/842], Loss: 0.0421\n",
      "Epoch [2/5], Step [782/842], Loss: 0.2100\n",
      "Epoch [2/5], Step [783/842], Loss: 0.1313\n",
      "Epoch [2/5], Step [784/842], Loss: 0.0437\n",
      "Epoch [2/5], Step [785/842], Loss: 0.0256\n",
      "Epoch [2/5], Step [786/842], Loss: 0.0205\n",
      "Epoch [2/5], Step [787/842], Loss: 0.0457\n",
      "Epoch [2/5], Step [788/842], Loss: 0.1299\n",
      "Epoch [2/5], Step [789/842], Loss: 0.0458\n",
      "Epoch [2/5], Step [790/842], Loss: 0.5033\n",
      "Epoch [2/5], Step [791/842], Loss: 0.2429\n",
      "Epoch [2/5], Step [792/842], Loss: 0.0038\n",
      "Epoch [2/5], Step [793/842], Loss: 0.2927\n",
      "Epoch [2/5], Step [794/842], Loss: 0.4891\n",
      "Epoch [2/5], Step [795/842], Loss: 0.1927\n",
      "Epoch [2/5], Step [796/842], Loss: 0.1526\n",
      "Epoch [2/5], Step [797/842], Loss: 0.1230\n",
      "Epoch [2/5], Step [798/842], Loss: 0.2271\n",
      "Epoch [2/5], Step [799/842], Loss: 0.1235\n",
      "Epoch [2/5], Step [800/842], Loss: 0.0547\n",
      "Epoch [2/5], Step [801/842], Loss: 0.0398\n",
      "Epoch [2/5], Step [802/842], Loss: 0.0248\n",
      "Epoch [2/5], Step [803/842], Loss: 0.2242\n",
      "Epoch [2/5], Step [804/842], Loss: 0.0911\n",
      "Epoch [2/5], Step [805/842], Loss: 0.0076\n",
      "Epoch [2/5], Step [806/842], Loss: 0.0091\n",
      "Epoch [2/5], Step [807/842], Loss: 0.0155\n",
      "Epoch [2/5], Step [808/842], Loss: 0.1201\n",
      "Epoch [2/5], Step [809/842], Loss: 0.1047\n",
      "Epoch [2/5], Step [810/842], Loss: 0.0126\n",
      "Epoch [2/5], Step [811/842], Loss: 0.0395\n",
      "Epoch [2/5], Step [812/842], Loss: 0.1380\n",
      "Epoch [2/5], Step [813/842], Loss: 0.0141\n",
      "Epoch [2/5], Step [814/842], Loss: 0.0164\n",
      "Epoch [2/5], Step [815/842], Loss: 0.0661\n",
      "Epoch [2/5], Step [816/842], Loss: 0.1637\n",
      "Epoch [2/5], Step [817/842], Loss: 0.2274\n",
      "Epoch [2/5], Step [818/842], Loss: 0.0697\n",
      "Epoch [2/5], Step [819/842], Loss: 0.2434\n",
      "Epoch [2/5], Step [820/842], Loss: 0.0344\n",
      "Epoch [2/5], Step [821/842], Loss: 0.0475\n",
      "Epoch [2/5], Step [822/842], Loss: 0.0200\n",
      "Epoch [2/5], Step [823/842], Loss: 0.0525\n",
      "Epoch [2/5], Step [824/842], Loss: 0.3670\n",
      "Epoch [2/5], Step [825/842], Loss: 0.3160\n",
      "Epoch [2/5], Step [826/842], Loss: 0.0339\n",
      "Epoch [2/5], Step [827/842], Loss: 0.0240\n",
      "Epoch [2/5], Step [828/842], Loss: 0.1359\n",
      "Epoch [2/5], Step [829/842], Loss: 0.1327\n",
      "Epoch [2/5], Step [830/842], Loss: 0.1718\n",
      "Epoch [2/5], Step [831/842], Loss: 0.1176\n",
      "Epoch [2/5], Step [832/842], Loss: 0.0393\n",
      "Epoch [2/5], Step [833/842], Loss: 0.3396\n",
      "Epoch [2/5], Step [834/842], Loss: 0.6153\n",
      "Epoch [2/5], Step [835/842], Loss: 0.0850\n",
      "Epoch [2/5], Step [836/842], Loss: 0.0356\n",
      "Epoch [2/5], Step [837/842], Loss: 0.2093\n",
      "Epoch [2/5], Step [838/842], Loss: 0.0923\n",
      "Epoch [2/5], Step [839/842], Loss: 0.0766\n",
      "Epoch [2/5], Step [840/842], Loss: 0.0904\n",
      "Epoch [2/5], Step [841/842], Loss: 0.0757\n",
      "Epoch [2/5], Step [842/842], Loss: 0.0429\n",
      "Epoch [2/5], Step [843/842], Loss: 0.0046\n",
      "Epoch [3/5], Step [1/842], Loss: 0.2923\n",
      "Epoch [3/5], Step [2/842], Loss: 0.0571\n",
      "Epoch [3/5], Step [3/842], Loss: 0.1960\n",
      "Epoch [3/5], Step [4/842], Loss: 0.3040\n",
      "Epoch [3/5], Step [5/842], Loss: 0.1952\n",
      "Epoch [3/5], Step [6/842], Loss: 0.2426\n",
      "Epoch [3/5], Step [7/842], Loss: 0.0447\n",
      "Epoch [3/5], Step [8/842], Loss: 0.1190\n",
      "Epoch [3/5], Step [9/842], Loss: 0.0237\n",
      "Epoch [3/5], Step [10/842], Loss: 0.0707\n",
      "Epoch [3/5], Step [11/842], Loss: 0.0444\n",
      "Epoch [3/5], Step [12/842], Loss: 0.3796\n",
      "Epoch [3/5], Step [13/842], Loss: 0.0557\n",
      "Epoch [3/5], Step [14/842], Loss: 0.3365\n",
      "Epoch [3/5], Step [15/842], Loss: 0.0289\n",
      "Epoch [3/5], Step [16/842], Loss: 0.0198\n",
      "Epoch [3/5], Step [17/842], Loss: 0.1074\n",
      "Epoch [3/5], Step [18/842], Loss: 0.0710\n",
      "Epoch [3/5], Step [19/842], Loss: 0.0056\n",
      "Epoch [3/5], Step [20/842], Loss: 0.1860\n",
      "Epoch [3/5], Step [21/842], Loss: 0.0330\n",
      "Epoch [3/5], Step [22/842], Loss: 0.2398\n",
      "Epoch [3/5], Step [23/842], Loss: 0.1343\n",
      "Epoch [3/5], Step [24/842], Loss: 0.0533\n",
      "Epoch [3/5], Step [25/842], Loss: 0.3174\n",
      "Epoch [3/5], Step [26/842], Loss: 0.3192\n",
      "Epoch [3/5], Step [27/842], Loss: 0.0317\n",
      "Epoch [3/5], Step [28/842], Loss: 0.0399\n",
      "Epoch [3/5], Step [29/842], Loss: 0.0144\n",
      "Epoch [3/5], Step [30/842], Loss: 0.1002\n",
      "Epoch [3/5], Step [31/842], Loss: 0.0208\n",
      "Epoch [3/5], Step [32/842], Loss: 0.1575\n",
      "Epoch [3/5], Step [33/842], Loss: 0.0583\n",
      "Epoch [3/5], Step [34/842], Loss: 0.0411\n",
      "Epoch [3/5], Step [35/842], Loss: 0.1330\n",
      "Epoch [3/5], Step [36/842], Loss: 0.0168\n",
      "Epoch [3/5], Step [37/842], Loss: 0.0150\n",
      "Epoch [3/5], Step [38/842], Loss: 0.0065\n",
      "Epoch [3/5], Step [39/842], Loss: 0.1625\n",
      "Epoch [3/5], Step [40/842], Loss: 0.2224\n",
      "Epoch [3/5], Step [41/842], Loss: 0.1692\n",
      "Epoch [3/5], Step [42/842], Loss: 0.1918\n",
      "Epoch [3/5], Step [43/842], Loss: 0.0038\n",
      "Epoch [3/5], Step [44/842], Loss: 0.1559\n",
      "Epoch [3/5], Step [45/842], Loss: 0.1622\n",
      "Epoch [3/5], Step [46/842], Loss: 0.1787\n",
      "Epoch [3/5], Step [47/842], Loss: 0.1155\n",
      "Epoch [3/5], Step [48/842], Loss: 0.0192\n",
      "Epoch [3/5], Step [49/842], Loss: 0.0438\n",
      "Epoch [3/5], Step [50/842], Loss: 0.2031\n",
      "Epoch [3/5], Step [51/842], Loss: 0.0250\n",
      "Epoch [3/5], Step [52/842], Loss: 0.0314\n",
      "Epoch [3/5], Step [53/842], Loss: 0.0424\n",
      "Epoch [3/5], Step [54/842], Loss: 0.0436\n",
      "Epoch [3/5], Step [55/842], Loss: 0.1441\n",
      "Epoch [3/5], Step [56/842], Loss: 0.1822\n",
      "Epoch [3/5], Step [57/842], Loss: 0.0939\n",
      "Epoch [3/5], Step [58/842], Loss: 0.0850\n",
      "Epoch [3/5], Step [59/842], Loss: 0.0061\n",
      "Epoch [3/5], Step [60/842], Loss: 0.0241\n",
      "Epoch [3/5], Step [61/842], Loss: 0.1387\n",
      "Epoch [3/5], Step [62/842], Loss: 0.0774\n",
      "Epoch [3/5], Step [63/842], Loss: 0.1234\n",
      "Epoch [3/5], Step [64/842], Loss: 0.0966\n",
      "Epoch [3/5], Step [65/842], Loss: 0.0117\n",
      "Epoch [3/5], Step [66/842], Loss: 0.0058\n",
      "Epoch [3/5], Step [67/842], Loss: 0.0053\n",
      "Epoch [3/5], Step [68/842], Loss: 0.1318\n",
      "Epoch [3/5], Step [69/842], Loss: 0.0318\n",
      "Epoch [3/5], Step [70/842], Loss: 0.0154\n",
      "Epoch [3/5], Step [71/842], Loss: 0.0079\n",
      "Epoch [3/5], Step [72/842], Loss: 0.0087\n",
      "Epoch [3/5], Step [73/842], Loss: 0.0146\n",
      "Epoch [3/5], Step [74/842], Loss: 0.0033\n",
      "Epoch [3/5], Step [75/842], Loss: 0.0085\n",
      "Epoch [3/5], Step [76/842], Loss: 0.0475\n",
      "Epoch [3/5], Step [77/842], Loss: 0.0142\n",
      "Epoch [3/5], Step [78/842], Loss: 0.1822\n",
      "Epoch [3/5], Step [79/842], Loss: 0.0628\n",
      "Epoch [3/5], Step [80/842], Loss: 0.1878\n",
      "Epoch [3/5], Step [81/842], Loss: 0.1024\n",
      "Epoch [3/5], Step [82/842], Loss: 0.1009\n",
      "Epoch [3/5], Step [83/842], Loss: 0.2316\n",
      "Epoch [3/5], Step [84/842], Loss: 0.0354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [85/842], Loss: 0.0830\n",
      "Epoch [3/5], Step [86/842], Loss: 0.0872\n",
      "Epoch [3/5], Step [87/842], Loss: 0.0023\n",
      "Epoch [3/5], Step [88/842], Loss: 0.1276\n",
      "Epoch [3/5], Step [89/842], Loss: 0.1078\n",
      "Epoch [3/5], Step [90/842], Loss: 0.0928\n",
      "Epoch [3/5], Step [91/842], Loss: 0.0626\n",
      "Epoch [3/5], Step [92/842], Loss: 0.0171\n",
      "Epoch [3/5], Step [93/842], Loss: 0.0248\n",
      "Epoch [3/5], Step [94/842], Loss: 0.0044\n",
      "Epoch [3/5], Step [95/842], Loss: 0.0548\n",
      "Epoch [3/5], Step [96/842], Loss: 0.2111\n",
      "Epoch [3/5], Step [97/842], Loss: 0.0187\n",
      "Epoch [3/5], Step [98/842], Loss: 0.0039\n",
      "Epoch [3/5], Step [99/842], Loss: 0.3649\n",
      "Epoch [3/5], Step [100/842], Loss: 0.1759\n",
      "Epoch [3/5], Step [101/842], Loss: 0.0355\n",
      "Epoch [3/5], Step [102/842], Loss: 0.1004\n",
      "Epoch [3/5], Step [103/842], Loss: 0.0515\n",
      "Epoch [3/5], Step [104/842], Loss: 0.0073\n",
      "Epoch [3/5], Step [105/842], Loss: 0.0183\n",
      "Epoch [3/5], Step [106/842], Loss: 0.1176\n",
      "Epoch [3/5], Step [107/842], Loss: 0.1102\n",
      "Epoch [3/5], Step [108/842], Loss: 0.0791\n",
      "Epoch [3/5], Step [109/842], Loss: 0.1400\n",
      "Epoch [3/5], Step [110/842], Loss: 0.1400\n",
      "Epoch [3/5], Step [111/842], Loss: 0.0658\n",
      "Epoch [3/5], Step [112/842], Loss: 0.0161\n",
      "Epoch [3/5], Step [113/842], Loss: 0.0220\n",
      "Epoch [3/5], Step [114/842], Loss: 0.0153\n",
      "Epoch [3/5], Step [115/842], Loss: 0.1902\n",
      "Epoch [3/5], Step [116/842], Loss: 0.0873\n",
      "Epoch [3/5], Step [117/842], Loss: 0.0441\n",
      "Epoch [3/5], Step [118/842], Loss: 0.4178\n",
      "Epoch [3/5], Step [119/842], Loss: 0.0813\n",
      "Epoch [3/5], Step [120/842], Loss: 0.0248\n",
      "Epoch [3/5], Step [121/842], Loss: 0.0951\n",
      "Epoch [3/5], Step [122/842], Loss: 0.0629\n",
      "Epoch [3/5], Step [123/842], Loss: 0.0353\n",
      "Epoch [3/5], Step [124/842], Loss: 0.0148\n",
      "Epoch [3/5], Step [125/842], Loss: 0.0229\n",
      "Epoch [3/5], Step [126/842], Loss: 0.0088\n",
      "Epoch [3/5], Step [127/842], Loss: 0.2309\n",
      "Epoch [3/5], Step [128/842], Loss: 0.0169\n",
      "Epoch [3/5], Step [129/842], Loss: 0.2678\n",
      "Epoch [3/5], Step [130/842], Loss: 0.0202\n",
      "Epoch [3/5], Step [131/842], Loss: 0.0241\n",
      "Epoch [3/5], Step [132/842], Loss: 0.0832\n",
      "Epoch [3/5], Step [133/842], Loss: 0.0731\n",
      "Epoch [3/5], Step [134/842], Loss: 0.0036\n",
      "Epoch [3/5], Step [135/842], Loss: 0.0235\n",
      "Epoch [3/5], Step [136/842], Loss: 0.0313\n",
      "Epoch [3/5], Step [137/842], Loss: 0.1022\n",
      "Epoch [3/5], Step [138/842], Loss: 0.3310\n",
      "Epoch [3/5], Step [139/842], Loss: 0.0131\n",
      "Epoch [3/5], Step [140/842], Loss: 0.0507\n",
      "Epoch [3/5], Step [141/842], Loss: 0.2854\n",
      "Epoch [3/5], Step [142/842], Loss: 0.2009\n",
      "Epoch [3/5], Step [143/842], Loss: 0.3763\n",
      "Epoch [3/5], Step [144/842], Loss: 0.0053\n",
      "Epoch [3/5], Step [145/842], Loss: 0.0067\n",
      "Epoch [3/5], Step [146/842], Loss: 0.0170\n",
      "Epoch [3/5], Step [147/842], Loss: 0.0184\n",
      "Epoch [3/5], Step [148/842], Loss: 0.1264\n",
      "Epoch [3/5], Step [149/842], Loss: 0.0637\n",
      "Epoch [3/5], Step [150/842], Loss: 0.0424\n",
      "Epoch [3/5], Step [151/842], Loss: 0.0050\n",
      "Epoch [3/5], Step [152/842], Loss: 0.0809\n",
      "Epoch [3/5], Step [153/842], Loss: 0.0146\n",
      "Epoch [3/5], Step [154/842], Loss: 0.1714\n",
      "Epoch [3/5], Step [155/842], Loss: 0.0553\n",
      "Epoch [3/5], Step [156/842], Loss: 0.1682\n",
      "Epoch [3/5], Step [157/842], Loss: 0.0407\n",
      "Epoch [3/5], Step [158/842], Loss: 0.0274\n",
      "Epoch [3/5], Step [159/842], Loss: 0.2488\n",
      "Epoch [3/5], Step [160/842], Loss: 0.0120\n",
      "Epoch [3/5], Step [161/842], Loss: 0.0089\n",
      "Epoch [3/5], Step [162/842], Loss: 0.0335\n",
      "Epoch [3/5], Step [163/842], Loss: 0.0069\n",
      "Epoch [3/5], Step [164/842], Loss: 0.1165\n",
      "Epoch [3/5], Step [165/842], Loss: 0.0103\n",
      "Epoch [3/5], Step [166/842], Loss: 0.2530\n",
      "Epoch [3/5], Step [167/842], Loss: 0.0113\n",
      "Epoch [3/5], Step [168/842], Loss: 0.1323\n",
      "Epoch [3/5], Step [169/842], Loss: 0.0059\n",
      "Epoch [3/5], Step [170/842], Loss: 0.0636\n",
      "Epoch [3/5], Step [171/842], Loss: 0.0085\n",
      "Epoch [3/5], Step [172/842], Loss: 0.0106\n",
      "Epoch [3/5], Step [173/842], Loss: 0.0687\n",
      "Epoch [3/5], Step [174/842], Loss: 0.0233\n",
      "Epoch [3/5], Step [175/842], Loss: 0.0214\n",
      "Epoch [3/5], Step [176/842], Loss: 0.2199\n",
      "Epoch [3/5], Step [177/842], Loss: 0.1145\n",
      "Epoch [3/5], Step [178/842], Loss: 0.0618\n",
      "Epoch [3/5], Step [179/842], Loss: 0.0172\n",
      "Epoch [3/5], Step [180/842], Loss: 0.0690\n",
      "Epoch [3/5], Step [181/842], Loss: 0.0132\n",
      "Epoch [3/5], Step [182/842], Loss: 0.1603\n",
      "Epoch [3/5], Step [183/842], Loss: 0.2085\n",
      "Epoch [3/5], Step [184/842], Loss: 0.3019\n",
      "Epoch [3/5], Step [185/842], Loss: 0.0789\n",
      "Epoch [3/5], Step [186/842], Loss: 0.1491\n",
      "Epoch [3/5], Step [187/842], Loss: 0.0355\n",
      "Epoch [3/5], Step [188/842], Loss: 0.1988\n",
      "Epoch [3/5], Step [189/842], Loss: 0.2173\n",
      "Epoch [3/5], Step [190/842], Loss: 0.1816\n",
      "Epoch [3/5], Step [191/842], Loss: 0.0670\n",
      "Epoch [3/5], Step [192/842], Loss: 0.0464\n",
      "Epoch [3/5], Step [193/842], Loss: 0.1102\n",
      "Epoch [3/5], Step [194/842], Loss: 0.0063\n",
      "Epoch [3/5], Step [195/842], Loss: 0.0119\n",
      "Epoch [3/5], Step [196/842], Loss: 0.0081\n",
      "Epoch [3/5], Step [197/842], Loss: 0.0445\n",
      "Epoch [3/5], Step [198/842], Loss: 0.1140\n",
      "Epoch [3/5], Step [199/842], Loss: 0.0891\n",
      "Epoch [3/5], Step [200/842], Loss: 0.0064\n",
      "Epoch [3/5], Step [201/842], Loss: 0.0274\n",
      "Epoch [3/5], Step [202/842], Loss: 0.0435\n",
      "Epoch [3/5], Step [203/842], Loss: 0.0428\n",
      "Epoch [3/5], Step [204/842], Loss: 0.0290\n",
      "Epoch [3/5], Step [205/842], Loss: 0.0954\n",
      "Epoch [3/5], Step [206/842], Loss: 0.1314\n",
      "Epoch [3/5], Step [207/842], Loss: 0.0067\n",
      "Epoch [3/5], Step [208/842], Loss: 0.0291\n",
      "Epoch [3/5], Step [209/842], Loss: 0.1041\n",
      "Epoch [3/5], Step [210/842], Loss: 0.1675\n",
      "Epoch [3/5], Step [211/842], Loss: 0.0990\n",
      "Epoch [3/5], Step [212/842], Loss: 0.0152\n",
      "Epoch [3/5], Step [213/842], Loss: 0.0013\n",
      "Epoch [3/5], Step [214/842], Loss: 0.0180\n",
      "Epoch [3/5], Step [215/842], Loss: 0.0076\n",
      "Epoch [3/5], Step [216/842], Loss: 0.0403\n",
      "Epoch [3/5], Step [217/842], Loss: 0.0603\n",
      "Epoch [3/5], Step [218/842], Loss: 0.0134\n",
      "Epoch [3/5], Step [219/842], Loss: 0.0026\n",
      "Epoch [3/5], Step [220/842], Loss: 0.1215\n",
      "Epoch [3/5], Step [221/842], Loss: 0.0033\n",
      "Epoch [3/5], Step [222/842], Loss: 0.0549\n",
      "Epoch [3/5], Step [223/842], Loss: 0.0267\n",
      "Epoch [3/5], Step [224/842], Loss: 0.0776\n",
      "Epoch [3/5], Step [225/842], Loss: 0.0599\n",
      "Epoch [3/5], Step [226/842], Loss: 0.1789\n",
      "Epoch [3/5], Step [227/842], Loss: 0.1980\n",
      "Epoch [3/5], Step [228/842], Loss: 0.0152\n",
      "Epoch [3/5], Step [229/842], Loss: 0.0636\n",
      "Epoch [3/5], Step [230/842], Loss: 0.0029\n",
      "Epoch [3/5], Step [231/842], Loss: 0.2024\n",
      "Epoch [3/5], Step [232/842], Loss: 0.1264\n",
      "Epoch [3/5], Step [233/842], Loss: 0.0190\n",
      "Epoch [3/5], Step [234/842], Loss: 0.1622\n",
      "Epoch [3/5], Step [235/842], Loss: 0.0102\n",
      "Epoch [3/5], Step [236/842], Loss: 0.0038\n",
      "Epoch [3/5], Step [237/842], Loss: 0.0429\n",
      "Epoch [3/5], Step [238/842], Loss: 0.0067\n",
      "Epoch [3/5], Step [239/842], Loss: 0.0837\n",
      "Epoch [3/5], Step [240/842], Loss: 0.0346\n",
      "Epoch [3/5], Step [241/842], Loss: 0.1243\n",
      "Epoch [3/5], Step [242/842], Loss: 0.0022\n",
      "Epoch [3/5], Step [243/842], Loss: 0.1117\n",
      "Epoch [3/5], Step [244/842], Loss: 0.0471\n",
      "Epoch [3/5], Step [245/842], Loss: 0.0581\n",
      "Epoch [3/5], Step [246/842], Loss: 0.0100\n",
      "Epoch [3/5], Step [247/842], Loss: 0.0145\n",
      "Epoch [3/5], Step [248/842], Loss: 0.0058\n",
      "Epoch [3/5], Step [249/842], Loss: 0.1431\n",
      "Epoch [3/5], Step [250/842], Loss: 0.1478\n",
      "Epoch [3/5], Step [251/842], Loss: 0.0088\n",
      "Epoch [3/5], Step [252/842], Loss: 0.3061\n",
      "Epoch [3/5], Step [253/842], Loss: 0.2573\n",
      "Epoch [3/5], Step [254/842], Loss: 0.3147\n",
      "Epoch [3/5], Step [255/842], Loss: 0.1134\n",
      "Epoch [3/5], Step [256/842], Loss: 0.0159\n",
      "Epoch [3/5], Step [257/842], Loss: 0.0939\n",
      "Epoch [3/5], Step [258/842], Loss: 0.0441\n",
      "Epoch [3/5], Step [259/842], Loss: 0.0630\n",
      "Epoch [3/5], Step [260/842], Loss: 0.0436\n",
      "Epoch [3/5], Step [261/842], Loss: 0.1660\n",
      "Epoch [3/5], Step [262/842], Loss: 0.0225\n",
      "Epoch [3/5], Step [263/842], Loss: 0.0161\n",
      "Epoch [3/5], Step [264/842], Loss: 0.1560\n",
      "Epoch [3/5], Step [265/842], Loss: 0.1706\n",
      "Epoch [3/5], Step [266/842], Loss: 0.0264\n",
      "Epoch [3/5], Step [267/842], Loss: 0.0088\n",
      "Epoch [3/5], Step [268/842], Loss: 0.1156\n",
      "Epoch [3/5], Step [269/842], Loss: 0.0084\n",
      "Epoch [3/5], Step [270/842], Loss: 0.0270\n",
      "Epoch [3/5], Step [271/842], Loss: 0.0054\n",
      "Epoch [3/5], Step [272/842], Loss: 0.1536\n",
      "Epoch [3/5], Step [273/842], Loss: 0.1735\n",
      "Epoch [3/5], Step [274/842], Loss: 0.0570\n",
      "Epoch [3/5], Step [275/842], Loss: 0.1307\n",
      "Epoch [3/5], Step [276/842], Loss: 0.0577\n",
      "Epoch [3/5], Step [277/842], Loss: 0.1144\n",
      "Epoch [3/5], Step [278/842], Loss: 0.3192\n",
      "Epoch [3/5], Step [279/842], Loss: 0.3776\n",
      "Epoch [3/5], Step [280/842], Loss: 0.2776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [281/842], Loss: 0.0209\n",
      "Epoch [3/5], Step [282/842], Loss: 0.3778\n",
      "Epoch [3/5], Step [283/842], Loss: 0.0790\n",
      "Epoch [3/5], Step [284/842], Loss: 0.0482\n",
      "Epoch [3/5], Step [285/842], Loss: 0.1261\n",
      "Epoch [3/5], Step [286/842], Loss: 0.0056\n",
      "Epoch [3/5], Step [287/842], Loss: 0.0869\n",
      "Epoch [3/5], Step [288/842], Loss: 0.3328\n",
      "Epoch [3/5], Step [289/842], Loss: 0.0414\n",
      "Epoch [3/5], Step [290/842], Loss: 0.0891\n",
      "Epoch [3/5], Step [291/842], Loss: 0.0887\n",
      "Epoch [3/5], Step [292/842], Loss: 0.1328\n",
      "Epoch [3/5], Step [293/842], Loss: 0.1189\n",
      "Epoch [3/5], Step [294/842], Loss: 0.0769\n",
      "Epoch [3/5], Step [295/842], Loss: 0.0532\n",
      "Epoch [3/5], Step [296/842], Loss: 0.0285\n",
      "Epoch [3/5], Step [297/842], Loss: 0.0173\n",
      "Epoch [3/5], Step [298/842], Loss: 0.0638\n",
      "Epoch [3/5], Step [299/842], Loss: 0.0382\n",
      "Epoch [3/5], Step [300/842], Loss: 0.6376\n",
      "Epoch [3/5], Step [301/842], Loss: 0.0614\n",
      "Epoch [3/5], Step [302/842], Loss: 0.1226\n",
      "Epoch [3/5], Step [303/842], Loss: 0.0407\n",
      "Epoch [3/5], Step [304/842], Loss: 0.0487\n",
      "Epoch [3/5], Step [305/842], Loss: 0.1537\n",
      "Epoch [3/5], Step [306/842], Loss: 0.0186\n",
      "Epoch [3/5], Step [307/842], Loss: 0.2161\n",
      "Epoch [3/5], Step [308/842], Loss: 0.2252\n",
      "Epoch [3/5], Step [309/842], Loss: 0.0576\n",
      "Epoch [3/5], Step [310/842], Loss: 0.0918\n",
      "Epoch [3/5], Step [311/842], Loss: 0.3948\n",
      "Epoch [3/5], Step [312/842], Loss: 0.1243\n",
      "Epoch [3/5], Step [313/842], Loss: 0.0537\n",
      "Epoch [3/5], Step [314/842], Loss: 0.1596\n",
      "Epoch [3/5], Step [315/842], Loss: 0.0840\n",
      "Epoch [3/5], Step [316/842], Loss: 0.1459\n",
      "Epoch [3/5], Step [317/842], Loss: 0.0220\n",
      "Epoch [3/5], Step [318/842], Loss: 0.1731\n",
      "Epoch [3/5], Step [319/842], Loss: 0.0175\n",
      "Epoch [3/5], Step [320/842], Loss: 0.0062\n",
      "Epoch [3/5], Step [321/842], Loss: 0.0144\n",
      "Epoch [3/5], Step [322/842], Loss: 0.0961\n",
      "Epoch [3/5], Step [323/842], Loss: 0.1476\n",
      "Epoch [3/5], Step [324/842], Loss: 0.0458\n",
      "Epoch [3/5], Step [325/842], Loss: 0.0503\n",
      "Epoch [3/5], Step [326/842], Loss: 0.0143\n",
      "Epoch [3/5], Step [327/842], Loss: 0.0777\n",
      "Epoch [3/5], Step [328/842], Loss: 0.0054\n",
      "Epoch [3/5], Step [329/842], Loss: 0.3066\n",
      "Epoch [3/5], Step [330/842], Loss: 0.1605\n",
      "Epoch [3/5], Step [331/842], Loss: 0.0935\n",
      "Epoch [3/5], Step [332/842], Loss: 0.2333\n",
      "Epoch [3/5], Step [333/842], Loss: 0.2761\n",
      "Epoch [3/5], Step [334/842], Loss: 0.0394\n",
      "Epoch [3/5], Step [335/842], Loss: 0.0076\n",
      "Epoch [3/5], Step [336/842], Loss: 0.1608\n",
      "Epoch [3/5], Step [337/842], Loss: 0.0324\n",
      "Epoch [3/5], Step [338/842], Loss: 0.0408\n",
      "Epoch [3/5], Step [339/842], Loss: 0.0075\n",
      "Epoch [3/5], Step [340/842], Loss: 0.2020\n",
      "Epoch [3/5], Step [341/842], Loss: 0.0232\n",
      "Epoch [3/5], Step [342/842], Loss: 0.1838\n",
      "Epoch [3/5], Step [343/842], Loss: 0.0083\n",
      "Epoch [3/5], Step [344/842], Loss: 0.0116\n",
      "Epoch [3/5], Step [345/842], Loss: 0.0544\n",
      "Epoch [3/5], Step [346/842], Loss: 0.1166\n",
      "Epoch [3/5], Step [347/842], Loss: 0.1602\n",
      "Epoch [3/5], Step [348/842], Loss: 0.1993\n",
      "Epoch [3/5], Step [349/842], Loss: 0.0984\n",
      "Epoch [3/5], Step [350/842], Loss: 0.1738\n",
      "Epoch [3/5], Step [351/842], Loss: 0.1062\n",
      "Epoch [3/5], Step [352/842], Loss: 0.0056\n",
      "Epoch [3/5], Step [353/842], Loss: 0.0961\n",
      "Epoch [3/5], Step [354/842], Loss: 0.0271\n",
      "Epoch [3/5], Step [355/842], Loss: 0.0031\n",
      "Epoch [3/5], Step [356/842], Loss: 0.0346\n",
      "Epoch [3/5], Step [357/842], Loss: 0.0243\n",
      "Epoch [3/5], Step [358/842], Loss: 0.1673\n",
      "Epoch [3/5], Step [359/842], Loss: 0.0056\n",
      "Epoch [3/5], Step [360/842], Loss: 0.0030\n",
      "Epoch [3/5], Step [361/842], Loss: 0.0646\n",
      "Epoch [3/5], Step [362/842], Loss: 0.0804\n",
      "Epoch [3/5], Step [363/842], Loss: 0.0052\n",
      "Epoch [3/5], Step [364/842], Loss: 0.0420\n",
      "Epoch [3/5], Step [365/842], Loss: 0.0860\n",
      "Epoch [3/5], Step [366/842], Loss: 0.0443\n",
      "Epoch [3/5], Step [367/842], Loss: 0.1518\n",
      "Epoch [3/5], Step [368/842], Loss: 0.1385\n",
      "Epoch [3/5], Step [369/842], Loss: 0.0974\n",
      "Epoch [3/5], Step [370/842], Loss: 0.1608\n",
      "Epoch [3/5], Step [371/842], Loss: 0.0487\n",
      "Epoch [3/5], Step [372/842], Loss: 0.0585\n",
      "Epoch [3/5], Step [373/842], Loss: 0.0014\n",
      "Epoch [3/5], Step [374/842], Loss: 0.0746\n",
      "Epoch [3/5], Step [375/842], Loss: 0.0083\n",
      "Epoch [3/5], Step [376/842], Loss: 0.0219\n",
      "Epoch [3/5], Step [377/842], Loss: 0.2255\n",
      "Epoch [3/5], Step [378/842], Loss: 0.0390\n",
      "Epoch [3/5], Step [379/842], Loss: 0.0093\n",
      "Epoch [3/5], Step [380/842], Loss: 0.0059\n",
      "Epoch [3/5], Step [381/842], Loss: 0.0050\n",
      "Epoch [3/5], Step [382/842], Loss: 0.0458\n",
      "Epoch [3/5], Step [383/842], Loss: 0.0237\n",
      "Epoch [3/5], Step [384/842], Loss: 0.0061\n",
      "Epoch [3/5], Step [385/842], Loss: 0.2394\n",
      "Epoch [3/5], Step [386/842], Loss: 0.2698\n",
      "Epoch [3/5], Step [387/842], Loss: 0.0059\n",
      "Epoch [3/5], Step [388/842], Loss: 0.0515\n",
      "Epoch [3/5], Step [389/842], Loss: 0.0607\n",
      "Epoch [3/5], Step [390/842], Loss: 0.1315\n",
      "Epoch [3/5], Step [391/842], Loss: 0.0028\n",
      "Epoch [3/5], Step [392/842], Loss: 0.0390\n",
      "Epoch [3/5], Step [393/842], Loss: 0.0190\n",
      "Epoch [3/5], Step [394/842], Loss: 0.0295\n",
      "Epoch [3/5], Step [395/842], Loss: 0.0184\n",
      "Epoch [3/5], Step [396/842], Loss: 0.0593\n",
      "Epoch [3/5], Step [397/842], Loss: 0.0421\n",
      "Epoch [3/5], Step [398/842], Loss: 0.0131\n",
      "Epoch [3/5], Step [399/842], Loss: 0.1309\n",
      "Epoch [3/5], Step [400/842], Loss: 0.0624\n",
      "Epoch [3/5], Step [401/842], Loss: 0.2746\n",
      "Epoch [3/5], Step [402/842], Loss: 0.0510\n",
      "Epoch [3/5], Step [403/842], Loss: 0.0186\n",
      "Epoch [3/5], Step [404/842], Loss: 0.0105\n",
      "Epoch [3/5], Step [405/842], Loss: 0.0165\n",
      "Epoch [3/5], Step [406/842], Loss: 0.0063\n",
      "Epoch [3/5], Step [407/842], Loss: 0.0010\n",
      "Epoch [3/5], Step [408/842], Loss: 0.1566\n",
      "Epoch [3/5], Step [409/842], Loss: 0.0025\n",
      "Epoch [3/5], Step [410/842], Loss: 0.0704\n",
      "Epoch [3/5], Step [411/842], Loss: 0.0158\n",
      "Epoch [3/5], Step [412/842], Loss: 0.0455\n",
      "Epoch [3/5], Step [413/842], Loss: 0.2791\n",
      "Epoch [3/5], Step [414/842], Loss: 0.0797\n",
      "Epoch [3/5], Step [415/842], Loss: 0.0121\n",
      "Epoch [3/5], Step [416/842], Loss: 0.0291\n",
      "Epoch [3/5], Step [417/842], Loss: 0.0977\n",
      "Epoch [3/5], Step [418/842], Loss: 0.0633\n",
      "Epoch [3/5], Step [419/842], Loss: 0.0150\n",
      "Epoch [3/5], Step [420/842], Loss: 0.0197\n",
      "Epoch [3/5], Step [421/842], Loss: 0.0135\n",
      "Epoch [3/5], Step [422/842], Loss: 0.1022\n",
      "Epoch [3/5], Step [423/842], Loss: 0.0216\n",
      "Epoch [3/5], Step [424/842], Loss: 0.0097\n",
      "Epoch [3/5], Step [425/842], Loss: 0.0549\n",
      "Epoch [3/5], Step [426/842], Loss: 0.0936\n",
      "Epoch [3/5], Step [427/842], Loss: 0.0084\n",
      "Epoch [3/5], Step [428/842], Loss: 0.0088\n",
      "Epoch [3/5], Step [429/842], Loss: 0.0356\n",
      "Epoch [3/5], Step [430/842], Loss: 0.0181\n",
      "Epoch [3/5], Step [431/842], Loss: 0.0298\n",
      "Epoch [3/5], Step [432/842], Loss: 0.0130\n",
      "Epoch [3/5], Step [433/842], Loss: 0.0056\n",
      "Epoch [3/5], Step [434/842], Loss: 0.0253\n",
      "Epoch [3/5], Step [435/842], Loss: 0.1560\n",
      "Epoch [3/5], Step [436/842], Loss: 0.0140\n",
      "Epoch [3/5], Step [437/842], Loss: 0.0526\n",
      "Epoch [3/5], Step [438/842], Loss: 0.0419\n",
      "Epoch [3/5], Step [439/842], Loss: 0.0797\n",
      "Epoch [3/5], Step [440/842], Loss: 0.0276\n",
      "Epoch [3/5], Step [441/842], Loss: 0.0028\n",
      "Epoch [3/5], Step [442/842], Loss: 0.0036\n",
      "Epoch [3/5], Step [443/842], Loss: 0.0354\n",
      "Epoch [3/5], Step [444/842], Loss: 0.4373\n",
      "Epoch [3/5], Step [445/842], Loss: 0.0268\n",
      "Epoch [3/5], Step [446/842], Loss: 0.0437\n",
      "Epoch [3/5], Step [447/842], Loss: 0.0212\n",
      "Epoch [3/5], Step [448/842], Loss: 0.0038\n",
      "Epoch [3/5], Step [449/842], Loss: 0.3637\n",
      "Epoch [3/5], Step [450/842], Loss: 0.0386\n",
      "Epoch [3/5], Step [451/842], Loss: 0.1589\n",
      "Epoch [3/5], Step [452/842], Loss: 0.0362\n",
      "Epoch [3/5], Step [453/842], Loss: 0.0205\n",
      "Epoch [3/5], Step [454/842], Loss: 0.0038\n",
      "Epoch [3/5], Step [455/842], Loss: 0.0112\n",
      "Epoch [3/5], Step [456/842], Loss: 0.0945\n",
      "Epoch [3/5], Step [457/842], Loss: 0.0650\n",
      "Epoch [3/5], Step [458/842], Loss: 0.0453\n",
      "Epoch [3/5], Step [459/842], Loss: 0.0697\n",
      "Epoch [3/5], Step [460/842], Loss: 0.1582\n",
      "Epoch [3/5], Step [461/842], Loss: 0.1336\n",
      "Epoch [3/5], Step [462/842], Loss: 0.1766\n",
      "Epoch [3/5], Step [463/842], Loss: 0.0029\n",
      "Epoch [3/5], Step [464/842], Loss: 0.1441\n",
      "Epoch [3/5], Step [465/842], Loss: 0.0153\n",
      "Epoch [3/5], Step [466/842], Loss: 0.0300\n",
      "Epoch [3/5], Step [467/842], Loss: 0.0312\n",
      "Epoch [3/5], Step [468/842], Loss: 0.1871\n",
      "Epoch [3/5], Step [469/842], Loss: 0.0073\n",
      "Epoch [3/5], Step [470/842], Loss: 0.1579\n",
      "Epoch [3/5], Step [471/842], Loss: 0.2003\n",
      "Epoch [3/5], Step [472/842], Loss: 0.1203\n",
      "Epoch [3/5], Step [473/842], Loss: 0.0114\n",
      "Epoch [3/5], Step [474/842], Loss: 0.0605\n",
      "Epoch [3/5], Step [475/842], Loss: 0.1045\n",
      "Epoch [3/5], Step [476/842], Loss: 0.2060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [477/842], Loss: 0.0593\n",
      "Epoch [3/5], Step [478/842], Loss: 0.0717\n",
      "Epoch [3/5], Step [479/842], Loss: 0.0520\n",
      "Epoch [3/5], Step [480/842], Loss: 0.0804\n",
      "Epoch [3/5], Step [481/842], Loss: 0.0566\n",
      "Epoch [3/5], Step [482/842], Loss: 0.0279\n",
      "Epoch [3/5], Step [483/842], Loss: 0.0622\n",
      "Epoch [3/5], Step [484/842], Loss: 0.0309\n",
      "Epoch [3/5], Step [485/842], Loss: 0.0079\n",
      "Epoch [3/5], Step [486/842], Loss: 0.0166\n",
      "Epoch [3/5], Step [487/842], Loss: 0.0394\n",
      "Epoch [3/5], Step [488/842], Loss: 0.0476\n",
      "Epoch [3/5], Step [489/842], Loss: 0.0967\n",
      "Epoch [3/5], Step [490/842], Loss: 0.2466\n",
      "Epoch [3/5], Step [491/842], Loss: 0.0751\n",
      "Epoch [3/5], Step [492/842], Loss: 0.1890\n",
      "Epoch [3/5], Step [493/842], Loss: 0.0053\n",
      "Epoch [3/5], Step [494/842], Loss: 0.0425\n",
      "Epoch [3/5], Step [495/842], Loss: 0.0850\n",
      "Epoch [3/5], Step [496/842], Loss: 0.0329\n",
      "Epoch [3/5], Step [497/842], Loss: 0.0010\n",
      "Epoch [3/5], Step [498/842], Loss: 0.2328\n",
      "Epoch [3/5], Step [499/842], Loss: 0.3636\n",
      "Epoch [3/5], Step [500/842], Loss: 0.0521\n",
      "Epoch [3/5], Step [501/842], Loss: 0.1437\n",
      "Epoch [3/5], Step [502/842], Loss: 0.0257\n",
      "Epoch [3/5], Step [503/842], Loss: 0.1414\n",
      "Epoch [3/5], Step [504/842], Loss: 0.0606\n",
      "Epoch [3/5], Step [505/842], Loss: 0.0167\n",
      "Epoch [3/5], Step [506/842], Loss: 0.0479\n",
      "Epoch [3/5], Step [507/842], Loss: 0.3326\n",
      "Epoch [3/5], Step [508/842], Loss: 0.3316\n",
      "Epoch [3/5], Step [509/842], Loss: 0.0074\n",
      "Epoch [3/5], Step [510/842], Loss: 0.0302\n",
      "Epoch [3/5], Step [511/842], Loss: 0.0116\n",
      "Epoch [3/5], Step [512/842], Loss: 0.0093\n",
      "Epoch [3/5], Step [513/842], Loss: 0.1273\n",
      "Epoch [3/5], Step [514/842], Loss: 0.0208\n",
      "Epoch [3/5], Step [515/842], Loss: 0.1226\n",
      "Epoch [3/5], Step [516/842], Loss: 0.0431\n",
      "Epoch [3/5], Step [517/842], Loss: 0.4385\n",
      "Epoch [3/5], Step [518/842], Loss: 0.0551\n",
      "Epoch [3/5], Step [519/842], Loss: 0.3006\n",
      "Epoch [3/5], Step [520/842], Loss: 0.0140\n",
      "Epoch [3/5], Step [521/842], Loss: 0.0170\n",
      "Epoch [3/5], Step [522/842], Loss: 0.4193\n",
      "Epoch [3/5], Step [523/842], Loss: 0.0660\n",
      "Epoch [3/5], Step [524/842], Loss: 0.1847\n",
      "Epoch [3/5], Step [525/842], Loss: 0.0522\n",
      "Epoch [3/5], Step [526/842], Loss: 0.2173\n",
      "Epoch [3/5], Step [527/842], Loss: 0.0159\n",
      "Epoch [3/5], Step [528/842], Loss: 0.0723\n",
      "Epoch [3/5], Step [529/842], Loss: 0.0706\n",
      "Epoch [3/5], Step [530/842], Loss: 0.0140\n",
      "Epoch [3/5], Step [531/842], Loss: 0.0089\n",
      "Epoch [3/5], Step [532/842], Loss: 0.2207\n",
      "Epoch [3/5], Step [533/842], Loss: 0.1372\n",
      "Epoch [3/5], Step [534/842], Loss: 0.0105\n",
      "Epoch [3/5], Step [535/842], Loss: 0.2094\n",
      "Epoch [3/5], Step [536/842], Loss: 0.0892\n",
      "Epoch [3/5], Step [537/842], Loss: 0.0625\n",
      "Epoch [3/5], Step [538/842], Loss: 0.0166\n",
      "Epoch [3/5], Step [539/842], Loss: 0.0734\n",
      "Epoch [3/5], Step [540/842], Loss: 0.5053\n",
      "Epoch [3/5], Step [541/842], Loss: 0.0025\n",
      "Epoch [3/5], Step [542/842], Loss: 0.0325\n",
      "Epoch [3/5], Step [543/842], Loss: 0.0353\n",
      "Epoch [3/5], Step [544/842], Loss: 0.1609\n",
      "Epoch [3/5], Step [545/842], Loss: 0.1404\n",
      "Epoch [3/5], Step [546/842], Loss: 0.1294\n",
      "Epoch [3/5], Step [547/842], Loss: 0.2068\n",
      "Epoch [3/5], Step [548/842], Loss: 0.2615\n",
      "Epoch [3/5], Step [549/842], Loss: 0.0031\n",
      "Epoch [3/5], Step [550/842], Loss: 0.1156\n",
      "Epoch [3/5], Step [551/842], Loss: 0.0929\n",
      "Epoch [3/5], Step [552/842], Loss: 0.1128\n",
      "Epoch [3/5], Step [553/842], Loss: 0.2598\n",
      "Epoch [3/5], Step [554/842], Loss: 0.0890\n",
      "Epoch [3/5], Step [555/842], Loss: 0.1136\n",
      "Epoch [3/5], Step [556/842], Loss: 0.0629\n",
      "Epoch [3/5], Step [557/842], Loss: 0.0868\n",
      "Epoch [3/5], Step [558/842], Loss: 0.1382\n",
      "Epoch [3/5], Step [559/842], Loss: 0.3776\n",
      "Epoch [3/5], Step [560/842], Loss: 0.0108\n",
      "Epoch [3/5], Step [561/842], Loss: 0.0191\n",
      "Epoch [3/5], Step [562/842], Loss: 0.0713\n",
      "Epoch [3/5], Step [563/842], Loss: 0.0693\n",
      "Epoch [3/5], Step [564/842], Loss: 0.1405\n",
      "Epoch [3/5], Step [565/842], Loss: 0.0961\n",
      "Epoch [3/5], Step [566/842], Loss: 0.1272\n",
      "Epoch [3/5], Step [567/842], Loss: 0.2251\n",
      "Epoch [3/5], Step [568/842], Loss: 0.0872\n",
      "Epoch [3/5], Step [569/842], Loss: 0.0612\n",
      "Epoch [3/5], Step [570/842], Loss: 0.0205\n",
      "Epoch [3/5], Step [571/842], Loss: 0.1046\n",
      "Epoch [3/5], Step [572/842], Loss: 0.2054\n",
      "Epoch [3/5], Step [573/842], Loss: 0.0753\n",
      "Epoch [3/5], Step [574/842], Loss: 0.1534\n",
      "Epoch [3/5], Step [575/842], Loss: 0.0328\n",
      "Epoch [3/5], Step [576/842], Loss: 0.2417\n",
      "Epoch [3/5], Step [577/842], Loss: 0.1511\n",
      "Epoch [3/5], Step [578/842], Loss: 0.1892\n",
      "Epoch [3/5], Step [579/842], Loss: 0.0125\n",
      "Epoch [3/5], Step [580/842], Loss: 0.1336\n",
      "Epoch [3/5], Step [581/842], Loss: 0.4772\n",
      "Epoch [3/5], Step [582/842], Loss: 0.1986\n",
      "Epoch [3/5], Step [583/842], Loss: 0.1654\n",
      "Epoch [3/5], Step [584/842], Loss: 0.0747\n",
      "Epoch [3/5], Step [585/842], Loss: 0.0123\n",
      "Epoch [3/5], Step [586/842], Loss: 0.0240\n",
      "Epoch [3/5], Step [587/842], Loss: 0.0081\n",
      "Epoch [3/5], Step [588/842], Loss: 0.1363\n",
      "Epoch [3/5], Step [589/842], Loss: 0.3008\n",
      "Epoch [3/5], Step [590/842], Loss: 0.1192\n",
      "Epoch [3/5], Step [591/842], Loss: 0.0194\n",
      "Epoch [3/5], Step [592/842], Loss: 0.0068\n",
      "Epoch [3/5], Step [593/842], Loss: 0.2588\n",
      "Epoch [3/5], Step [594/842], Loss: 0.0367\n",
      "Epoch [3/5], Step [595/842], Loss: 0.0679\n",
      "Epoch [3/5], Step [596/842], Loss: 0.1735\n",
      "Epoch [3/5], Step [597/842], Loss: 0.0233\n",
      "Epoch [3/5], Step [598/842], Loss: 0.2993\n",
      "Epoch [3/5], Step [599/842], Loss: 0.0110\n",
      "Epoch [3/5], Step [600/842], Loss: 0.0065\n",
      "Epoch [3/5], Step [601/842], Loss: 0.0247\n",
      "Epoch [3/5], Step [602/842], Loss: 0.0040\n",
      "Epoch [3/5], Step [603/842], Loss: 0.4780\n",
      "Epoch [3/5], Step [604/842], Loss: 0.0343\n",
      "Epoch [3/5], Step [605/842], Loss: 0.4182\n",
      "Epoch [3/5], Step [606/842], Loss: 0.0096\n",
      "Epoch [3/5], Step [607/842], Loss: 0.0192\n",
      "Epoch [3/5], Step [608/842], Loss: 0.0908\n",
      "Epoch [3/5], Step [609/842], Loss: 0.1103\n",
      "Epoch [3/5], Step [610/842], Loss: 0.0209\n",
      "Epoch [3/5], Step [611/842], Loss: 0.0978\n",
      "Epoch [3/5], Step [612/842], Loss: 0.0304\n",
      "Epoch [3/5], Step [613/842], Loss: 0.3944\n",
      "Epoch [3/5], Step [614/842], Loss: 0.0451\n",
      "Epoch [3/5], Step [615/842], Loss: 0.0672\n",
      "Epoch [3/5], Step [616/842], Loss: 0.0938\n",
      "Epoch [3/5], Step [617/842], Loss: 0.0695\n",
      "Epoch [3/5], Step [618/842], Loss: 0.0477\n",
      "Epoch [3/5], Step [619/842], Loss: 0.0231\n",
      "Epoch [3/5], Step [620/842], Loss: 0.1179\n",
      "Epoch [3/5], Step [621/842], Loss: 0.0718\n",
      "Epoch [3/5], Step [622/842], Loss: 0.0077\n",
      "Epoch [3/5], Step [623/842], Loss: 0.0055\n",
      "Epoch [3/5], Step [624/842], Loss: 0.0643\n",
      "Epoch [3/5], Step [625/842], Loss: 0.1464\n",
      "Epoch [3/5], Step [626/842], Loss: 0.0490\n",
      "Epoch [3/5], Step [627/842], Loss: 0.0679\n",
      "Epoch [3/5], Step [628/842], Loss: 0.0955\n",
      "Epoch [3/5], Step [629/842], Loss: 0.2032\n",
      "Epoch [3/5], Step [630/842], Loss: 0.1523\n",
      "Epoch [3/5], Step [631/842], Loss: 0.0967\n",
      "Epoch [3/5], Step [632/842], Loss: 0.0212\n",
      "Epoch [3/5], Step [633/842], Loss: 0.1601\n",
      "Epoch [3/5], Step [634/842], Loss: 0.0108\n",
      "Epoch [3/5], Step [635/842], Loss: 0.2493\n",
      "Epoch [3/5], Step [636/842], Loss: 0.0453\n",
      "Epoch [3/5], Step [637/842], Loss: 0.0602\n",
      "Epoch [3/5], Step [638/842], Loss: 0.0295\n",
      "Epoch [3/5], Step [639/842], Loss: 0.0664\n",
      "Epoch [3/5], Step [640/842], Loss: 0.0419\n",
      "Epoch [3/5], Step [641/842], Loss: 0.0902\n",
      "Epoch [3/5], Step [642/842], Loss: 0.1166\n",
      "Epoch [3/5], Step [643/842], Loss: 0.0298\n",
      "Epoch [3/5], Step [644/842], Loss: 0.0799\n",
      "Epoch [3/5], Step [645/842], Loss: 0.1197\n",
      "Epoch [3/5], Step [646/842], Loss: 0.0752\n",
      "Epoch [3/5], Step [647/842], Loss: 0.0250\n",
      "Epoch [3/5], Step [648/842], Loss: 0.2776\n",
      "Epoch [3/5], Step [649/842], Loss: 0.0063\n",
      "Epoch [3/5], Step [650/842], Loss: 0.0220\n",
      "Epoch [3/5], Step [651/842], Loss: 0.0794\n",
      "Epoch [3/5], Step [652/842], Loss: 0.0586\n",
      "Epoch [3/5], Step [653/842], Loss: 0.1121\n",
      "Epoch [3/5], Step [654/842], Loss: 0.0133\n",
      "Epoch [3/5], Step [655/842], Loss: 0.0665\n",
      "Epoch [3/5], Step [656/842], Loss: 0.4221\n",
      "Epoch [3/5], Step [657/842], Loss: 0.1286\n",
      "Epoch [3/5], Step [658/842], Loss: 0.1355\n",
      "Epoch [3/5], Step [659/842], Loss: 0.0053\n",
      "Epoch [3/5], Step [660/842], Loss: 0.3119\n",
      "Epoch [3/5], Step [661/842], Loss: 0.0291\n",
      "Epoch [3/5], Step [662/842], Loss: 0.0283\n",
      "Epoch [3/5], Step [663/842], Loss: 0.3796\n",
      "Epoch [3/5], Step [664/842], Loss: 0.3328\n",
      "Epoch [3/5], Step [665/842], Loss: 0.0672\n",
      "Epoch [3/5], Step [666/842], Loss: 0.1777\n",
      "Epoch [3/5], Step [667/842], Loss: 0.1359\n",
      "Epoch [3/5], Step [668/842], Loss: 0.1213\n",
      "Epoch [3/5], Step [669/842], Loss: 0.0662\n",
      "Epoch [3/5], Step [670/842], Loss: 0.1110\n",
      "Epoch [3/5], Step [671/842], Loss: 0.0062\n",
      "Epoch [3/5], Step [672/842], Loss: 0.0042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [673/842], Loss: 0.0165\n",
      "Epoch [3/5], Step [674/842], Loss: 0.0268\n",
      "Epoch [3/5], Step [675/842], Loss: 0.2840\n",
      "Epoch [3/5], Step [676/842], Loss: 0.1054\n",
      "Epoch [3/5], Step [677/842], Loss: 0.0069\n",
      "Epoch [3/5], Step [678/842], Loss: 0.2273\n",
      "Epoch [3/5], Step [679/842], Loss: 0.3322\n",
      "Epoch [3/5], Step [680/842], Loss: 0.0022\n",
      "Epoch [3/5], Step [681/842], Loss: 0.0119\n",
      "Epoch [3/5], Step [682/842], Loss: 0.2047\n",
      "Epoch [3/5], Step [683/842], Loss: 0.1727\n",
      "Epoch [3/5], Step [684/842], Loss: 0.0088\n",
      "Epoch [3/5], Step [685/842], Loss: 0.3211\n",
      "Epoch [3/5], Step [686/842], Loss: 0.0582\n",
      "Epoch [3/5], Step [687/842], Loss: 0.0425\n",
      "Epoch [3/5], Step [688/842], Loss: 0.0738\n",
      "Epoch [3/5], Step [689/842], Loss: 0.2956\n",
      "Epoch [3/5], Step [690/842], Loss: 0.0470\n",
      "Epoch [3/5], Step [691/842], Loss: 0.1893\n",
      "Epoch [3/5], Step [692/842], Loss: 0.0240\n",
      "Epoch [3/5], Step [693/842], Loss: 0.1866\n",
      "Epoch [3/5], Step [694/842], Loss: 0.1122\n",
      "Epoch [3/5], Step [695/842], Loss: 0.1379\n",
      "Epoch [3/5], Step [696/842], Loss: 0.0754\n",
      "Epoch [3/5], Step [697/842], Loss: 0.0741\n",
      "Epoch [3/5], Step [698/842], Loss: 0.0214\n",
      "Epoch [3/5], Step [699/842], Loss: 0.0903\n",
      "Epoch [3/5], Step [700/842], Loss: 0.0697\n",
      "Epoch [3/5], Step [701/842], Loss: 0.2139\n",
      "Epoch [3/5], Step [702/842], Loss: 0.2437\n",
      "Epoch [3/5], Step [703/842], Loss: 0.1615\n",
      "Epoch [3/5], Step [704/842], Loss: 0.0453\n",
      "Epoch [3/5], Step [705/842], Loss: 0.0027\n",
      "Epoch [3/5], Step [706/842], Loss: 0.0444\n",
      "Epoch [3/5], Step [707/842], Loss: 0.2794\n",
      "Epoch [3/5], Step [708/842], Loss: 0.0053\n",
      "Epoch [3/5], Step [709/842], Loss: 0.1196\n",
      "Epoch [3/5], Step [710/842], Loss: 0.2032\n",
      "Epoch [3/5], Step [711/842], Loss: 0.1378\n",
      "Epoch [3/5], Step [712/842], Loss: 0.0698\n",
      "Epoch [3/5], Step [713/842], Loss: 0.4491\n",
      "Epoch [3/5], Step [714/842], Loss: 0.1912\n",
      "Epoch [3/5], Step [715/842], Loss: 0.3776\n",
      "Epoch [3/5], Step [716/842], Loss: 0.0046\n",
      "Epoch [3/5], Step [717/842], Loss: 0.0158\n",
      "Epoch [3/5], Step [718/842], Loss: 0.0618\n",
      "Epoch [3/5], Step [719/842], Loss: 0.2152\n",
      "Epoch [3/5], Step [720/842], Loss: 0.2561\n",
      "Epoch [3/5], Step [721/842], Loss: 0.3174\n",
      "Epoch [3/5], Step [722/842], Loss: 0.4090\n",
      "Epoch [3/5], Step [723/842], Loss: 0.2126\n",
      "Epoch [3/5], Step [724/842], Loss: 0.0180\n",
      "Epoch [3/5], Step [725/842], Loss: 0.0699\n",
      "Epoch [3/5], Step [726/842], Loss: 0.0499\n",
      "Epoch [3/5], Step [727/842], Loss: 0.0290\n",
      "Epoch [3/5], Step [728/842], Loss: 0.0035\n",
      "Epoch [3/5], Step [729/842], Loss: 0.1684\n",
      "Epoch [3/5], Step [730/842], Loss: 0.0574\n",
      "Epoch [3/5], Step [731/842], Loss: 0.1398\n",
      "Epoch [3/5], Step [732/842], Loss: 0.1337\n",
      "Epoch [3/5], Step [733/842], Loss: 0.1867\n",
      "Epoch [3/5], Step [734/842], Loss: 0.0075\n",
      "Epoch [3/5], Step [735/842], Loss: 0.1240\n",
      "Epoch [3/5], Step [736/842], Loss: 0.0325\n",
      "Epoch [3/5], Step [737/842], Loss: 0.6927\n",
      "Epoch [3/5], Step [738/842], Loss: 0.4018\n",
      "Epoch [3/5], Step [739/842], Loss: 0.0771\n",
      "Epoch [3/5], Step [740/842], Loss: 0.0268\n",
      "Epoch [3/5], Step [741/842], Loss: 0.0975\n",
      "Epoch [3/5], Step [742/842], Loss: 0.0371\n",
      "Epoch [3/5], Step [743/842], Loss: 0.3604\n",
      "Epoch [3/5], Step [744/842], Loss: 0.0513\n",
      "Epoch [3/5], Step [745/842], Loss: 0.0085\n",
      "Epoch [3/5], Step [746/842], Loss: 0.0471\n",
      "Epoch [3/5], Step [747/842], Loss: 0.1665\n",
      "Epoch [3/5], Step [748/842], Loss: 0.0327\n",
      "Epoch [3/5], Step [749/842], Loss: 0.0095\n",
      "Epoch [3/5], Step [750/842], Loss: 0.0445\n",
      "Epoch [3/5], Step [751/842], Loss: 0.0148\n",
      "Epoch [3/5], Step [752/842], Loss: 0.0573\n",
      "Epoch [3/5], Step [753/842], Loss: 0.1050\n",
      "Epoch [3/5], Step [754/842], Loss: 0.3939\n",
      "Epoch [3/5], Step [755/842], Loss: 0.3160\n",
      "Epoch [3/5], Step [756/842], Loss: 0.0051\n",
      "Epoch [3/5], Step [757/842], Loss: 0.0915\n",
      "Epoch [3/5], Step [758/842], Loss: 0.0183\n",
      "Epoch [3/5], Step [759/842], Loss: 0.1717\n",
      "Epoch [3/5], Step [760/842], Loss: 0.0817\n",
      "Epoch [3/5], Step [761/842], Loss: 0.0234\n",
      "Epoch [3/5], Step [762/842], Loss: 0.1669\n",
      "Epoch [3/5], Step [763/842], Loss: 0.0577\n",
      "Epoch [3/5], Step [764/842], Loss: 0.1343\n",
      "Epoch [3/5], Step [765/842], Loss: 0.1591\n",
      "Epoch [3/5], Step [766/842], Loss: 0.2093\n",
      "Epoch [3/5], Step [767/842], Loss: 0.0138\n",
      "Epoch [3/5], Step [768/842], Loss: 0.0139\n",
      "Epoch [3/5], Step [769/842], Loss: 0.1803\n",
      "Epoch [3/5], Step [770/842], Loss: 0.2180\n",
      "Epoch [3/5], Step [771/842], Loss: 0.0198\n",
      "Epoch [3/5], Step [772/842], Loss: 0.4628\n",
      "Epoch [3/5], Step [773/842], Loss: 0.0695\n",
      "Epoch [3/5], Step [774/842], Loss: 0.0235\n",
      "Epoch [3/5], Step [775/842], Loss: 0.2758\n",
      "Epoch [3/5], Step [776/842], Loss: 0.4681\n",
      "Epoch [3/5], Step [777/842], Loss: 0.0856\n",
      "Epoch [3/5], Step [778/842], Loss: 0.0225\n",
      "Epoch [3/5], Step [779/842], Loss: 0.2934\n",
      "Epoch [3/5], Step [780/842], Loss: 0.0174\n",
      "Epoch [3/5], Step [781/842], Loss: 0.0446\n",
      "Epoch [3/5], Step [782/842], Loss: 0.2034\n",
      "Epoch [3/5], Step [783/842], Loss: 0.0073\n",
      "Epoch [3/5], Step [784/842], Loss: 0.1482\n",
      "Epoch [3/5], Step [785/842], Loss: 0.2305\n",
      "Epoch [3/5], Step [786/842], Loss: 0.4467\n",
      "Epoch [3/5], Step [787/842], Loss: 0.0604\n",
      "Epoch [3/5], Step [788/842], Loss: 0.2172\n",
      "Epoch [3/5], Step [789/842], Loss: 0.0507\n",
      "Epoch [3/5], Step [790/842], Loss: 0.2319\n",
      "Epoch [3/5], Step [791/842], Loss: 0.0135\n",
      "Epoch [3/5], Step [792/842], Loss: 0.0171\n",
      "Epoch [3/5], Step [793/842], Loss: 0.1687\n",
      "Epoch [3/5], Step [794/842], Loss: 0.2849\n",
      "Epoch [3/5], Step [795/842], Loss: 0.0047\n",
      "Epoch [3/5], Step [796/842], Loss: 0.2343\n",
      "Epoch [3/5], Step [797/842], Loss: 0.0197\n",
      "Epoch [3/5], Step [798/842], Loss: 0.0597\n",
      "Epoch [3/5], Step [799/842], Loss: 0.1360\n",
      "Epoch [3/5], Step [800/842], Loss: 0.3791\n",
      "Epoch [3/5], Step [801/842], Loss: 0.0032\n",
      "Epoch [3/5], Step [802/842], Loss: 0.2041\n",
      "Epoch [3/5], Step [803/842], Loss: 0.0509\n",
      "Epoch [3/5], Step [804/842], Loss: 0.0295\n",
      "Epoch [3/5], Step [805/842], Loss: 0.0232\n",
      "Epoch [3/5], Step [806/842], Loss: 0.0438\n",
      "Epoch [3/5], Step [807/842], Loss: 0.0635\n",
      "Epoch [3/5], Step [808/842], Loss: 0.0040\n",
      "Epoch [3/5], Step [809/842], Loss: 0.1756\n",
      "Epoch [3/5], Step [810/842], Loss: 0.0242\n",
      "Epoch [3/5], Step [811/842], Loss: 0.2368\n",
      "Epoch [3/5], Step [812/842], Loss: 0.3990\n",
      "Epoch [3/5], Step [813/842], Loss: 0.1713\n",
      "Epoch [3/5], Step [814/842], Loss: 0.0627\n",
      "Epoch [3/5], Step [815/842], Loss: 0.0032\n",
      "Epoch [3/5], Step [816/842], Loss: 0.0022\n",
      "Epoch [3/5], Step [817/842], Loss: 0.1803\n",
      "Epoch [3/5], Step [818/842], Loss: 0.2297\n",
      "Epoch [3/5], Step [819/842], Loss: 0.1595\n",
      "Epoch [3/5], Step [820/842], Loss: 0.1383\n",
      "Epoch [3/5], Step [821/842], Loss: 0.3299\n",
      "Epoch [3/5], Step [822/842], Loss: 0.0599\n",
      "Epoch [3/5], Step [823/842], Loss: 0.0225\n",
      "Epoch [3/5], Step [824/842], Loss: 0.0520\n",
      "Epoch [3/5], Step [825/842], Loss: 0.0316\n",
      "Epoch [3/5], Step [826/842], Loss: 0.1025\n",
      "Epoch [3/5], Step [827/842], Loss: 0.0771\n",
      "Epoch [3/5], Step [828/842], Loss: 0.0049\n",
      "Epoch [3/5], Step [829/842], Loss: 0.0159\n",
      "Epoch [3/5], Step [830/842], Loss: 0.0124\n",
      "Epoch [3/5], Step [831/842], Loss: 0.0204\n",
      "Epoch [3/5], Step [832/842], Loss: 0.0827\n",
      "Epoch [3/5], Step [833/842], Loss: 0.0131\n",
      "Epoch [3/5], Step [834/842], Loss: 0.0573\n",
      "Epoch [3/5], Step [835/842], Loss: 0.1904\n",
      "Epoch [3/5], Step [836/842], Loss: 0.0099\n",
      "Epoch [3/5], Step [837/842], Loss: 0.1500\n",
      "Epoch [3/5], Step [838/842], Loss: 0.1264\n",
      "Epoch [3/5], Step [839/842], Loss: 0.0139\n",
      "Epoch [3/5], Step [840/842], Loss: 0.0031\n",
      "Epoch [3/5], Step [841/842], Loss: 0.0287\n",
      "Epoch [3/5], Step [842/842], Loss: 0.0338\n",
      "Epoch [3/5], Step [843/842], Loss: 0.0833\n",
      "Epoch [4/5], Step [1/842], Loss: 0.0181\n",
      "Epoch [4/5], Step [2/842], Loss: 0.0196\n",
      "Epoch [4/5], Step [3/842], Loss: 0.0755\n",
      "Epoch [4/5], Step [4/842], Loss: 0.0064\n",
      "Epoch [4/5], Step [5/842], Loss: 0.0300\n",
      "Epoch [4/5], Step [6/842], Loss: 0.1308\n",
      "Epoch [4/5], Step [7/842], Loss: 0.0035\n",
      "Epoch [4/5], Step [8/842], Loss: 0.1603\n",
      "Epoch [4/5], Step [9/842], Loss: 0.0168\n",
      "Epoch [4/5], Step [10/842], Loss: 0.0379\n",
      "Epoch [4/5], Step [11/842], Loss: 0.0582\n",
      "Epoch [4/5], Step [12/842], Loss: 0.1043\n",
      "Epoch [4/5], Step [13/842], Loss: 0.0096\n",
      "Epoch [4/5], Step [14/842], Loss: 0.0069\n",
      "Epoch [4/5], Step [15/842], Loss: 0.0223\n",
      "Epoch [4/5], Step [16/842], Loss: 0.0370\n",
      "Epoch [4/5], Step [17/842], Loss: 0.0525\n",
      "Epoch [4/5], Step [18/842], Loss: 0.0324\n",
      "Epoch [4/5], Step [19/842], Loss: 0.1171\n",
      "Epoch [4/5], Step [20/842], Loss: 0.1403\n",
      "Epoch [4/5], Step [21/842], Loss: 0.0037\n",
      "Epoch [4/5], Step [22/842], Loss: 0.0597\n",
      "Epoch [4/5], Step [23/842], Loss: 0.0235\n",
      "Epoch [4/5], Step [24/842], Loss: 0.0130\n",
      "Epoch [4/5], Step [25/842], Loss: 0.0168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [26/842], Loss: 0.0192\n",
      "Epoch [4/5], Step [27/842], Loss: 0.0058\n",
      "Epoch [4/5], Step [28/842], Loss: 0.0201\n",
      "Epoch [4/5], Step [29/842], Loss: 0.0394\n",
      "Epoch [4/5], Step [30/842], Loss: 0.1875\n",
      "Epoch [4/5], Step [31/842], Loss: 0.0934\n",
      "Epoch [4/5], Step [32/842], Loss: 0.1472\n",
      "Epoch [4/5], Step [33/842], Loss: 0.0961\n",
      "Epoch [4/5], Step [34/842], Loss: 0.0900\n",
      "Epoch [4/5], Step [35/842], Loss: 0.0147\n",
      "Epoch [4/5], Step [36/842], Loss: 0.0040\n",
      "Epoch [4/5], Step [37/842], Loss: 0.0652\n",
      "Epoch [4/5], Step [38/842], Loss: 0.0240\n",
      "Epoch [4/5], Step [39/842], Loss: 0.0148\n",
      "Epoch [4/5], Step [40/842], Loss: 0.1544\n",
      "Epoch [4/5], Step [41/842], Loss: 0.0285\n",
      "Epoch [4/5], Step [42/842], Loss: 0.1209\n",
      "Epoch [4/5], Step [43/842], Loss: 0.1262\n",
      "Epoch [4/5], Step [44/842], Loss: 0.0920\n",
      "Epoch [4/5], Step [45/842], Loss: 0.0131\n",
      "Epoch [4/5], Step [46/842], Loss: 0.1284\n",
      "Epoch [4/5], Step [47/842], Loss: 0.0058\n",
      "Epoch [4/5], Step [48/842], Loss: 0.0038\n",
      "Epoch [4/5], Step [49/842], Loss: 0.0203\n",
      "Epoch [4/5], Step [50/842], Loss: 0.1777\n",
      "Epoch [4/5], Step [51/842], Loss: 0.0248\n",
      "Epoch [4/5], Step [52/842], Loss: 0.0070\n",
      "Epoch [4/5], Step [53/842], Loss: 0.0165\n",
      "Epoch [4/5], Step [54/842], Loss: 0.0284\n",
      "Epoch [4/5], Step [55/842], Loss: 0.1758\n",
      "Epoch [4/5], Step [56/842], Loss: 0.0034\n",
      "Epoch [4/5], Step [57/842], Loss: 0.0240\n",
      "Epoch [4/5], Step [58/842], Loss: 0.1472\n",
      "Epoch [4/5], Step [59/842], Loss: 0.0232\n",
      "Epoch [4/5], Step [60/842], Loss: 0.0347\n",
      "Epoch [4/5], Step [61/842], Loss: 0.0246\n",
      "Epoch [4/5], Step [62/842], Loss: 0.0370\n",
      "Epoch [4/5], Step [63/842], Loss: 0.0060\n",
      "Epoch [4/5], Step [64/842], Loss: 0.0224\n",
      "Epoch [4/5], Step [65/842], Loss: 0.0366\n",
      "Epoch [4/5], Step [66/842], Loss: 0.0596\n",
      "Epoch [4/5], Step [67/842], Loss: 0.0049\n",
      "Epoch [4/5], Step [68/842], Loss: 0.1386\n",
      "Epoch [4/5], Step [69/842], Loss: 0.0545\n",
      "Epoch [4/5], Step [70/842], Loss: 0.0186\n",
      "Epoch [4/5], Step [71/842], Loss: 0.1709\n",
      "Epoch [4/5], Step [72/842], Loss: 0.0646\n",
      "Epoch [4/5], Step [73/842], Loss: 0.0116\n",
      "Epoch [4/5], Step [74/842], Loss: 0.0866\n",
      "Epoch [4/5], Step [75/842], Loss: 0.0114\n",
      "Epoch [4/5], Step [76/842], Loss: 0.1004\n",
      "Epoch [4/5], Step [77/842], Loss: 0.0964\n",
      "Epoch [4/5], Step [78/842], Loss: 0.0287\n",
      "Epoch [4/5], Step [79/842], Loss: 0.1886\n",
      "Epoch [4/5], Step [80/842], Loss: 0.0047\n",
      "Epoch [4/5], Step [81/842], Loss: 0.0905\n",
      "Epoch [4/5], Step [82/842], Loss: 0.0015\n",
      "Epoch [4/5], Step [83/842], Loss: 0.0272\n",
      "Epoch [4/5], Step [84/842], Loss: 0.1857\n",
      "Epoch [4/5], Step [85/842], Loss: 0.0026\n",
      "Epoch [4/5], Step [86/842], Loss: 0.0027\n",
      "Epoch [4/5], Step [87/842], Loss: 0.1611\n",
      "Epoch [4/5], Step [88/842], Loss: 0.0072\n",
      "Epoch [4/5], Step [89/842], Loss: 0.0112\n",
      "Epoch [4/5], Step [90/842], Loss: 0.0563\n",
      "Epoch [4/5], Step [91/842], Loss: 0.0070\n",
      "Epoch [4/5], Step [92/842], Loss: 0.0015\n",
      "Epoch [4/5], Step [93/842], Loss: 0.0095\n",
      "Epoch [4/5], Step [94/842], Loss: 0.0755\n",
      "Epoch [4/5], Step [95/842], Loss: 0.0178\n",
      "Epoch [4/5], Step [96/842], Loss: 0.1449\n",
      "Epoch [4/5], Step [97/842], Loss: 0.0443\n",
      "Epoch [4/5], Step [98/842], Loss: 0.0656\n",
      "Epoch [4/5], Step [99/842], Loss: 0.0138\n",
      "Epoch [4/5], Step [100/842], Loss: 0.0157\n",
      "Epoch [4/5], Step [101/842], Loss: 0.2117\n",
      "Epoch [4/5], Step [102/842], Loss: 0.0025\n",
      "Epoch [4/5], Step [103/842], Loss: 0.0194\n",
      "Epoch [4/5], Step [104/842], Loss: 0.0016\n",
      "Epoch [4/5], Step [105/842], Loss: 0.0053\n",
      "Epoch [4/5], Step [106/842], Loss: 0.0044\n",
      "Epoch [4/5], Step [107/842], Loss: 0.0805\n",
      "Epoch [4/5], Step [108/842], Loss: 0.0094\n",
      "Epoch [4/5], Step [109/842], Loss: 0.0845\n",
      "Epoch [4/5], Step [110/842], Loss: 0.1621\n",
      "Epoch [4/5], Step [111/842], Loss: 0.0291\n",
      "Epoch [4/5], Step [112/842], Loss: 0.0295\n",
      "Epoch [4/5], Step [113/842], Loss: 0.0041\n",
      "Epoch [4/5], Step [114/842], Loss: 0.2522\n",
      "Epoch [4/5], Step [115/842], Loss: 0.1344\n",
      "Epoch [4/5], Step [116/842], Loss: 0.2128\n",
      "Epoch [4/5], Step [117/842], Loss: 0.0166\n",
      "Epoch [4/5], Step [118/842], Loss: 0.0806\n",
      "Epoch [4/5], Step [119/842], Loss: 0.0078\n",
      "Epoch [4/5], Step [120/842], Loss: 0.0142\n",
      "Epoch [4/5], Step [121/842], Loss: 0.0066\n",
      "Epoch [4/5], Step [122/842], Loss: 0.0671\n",
      "Epoch [4/5], Step [123/842], Loss: 0.0012\n",
      "Epoch [4/5], Step [124/842], Loss: 0.0440\n",
      "Epoch [4/5], Step [125/842], Loss: 0.0094\n",
      "Epoch [4/5], Step [126/842], Loss: 0.0190\n",
      "Epoch [4/5], Step [127/842], Loss: 0.1031\n",
      "Epoch [4/5], Step [128/842], Loss: 0.0248\n",
      "Epoch [4/5], Step [129/842], Loss: 0.2958\n",
      "Epoch [4/5], Step [130/842], Loss: 0.1942\n",
      "Epoch [4/5], Step [131/842], Loss: 0.0297\n",
      "Epoch [4/5], Step [132/842], Loss: 0.1679\n",
      "Epoch [4/5], Step [133/842], Loss: 0.0281\n",
      "Epoch [4/5], Step [134/842], Loss: 0.0023\n",
      "Epoch [4/5], Step [135/842], Loss: 0.0114\n",
      "Epoch [4/5], Step [136/842], Loss: 0.0254\n",
      "Epoch [4/5], Step [137/842], Loss: 0.0775\n",
      "Epoch [4/5], Step [138/842], Loss: 0.0446\n",
      "Epoch [4/5], Step [139/842], Loss: 0.1772\n",
      "Epoch [4/5], Step [140/842], Loss: 0.0108\n",
      "Epoch [4/5], Step [141/842], Loss: 0.0041\n",
      "Epoch [4/5], Step [142/842], Loss: 0.0025\n",
      "Epoch [4/5], Step [143/842], Loss: 0.0148\n",
      "Epoch [4/5], Step [144/842], Loss: 0.0014\n",
      "Epoch [4/5], Step [145/842], Loss: 0.0266\n",
      "Epoch [4/5], Step [146/842], Loss: 0.0016\n",
      "Epoch [4/5], Step [147/842], Loss: 0.0026\n",
      "Epoch [4/5], Step [148/842], Loss: 0.0889\n",
      "Epoch [4/5], Step [149/842], Loss: 0.0022\n",
      "Epoch [4/5], Step [150/842], Loss: 0.0275\n",
      "Epoch [4/5], Step [151/842], Loss: 0.1672\n",
      "Epoch [4/5], Step [152/842], Loss: 0.0105\n",
      "Epoch [4/5], Step [153/842], Loss: 0.0406\n",
      "Epoch [4/5], Step [154/842], Loss: 0.0112\n",
      "Epoch [4/5], Step [155/842], Loss: 0.0462\n",
      "Epoch [4/5], Step [156/842], Loss: 0.0629\n",
      "Epoch [4/5], Step [157/842], Loss: 0.0008\n",
      "Epoch [4/5], Step [158/842], Loss: 0.0016\n",
      "Epoch [4/5], Step [159/842], Loss: 0.0026\n",
      "Epoch [4/5], Step [160/842], Loss: 0.0100\n",
      "Epoch [4/5], Step [161/842], Loss: 0.1014\n",
      "Epoch [4/5], Step [162/842], Loss: 0.0432\n",
      "Epoch [4/5], Step [163/842], Loss: 0.0134\n",
      "Epoch [4/5], Step [164/842], Loss: 0.0050\n",
      "Epoch [4/5], Step [165/842], Loss: 0.0389\n",
      "Epoch [4/5], Step [166/842], Loss: 0.0110\n",
      "Epoch [4/5], Step [167/842], Loss: 0.0164\n",
      "Epoch [4/5], Step [168/842], Loss: 0.0043\n",
      "Epoch [4/5], Step [169/842], Loss: 0.0041\n",
      "Epoch [4/5], Step [170/842], Loss: 0.0055\n",
      "Epoch [4/5], Step [171/842], Loss: 0.2826\n",
      "Epoch [4/5], Step [172/842], Loss: 0.0041\n",
      "Epoch [4/5], Step [173/842], Loss: 0.0242\n",
      "Epoch [4/5], Step [174/842], Loss: 0.0174\n",
      "Epoch [4/5], Step [175/842], Loss: 0.0039\n",
      "Epoch [4/5], Step [176/842], Loss: 0.0148\n",
      "Epoch [4/5], Step [177/842], Loss: 0.0108\n",
      "Epoch [4/5], Step [178/842], Loss: 0.0310\n",
      "Epoch [4/5], Step [179/842], Loss: 0.0196\n",
      "Epoch [4/5], Step [180/842], Loss: 0.0691\n",
      "Epoch [4/5], Step [181/842], Loss: 0.0097\n",
      "Epoch [4/5], Step [182/842], Loss: 0.0823\n",
      "Epoch [4/5], Step [183/842], Loss: 0.0271\n",
      "Epoch [4/5], Step [184/842], Loss: 0.0042\n",
      "Epoch [4/5], Step [185/842], Loss: 0.0040\n",
      "Epoch [4/5], Step [186/842], Loss: 0.1227\n",
      "Epoch [4/5], Step [187/842], Loss: 0.0669\n",
      "Epoch [4/5], Step [188/842], Loss: 0.0786\n",
      "Epoch [4/5], Step [189/842], Loss: 0.0095\n",
      "Epoch [4/5], Step [190/842], Loss: 0.0057\n",
      "Epoch [4/5], Step [191/842], Loss: 0.0344\n",
      "Epoch [4/5], Step [192/842], Loss: 0.0039\n",
      "Epoch [4/5], Step [193/842], Loss: 0.0196\n",
      "Epoch [4/5], Step [194/842], Loss: 0.0888\n",
      "Epoch [4/5], Step [195/842], Loss: 0.0070\n",
      "Epoch [4/5], Step [196/842], Loss: 0.0171\n",
      "Epoch [4/5], Step [197/842], Loss: 0.0647\n",
      "Epoch [4/5], Step [198/842], Loss: 0.0226\n",
      "Epoch [4/5], Step [199/842], Loss: 0.0414\n",
      "Epoch [4/5], Step [200/842], Loss: 0.0333\n",
      "Epoch [4/5], Step [201/842], Loss: 0.0027\n",
      "Epoch [4/5], Step [202/842], Loss: 0.0225\n",
      "Epoch [4/5], Step [203/842], Loss: 0.0081\n",
      "Epoch [4/5], Step [204/842], Loss: 0.0516\n",
      "Epoch [4/5], Step [205/842], Loss: 0.0140\n",
      "Epoch [4/5], Step [206/842], Loss: 0.0138\n",
      "Epoch [4/5], Step [207/842], Loss: 0.0010\n",
      "Epoch [4/5], Step [208/842], Loss: 0.0189\n",
      "Epoch [4/5], Step [209/842], Loss: 0.0035\n",
      "Epoch [4/5], Step [210/842], Loss: 0.1851\n",
      "Epoch [4/5], Step [211/842], Loss: 0.0022\n",
      "Epoch [4/5], Step [212/842], Loss: 0.0827\n",
      "Epoch [4/5], Step [213/842], Loss: 0.0034\n",
      "Epoch [4/5], Step [214/842], Loss: 0.1642\n",
      "Epoch [4/5], Step [215/842], Loss: 0.0304\n",
      "Epoch [4/5], Step [216/842], Loss: 0.1718\n",
      "Epoch [4/5], Step [217/842], Loss: 0.0044\n",
      "Epoch [4/5], Step [218/842], Loss: 0.0767\n",
      "Epoch [4/5], Step [219/842], Loss: 0.0024\n",
      "Epoch [4/5], Step [220/842], Loss: 0.0070\n",
      "Epoch [4/5], Step [221/842], Loss: 0.0010\n",
      "Epoch [4/5], Step [222/842], Loss: 0.1101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [223/842], Loss: 0.2699\n",
      "Epoch [4/5], Step [224/842], Loss: 0.0385\n",
      "Epoch [4/5], Step [225/842], Loss: 0.0766\n",
      "Epoch [4/5], Step [226/842], Loss: 0.1066\n",
      "Epoch [4/5], Step [227/842], Loss: 0.0078\n",
      "Epoch [4/5], Step [228/842], Loss: 0.0331\n",
      "Epoch [4/5], Step [229/842], Loss: 0.2142\n",
      "Epoch [4/5], Step [230/842], Loss: 0.0224\n",
      "Epoch [4/5], Step [231/842], Loss: 0.0653\n",
      "Epoch [4/5], Step [232/842], Loss: 0.0102\n",
      "Epoch [4/5], Step [233/842], Loss: 0.0123\n",
      "Epoch [4/5], Step [234/842], Loss: 0.0684\n",
      "Epoch [4/5], Step [235/842], Loss: 0.0104\n",
      "Epoch [4/5], Step [236/842], Loss: 0.0151\n",
      "Epoch [4/5], Step [237/842], Loss: 0.0223\n",
      "Epoch [4/5], Step [238/842], Loss: 0.0230\n",
      "Epoch [4/5], Step [239/842], Loss: 0.0010\n",
      "Epoch [4/5], Step [240/842], Loss: 0.1457\n",
      "Epoch [4/5], Step [241/842], Loss: 0.0062\n",
      "Epoch [4/5], Step [242/842], Loss: 0.0062\n",
      "Epoch [4/5], Step [243/842], Loss: 0.2005\n",
      "Epoch [4/5], Step [244/842], Loss: 0.0341\n",
      "Epoch [4/5], Step [245/842], Loss: 0.1498\n",
      "Epoch [4/5], Step [246/842], Loss: 0.0229\n",
      "Epoch [4/5], Step [247/842], Loss: 0.3685\n",
      "Epoch [4/5], Step [248/842], Loss: 0.0045\n",
      "Epoch [4/5], Step [249/842], Loss: 0.1165\n",
      "Epoch [4/5], Step [250/842], Loss: 0.0084\n",
      "Epoch [4/5], Step [251/842], Loss: 0.0888\n",
      "Epoch [4/5], Step [252/842], Loss: 0.0207\n",
      "Epoch [4/5], Step [253/842], Loss: 0.1953\n",
      "Epoch [4/5], Step [254/842], Loss: 0.0628\n",
      "Epoch [4/5], Step [255/842], Loss: 0.0179\n",
      "Epoch [4/5], Step [256/842], Loss: 0.1659\n",
      "Epoch [4/5], Step [257/842], Loss: 0.0026\n",
      "Epoch [4/5], Step [258/842], Loss: 0.0103\n",
      "Epoch [4/5], Step [259/842], Loss: 0.0070\n",
      "Epoch [4/5], Step [260/842], Loss: 0.0249\n",
      "Epoch [4/5], Step [261/842], Loss: 0.0200\n",
      "Epoch [4/5], Step [262/842], Loss: 0.0581\n",
      "Epoch [4/5], Step [263/842], Loss: 0.1501\n",
      "Epoch [4/5], Step [264/842], Loss: 0.0078\n",
      "Epoch [4/5], Step [265/842], Loss: 0.0088\n",
      "Epoch [4/5], Step [266/842], Loss: 0.0584\n",
      "Epoch [4/5], Step [267/842], Loss: 0.0740\n",
      "Epoch [4/5], Step [268/842], Loss: 0.0682\n",
      "Epoch [4/5], Step [269/842], Loss: 0.0032\n",
      "Epoch [4/5], Step [270/842], Loss: 0.1169\n",
      "Epoch [4/5], Step [271/842], Loss: 0.0351\n",
      "Epoch [4/5], Step [272/842], Loss: 0.0842\n",
      "Epoch [4/5], Step [273/842], Loss: 0.0030\n",
      "Epoch [4/5], Step [274/842], Loss: 0.0074\n",
      "Epoch [4/5], Step [275/842], Loss: 0.3966\n",
      "Epoch [4/5], Step [276/842], Loss: 0.1958\n",
      "Epoch [4/5], Step [277/842], Loss: 0.0130\n",
      "Epoch [4/5], Step [278/842], Loss: 0.0393\n",
      "Epoch [4/5], Step [279/842], Loss: 0.0187\n",
      "Epoch [4/5], Step [280/842], Loss: 0.0029\n",
      "Epoch [4/5], Step [281/842], Loss: 0.0310\n",
      "Epoch [4/5], Step [282/842], Loss: 0.0016\n",
      "Epoch [4/5], Step [283/842], Loss: 0.2843\n",
      "Epoch [4/5], Step [284/842], Loss: 0.0079\n",
      "Epoch [4/5], Step [285/842], Loss: 0.0272\n",
      "Epoch [4/5], Step [286/842], Loss: 0.0193\n",
      "Epoch [4/5], Step [287/842], Loss: 0.0050\n",
      "Epoch [4/5], Step [288/842], Loss: 0.0778\n",
      "Epoch [4/5], Step [289/842], Loss: 0.0666\n",
      "Epoch [4/5], Step [290/842], Loss: 0.0420\n",
      "Epoch [4/5], Step [291/842], Loss: 0.0861\n",
      "Epoch [4/5], Step [292/842], Loss: 0.0630\n",
      "Epoch [4/5], Step [293/842], Loss: 0.1454\n",
      "Epoch [4/5], Step [294/842], Loss: 0.0798\n",
      "Epoch [4/5], Step [295/842], Loss: 0.0098\n",
      "Epoch [4/5], Step [296/842], Loss: 0.0387\n",
      "Epoch [4/5], Step [297/842], Loss: 0.0273\n",
      "Epoch [4/5], Step [298/842], Loss: 0.0218\n",
      "Epoch [4/5], Step [299/842], Loss: 0.0028\n",
      "Epoch [4/5], Step [300/842], Loss: 0.0111\n",
      "Epoch [4/5], Step [301/842], Loss: 0.1569\n",
      "Epoch [4/5], Step [302/842], Loss: 0.0036\n",
      "Epoch [4/5], Step [303/842], Loss: 0.0377\n",
      "Epoch [4/5], Step [304/842], Loss: 0.0017\n",
      "Epoch [4/5], Step [305/842], Loss: 0.1648\n",
      "Epoch [4/5], Step [306/842], Loss: 0.1108\n",
      "Epoch [4/5], Step [307/842], Loss: 0.0213\n",
      "Epoch [4/5], Step [308/842], Loss: 0.0166\n",
      "Epoch [4/5], Step [309/842], Loss: 0.3384\n",
      "Epoch [4/5], Step [310/842], Loss: 0.3908\n",
      "Epoch [4/5], Step [311/842], Loss: 0.2524\n",
      "Epoch [4/5], Step [312/842], Loss: 0.0093\n",
      "Epoch [4/5], Step [313/842], Loss: 0.0027\n",
      "Epoch [4/5], Step [314/842], Loss: 0.0780\n",
      "Epoch [4/5], Step [315/842], Loss: 0.0146\n",
      "Epoch [4/5], Step [316/842], Loss: 0.0179\n",
      "Epoch [4/5], Step [317/842], Loss: 0.0879\n",
      "Epoch [4/5], Step [318/842], Loss: 0.0054\n",
      "Epoch [4/5], Step [319/842], Loss: 0.0270\n",
      "Epoch [4/5], Step [320/842], Loss: 0.0187\n",
      "Epoch [4/5], Step [321/842], Loss: 0.0970\n",
      "Epoch [4/5], Step [322/842], Loss: 0.0125\n",
      "Epoch [4/5], Step [323/842], Loss: 0.0282\n",
      "Epoch [4/5], Step [324/842], Loss: 0.0016\n",
      "Epoch [4/5], Step [325/842], Loss: 0.0092\n",
      "Epoch [4/5], Step [326/842], Loss: 0.0761\n",
      "Epoch [4/5], Step [327/842], Loss: 0.0284\n",
      "Epoch [4/5], Step [328/842], Loss: 0.0050\n",
      "Epoch [4/5], Step [329/842], Loss: 0.0094\n",
      "Epoch [4/5], Step [330/842], Loss: 0.0962\n",
      "Epoch [4/5], Step [331/842], Loss: 0.0376\n",
      "Epoch [4/5], Step [332/842], Loss: 0.0780\n",
      "Epoch [4/5], Step [333/842], Loss: 0.4796\n",
      "Epoch [4/5], Step [334/842], Loss: 0.0377\n",
      "Epoch [4/5], Step [335/842], Loss: 0.0182\n",
      "Epoch [4/5], Step [336/842], Loss: 0.0463\n",
      "Epoch [4/5], Step [337/842], Loss: 0.2947\n",
      "Epoch [4/5], Step [338/842], Loss: 0.0365\n",
      "Epoch [4/5], Step [339/842], Loss: 0.1787\n",
      "Epoch [4/5], Step [340/842], Loss: 0.0012\n",
      "Epoch [4/5], Step [341/842], Loss: 0.1922\n",
      "Epoch [4/5], Step [342/842], Loss: 0.0983\n",
      "Epoch [4/5], Step [343/842], Loss: 0.1997\n",
      "Epoch [4/5], Step [344/842], Loss: 0.0200\n",
      "Epoch [4/5], Step [345/842], Loss: 0.0078\n",
      "Epoch [4/5], Step [346/842], Loss: 0.0029\n",
      "Epoch [4/5], Step [347/842], Loss: 0.0621\n",
      "Epoch [4/5], Step [348/842], Loss: 0.0060\n",
      "Epoch [4/5], Step [349/842], Loss: 0.0299\n",
      "Epoch [4/5], Step [350/842], Loss: 0.0978\n",
      "Epoch [4/5], Step [351/842], Loss: 0.0780\n",
      "Epoch [4/5], Step [352/842], Loss: 0.0096\n",
      "Epoch [4/5], Step [353/842], Loss: 0.2748\n",
      "Epoch [4/5], Step [354/842], Loss: 0.0105\n",
      "Epoch [4/5], Step [355/842], Loss: 0.0269\n",
      "Epoch [4/5], Step [356/842], Loss: 0.0055\n",
      "Epoch [4/5], Step [357/842], Loss: 0.1317\n",
      "Epoch [4/5], Step [358/842], Loss: 0.1825\n",
      "Epoch [4/5], Step [359/842], Loss: 0.0646\n",
      "Epoch [4/5], Step [360/842], Loss: 0.0920\n",
      "Epoch [4/5], Step [361/842], Loss: 0.0048\n",
      "Epoch [4/5], Step [362/842], Loss: 0.0430\n",
      "Epoch [4/5], Step [363/842], Loss: 0.0030\n",
      "Epoch [4/5], Step [364/842], Loss: 0.0024\n",
      "Epoch [4/5], Step [365/842], Loss: 0.0160\n",
      "Epoch [4/5], Step [366/842], Loss: 0.0962\n",
      "Epoch [4/5], Step [367/842], Loss: 0.0074\n",
      "Epoch [4/5], Step [368/842], Loss: 0.0051\n",
      "Epoch [4/5], Step [369/842], Loss: 0.0033\n",
      "Epoch [4/5], Step [370/842], Loss: 0.0628\n",
      "Epoch [4/5], Step [371/842], Loss: 0.3836\n",
      "Epoch [4/5], Step [372/842], Loss: 0.0082\n",
      "Epoch [4/5], Step [373/842], Loss: 0.0086\n",
      "Epoch [4/5], Step [374/842], Loss: 0.0082\n",
      "Epoch [4/5], Step [375/842], Loss: 0.1738\n",
      "Epoch [4/5], Step [376/842], Loss: 0.1503\n",
      "Epoch [4/5], Step [377/842], Loss: 0.0963\n",
      "Epoch [4/5], Step [378/842], Loss: 0.0301\n",
      "Epoch [4/5], Step [379/842], Loss: 0.0204\n",
      "Epoch [4/5], Step [380/842], Loss: 0.0131\n",
      "Epoch [4/5], Step [381/842], Loss: 0.0117\n",
      "Epoch [4/5], Step [382/842], Loss: 0.0883\n",
      "Epoch [4/5], Step [383/842], Loss: 0.0281\n",
      "Epoch [4/5], Step [384/842], Loss: 0.0041\n",
      "Epoch [4/5], Step [385/842], Loss: 0.0226\n",
      "Epoch [4/5], Step [386/842], Loss: 0.3514\n",
      "Epoch [4/5], Step [387/842], Loss: 0.1348\n",
      "Epoch [4/5], Step [388/842], Loss: 0.0704\n",
      "Epoch [4/5], Step [389/842], Loss: 0.0253\n",
      "Epoch [4/5], Step [390/842], Loss: 0.1323\n",
      "Epoch [4/5], Step [391/842], Loss: 0.0150\n",
      "Epoch [4/5], Step [392/842], Loss: 0.2188\n",
      "Epoch [4/5], Step [393/842], Loss: 0.0171\n",
      "Epoch [4/5], Step [394/842], Loss: 0.0649\n",
      "Epoch [4/5], Step [395/842], Loss: 0.0014\n",
      "Epoch [4/5], Step [396/842], Loss: 0.4040\n",
      "Epoch [4/5], Step [397/842], Loss: 0.0265\n",
      "Epoch [4/5], Step [398/842], Loss: 0.0397\n",
      "Epoch [4/5], Step [399/842], Loss: 0.3244\n",
      "Epoch [4/5], Step [400/842], Loss: 0.0035\n",
      "Epoch [4/5], Step [401/842], Loss: 0.0319\n",
      "Epoch [4/5], Step [402/842], Loss: 0.0077\n",
      "Epoch [4/5], Step [403/842], Loss: 0.2715\n",
      "Epoch [4/5], Step [404/842], Loss: 0.0066\n",
      "Epoch [4/5], Step [405/842], Loss: 0.2289\n",
      "Epoch [4/5], Step [406/842], Loss: 0.0014\n",
      "Epoch [4/5], Step [407/842], Loss: 0.0261\n",
      "Epoch [4/5], Step [408/842], Loss: 0.0548\n",
      "Epoch [4/5], Step [409/842], Loss: 0.0051\n",
      "Epoch [4/5], Step [410/842], Loss: 0.2253\n",
      "Epoch [4/5], Step [411/842], Loss: 0.0026\n",
      "Epoch [4/5], Step [412/842], Loss: 0.3756\n",
      "Epoch [4/5], Step [413/842], Loss: 0.0053\n",
      "Epoch [4/5], Step [414/842], Loss: 0.1776\n",
      "Epoch [4/5], Step [415/842], Loss: 0.0503\n",
      "Epoch [4/5], Step [416/842], Loss: 0.0651\n",
      "Epoch [4/5], Step [417/842], Loss: 0.2458\n",
      "Epoch [4/5], Step [418/842], Loss: 0.0211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [419/842], Loss: 0.0026\n",
      "Epoch [4/5], Step [420/842], Loss: 0.0571\n",
      "Epoch [4/5], Step [421/842], Loss: 0.1608\n",
      "Epoch [4/5], Step [422/842], Loss: 0.0128\n",
      "Epoch [4/5], Step [423/842], Loss: 0.0219\n",
      "Epoch [4/5], Step [424/842], Loss: 0.0673\n",
      "Epoch [4/5], Step [425/842], Loss: 0.3015\n",
      "Epoch [4/5], Step [426/842], Loss: 0.0636\n",
      "Epoch [4/5], Step [427/842], Loss: 0.3171\n",
      "Epoch [4/5], Step [428/842], Loss: 0.0062\n",
      "Epoch [4/5], Step [429/842], Loss: 0.0074\n",
      "Epoch [4/5], Step [430/842], Loss: 0.1388\n",
      "Epoch [4/5], Step [431/842], Loss: 0.0083\n",
      "Epoch [4/5], Step [432/842], Loss: 0.0329\n",
      "Epoch [4/5], Step [433/842], Loss: 0.0401\n",
      "Epoch [4/5], Step [434/842], Loss: 0.0674\n",
      "Epoch [4/5], Step [435/842], Loss: 0.0396\n",
      "Epoch [4/5], Step [436/842], Loss: 0.0075\n",
      "Epoch [4/5], Step [437/842], Loss: 0.0199\n",
      "Epoch [4/5], Step [438/842], Loss: 0.2186\n",
      "Epoch [4/5], Step [439/842], Loss: 0.0900\n",
      "Epoch [4/5], Step [440/842], Loss: 0.0032\n",
      "Epoch [4/5], Step [441/842], Loss: 0.1042\n",
      "Epoch [4/5], Step [442/842], Loss: 0.0158\n",
      "Epoch [4/5], Step [443/842], Loss: 0.1765\n",
      "Epoch [4/5], Step [444/842], Loss: 0.0128\n",
      "Epoch [4/5], Step [445/842], Loss: 0.0322\n",
      "Epoch [4/5], Step [446/842], Loss: 0.2463\n",
      "Epoch [4/5], Step [447/842], Loss: 0.0090\n",
      "Epoch [4/5], Step [448/842], Loss: 0.0076\n",
      "Epoch [4/5], Step [449/842], Loss: 0.2964\n",
      "Epoch [4/5], Step [450/842], Loss: 0.0055\n",
      "Epoch [4/5], Step [451/842], Loss: 0.1341\n",
      "Epoch [4/5], Step [452/842], Loss: 0.1421\n",
      "Epoch [4/5], Step [453/842], Loss: 0.0969\n",
      "Epoch [4/5], Step [454/842], Loss: 0.0056\n",
      "Epoch [4/5], Step [455/842], Loss: 0.0342\n",
      "Epoch [4/5], Step [456/842], Loss: 0.0253\n",
      "Epoch [4/5], Step [457/842], Loss: 0.0496\n",
      "Epoch [4/5], Step [458/842], Loss: 0.1631\n",
      "Epoch [4/5], Step [459/842], Loss: 0.0269\n",
      "Epoch [4/5], Step [460/842], Loss: 0.1288\n",
      "Epoch [4/5], Step [461/842], Loss: 0.0389\n",
      "Epoch [4/5], Step [462/842], Loss: 0.0430\n",
      "Epoch [4/5], Step [463/842], Loss: 0.2075\n",
      "Epoch [4/5], Step [464/842], Loss: 0.0185\n",
      "Epoch [4/5], Step [465/842], Loss: 0.1776\n",
      "Epoch [4/5], Step [466/842], Loss: 0.0294\n",
      "Epoch [4/5], Step [467/842], Loss: 0.0078\n",
      "Epoch [4/5], Step [468/842], Loss: 0.2080\n",
      "Epoch [4/5], Step [469/842], Loss: 0.0307\n",
      "Epoch [4/5], Step [470/842], Loss: 0.0873\n",
      "Epoch [4/5], Step [471/842], Loss: 0.0149\n",
      "Epoch [4/5], Step [472/842], Loss: 0.0512\n",
      "Epoch [4/5], Step [473/842], Loss: 0.0325\n",
      "Epoch [4/5], Step [474/842], Loss: 0.0710\n",
      "Epoch [4/5], Step [475/842], Loss: 0.0248\n",
      "Epoch [4/5], Step [476/842], Loss: 0.1046\n",
      "Epoch [4/5], Step [477/842], Loss: 0.0465\n",
      "Epoch [4/5], Step [478/842], Loss: 0.0203\n",
      "Epoch [4/5], Step [479/842], Loss: 0.0044\n",
      "Epoch [4/5], Step [480/842], Loss: 0.0530\n",
      "Epoch [4/5], Step [481/842], Loss: 0.0626\n",
      "Epoch [4/5], Step [482/842], Loss: 0.0992\n",
      "Epoch [4/5], Step [483/842], Loss: 0.0024\n",
      "Epoch [4/5], Step [484/842], Loss: 0.1512\n",
      "Epoch [4/5], Step [485/842], Loss: 0.0047\n",
      "Epoch [4/5], Step [486/842], Loss: 0.1152\n",
      "Epoch [4/5], Step [487/842], Loss: 0.0430\n",
      "Epoch [4/5], Step [488/842], Loss: 0.0068\n",
      "Epoch [4/5], Step [489/842], Loss: 0.0626\n",
      "Epoch [4/5], Step [490/842], Loss: 0.0226\n",
      "Epoch [4/5], Step [491/842], Loss: 0.0035\n",
      "Epoch [4/5], Step [492/842], Loss: 0.0050\n",
      "Epoch [4/5], Step [493/842], Loss: 0.1638\n",
      "Epoch [4/5], Step [494/842], Loss: 0.0054\n",
      "Epoch [4/5], Step [495/842], Loss: 0.0042\n",
      "Epoch [4/5], Step [496/842], Loss: 0.1305\n",
      "Epoch [4/5], Step [497/842], Loss: 0.0015\n",
      "Epoch [4/5], Step [498/842], Loss: 0.0225\n",
      "Epoch [4/5], Step [499/842], Loss: 0.0058\n",
      "Epoch [4/5], Step [500/842], Loss: 0.0035\n",
      "Epoch [4/5], Step [501/842], Loss: 0.0089\n",
      "Epoch [4/5], Step [502/842], Loss: 0.0071\n",
      "Epoch [4/5], Step [503/842], Loss: 0.0820\n",
      "Epoch [4/5], Step [504/842], Loss: 0.0135\n",
      "Epoch [4/5], Step [505/842], Loss: 0.0378\n",
      "Epoch [4/5], Step [506/842], Loss: 0.0115\n",
      "Epoch [4/5], Step [507/842], Loss: 0.0722\n",
      "Epoch [4/5], Step [508/842], Loss: 0.1599\n",
      "Epoch [4/5], Step [509/842], Loss: 0.3337\n",
      "Epoch [4/5], Step [510/842], Loss: 0.0176\n",
      "Epoch [4/5], Step [511/842], Loss: 0.1466\n",
      "Epoch [4/5], Step [512/842], Loss: 0.0101\n",
      "Epoch [4/5], Step [513/842], Loss: 0.0039\n",
      "Epoch [4/5], Step [514/842], Loss: 0.1151\n",
      "Epoch [4/5], Step [515/842], Loss: 0.0450\n",
      "Epoch [4/5], Step [516/842], Loss: 0.0169\n",
      "Epoch [4/5], Step [517/842], Loss: 0.0073\n",
      "Epoch [4/5], Step [518/842], Loss: 0.0169\n",
      "Epoch [4/5], Step [519/842], Loss: 0.2354\n",
      "Epoch [4/5], Step [520/842], Loss: 0.1808\n",
      "Epoch [4/5], Step [521/842], Loss: 0.0050\n",
      "Epoch [4/5], Step [522/842], Loss: 0.0433\n",
      "Epoch [4/5], Step [523/842], Loss: 0.2029\n",
      "Epoch [4/5], Step [524/842], Loss: 0.0064\n",
      "Epoch [4/5], Step [525/842], Loss: 0.0772\n",
      "Epoch [4/5], Step [526/842], Loss: 0.1794\n",
      "Epoch [4/5], Step [527/842], Loss: 0.0073\n",
      "Epoch [4/5], Step [528/842], Loss: 0.0057\n",
      "Epoch [4/5], Step [529/842], Loss: 0.2034\n",
      "Epoch [4/5], Step [530/842], Loss: 0.0947\n",
      "Epoch [4/5], Step [531/842], Loss: 0.0521\n",
      "Epoch [4/5], Step [532/842], Loss: 0.0386\n",
      "Epoch [4/5], Step [533/842], Loss: 0.0170\n",
      "Epoch [4/5], Step [534/842], Loss: 0.0921\n",
      "Epoch [4/5], Step [535/842], Loss: 0.1106\n",
      "Epoch [4/5], Step [536/842], Loss: 0.2664\n",
      "Epoch [4/5], Step [537/842], Loss: 0.1035\n",
      "Epoch [4/5], Step [538/842], Loss: 0.0170\n",
      "Epoch [4/5], Step [539/842], Loss: 0.0112\n",
      "Epoch [4/5], Step [540/842], Loss: 0.0495\n",
      "Epoch [4/5], Step [541/842], Loss: 0.0840\n",
      "Epoch [4/5], Step [542/842], Loss: 0.0169\n",
      "Epoch [4/5], Step [543/842], Loss: 0.0386\n",
      "Epoch [4/5], Step [544/842], Loss: 0.0019\n",
      "Epoch [4/5], Step [545/842], Loss: 0.4419\n",
      "Epoch [4/5], Step [546/842], Loss: 0.5075\n",
      "Epoch [4/5], Step [547/842], Loss: 0.0379\n",
      "Epoch [4/5], Step [548/842], Loss: 0.1225\n",
      "Epoch [4/5], Step [549/842], Loss: 0.0196\n",
      "Epoch [4/5], Step [550/842], Loss: 0.0725\n",
      "Epoch [4/5], Step [551/842], Loss: 0.0683\n",
      "Epoch [4/5], Step [552/842], Loss: 0.0071\n",
      "Epoch [4/5], Step [553/842], Loss: 0.0041\n",
      "Epoch [4/5], Step [554/842], Loss: 0.0939\n",
      "Epoch [4/5], Step [555/842], Loss: 0.1708\n",
      "Epoch [4/5], Step [556/842], Loss: 0.0039\n",
      "Epoch [4/5], Step [557/842], Loss: 0.1128\n",
      "Epoch [4/5], Step [558/842], Loss: 0.0031\n",
      "Epoch [4/5], Step [559/842], Loss: 0.0222\n",
      "Epoch [4/5], Step [560/842], Loss: 0.0615\n",
      "Epoch [4/5], Step [561/842], Loss: 0.2556\n",
      "Epoch [4/5], Step [562/842], Loss: 0.0612\n",
      "Epoch [4/5], Step [563/842], Loss: 0.0082\n",
      "Epoch [4/5], Step [564/842], Loss: 0.0243\n",
      "Epoch [4/5], Step [565/842], Loss: 0.0851\n",
      "Epoch [4/5], Step [566/842], Loss: 0.0163\n",
      "Epoch [4/5], Step [567/842], Loss: 0.0111\n",
      "Epoch [4/5], Step [568/842], Loss: 0.0527\n",
      "Epoch [4/5], Step [569/842], Loss: 0.0087\n",
      "Epoch [4/5], Step [570/842], Loss: 0.0850\n",
      "Epoch [4/5], Step [571/842], Loss: 0.0139\n",
      "Epoch [4/5], Step [572/842], Loss: 0.0118\n",
      "Epoch [4/5], Step [573/842], Loss: 0.2119\n",
      "Epoch [4/5], Step [574/842], Loss: 0.0785\n",
      "Epoch [4/5], Step [575/842], Loss: 0.0027\n",
      "Epoch [4/5], Step [576/842], Loss: 0.0298\n",
      "Epoch [4/5], Step [577/842], Loss: 0.0091\n",
      "Epoch [4/5], Step [578/842], Loss: 0.0093\n",
      "Epoch [4/5], Step [579/842], Loss: 0.0315\n",
      "Epoch [4/5], Step [580/842], Loss: 0.3162\n",
      "Epoch [4/5], Step [581/842], Loss: 0.1353\n",
      "Epoch [4/5], Step [582/842], Loss: 0.0055\n",
      "Epoch [4/5], Step [583/842], Loss: 0.0383\n",
      "Epoch [4/5], Step [584/842], Loss: 0.1041\n",
      "Epoch [4/5], Step [585/842], Loss: 0.1109\n",
      "Epoch [4/5], Step [586/842], Loss: 0.0208\n",
      "Epoch [4/5], Step [587/842], Loss: 0.0302\n",
      "Epoch [4/5], Step [588/842], Loss: 0.0096\n",
      "Epoch [4/5], Step [589/842], Loss: 0.0050\n",
      "Epoch [4/5], Step [590/842], Loss: 0.0310\n",
      "Epoch [4/5], Step [591/842], Loss: 0.0088\n",
      "Epoch [4/5], Step [592/842], Loss: 0.0312\n",
      "Epoch [4/5], Step [593/842], Loss: 0.0036\n",
      "Epoch [4/5], Step [594/842], Loss: 0.2836\n",
      "Epoch [4/5], Step [595/842], Loss: 0.0737\n",
      "Epoch [4/5], Step [596/842], Loss: 0.1015\n",
      "Epoch [4/5], Step [597/842], Loss: 0.1127\n",
      "Epoch [4/5], Step [598/842], Loss: 0.2083\n",
      "Epoch [4/5], Step [599/842], Loss: 0.0081\n",
      "Epoch [4/5], Step [600/842], Loss: 0.4048\n",
      "Epoch [4/5], Step [601/842], Loss: 0.0068\n",
      "Epoch [4/5], Step [602/842], Loss: 0.0205\n",
      "Epoch [4/5], Step [603/842], Loss: 0.2318\n",
      "Epoch [4/5], Step [604/842], Loss: 0.2395\n",
      "Epoch [4/5], Step [605/842], Loss: 0.0713\n",
      "Epoch [4/5], Step [606/842], Loss: 0.0395\n",
      "Epoch [4/5], Step [607/842], Loss: 0.1744\n",
      "Epoch [4/5], Step [608/842], Loss: 0.0029\n",
      "Epoch [4/5], Step [609/842], Loss: 0.0337\n",
      "Epoch [4/5], Step [610/842], Loss: 0.1323\n",
      "Epoch [4/5], Step [611/842], Loss: 0.0572\n",
      "Epoch [4/5], Step [612/842], Loss: 0.0092\n",
      "Epoch [4/5], Step [613/842], Loss: 0.1365\n",
      "Epoch [4/5], Step [614/842], Loss: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [615/842], Loss: 0.1993\n",
      "Epoch [4/5], Step [616/842], Loss: 0.0332\n",
      "Epoch [4/5], Step [617/842], Loss: 0.3790\n",
      "Epoch [4/5], Step [618/842], Loss: 0.1182\n",
      "Epoch [4/5], Step [619/842], Loss: 0.1095\n",
      "Epoch [4/5], Step [620/842], Loss: 0.0784\n",
      "Epoch [4/5], Step [621/842], Loss: 0.0444\n",
      "Epoch [4/5], Step [622/842], Loss: 0.1628\n",
      "Epoch [4/5], Step [623/842], Loss: 0.0100\n",
      "Epoch [4/5], Step [624/842], Loss: 0.1964\n",
      "Epoch [4/5], Step [625/842], Loss: 0.0833\n",
      "Epoch [4/5], Step [626/842], Loss: 0.0254\n",
      "Epoch [4/5], Step [627/842], Loss: 0.0076\n",
      "Epoch [4/5], Step [628/842], Loss: 0.1504\n",
      "Epoch [4/5], Step [629/842], Loss: 0.0377\n",
      "Epoch [4/5], Step [630/842], Loss: 0.0682\n",
      "Epoch [4/5], Step [631/842], Loss: 0.0142\n",
      "Epoch [4/5], Step [632/842], Loss: 0.0177\n",
      "Epoch [4/5], Step [633/842], Loss: 0.0803\n",
      "Epoch [4/5], Step [634/842], Loss: 0.1773\n",
      "Epoch [4/5], Step [635/842], Loss: 0.1609\n",
      "Epoch [4/5], Step [636/842], Loss: 0.0108\n",
      "Epoch [4/5], Step [637/842], Loss: 0.1432\n",
      "Epoch [4/5], Step [638/842], Loss: 0.0560\n",
      "Epoch [4/5], Step [639/842], Loss: 0.1599\n",
      "Epoch [4/5], Step [640/842], Loss: 0.0678\n",
      "Epoch [4/5], Step [641/842], Loss: 0.3242\n",
      "Epoch [4/5], Step [642/842], Loss: 0.1725\n",
      "Epoch [4/5], Step [643/842], Loss: 0.1277\n",
      "Epoch [4/5], Step [644/842], Loss: 0.0285\n",
      "Epoch [4/5], Step [645/842], Loss: 0.1777\n",
      "Epoch [4/5], Step [646/842], Loss: 0.0314\n",
      "Epoch [4/5], Step [647/842], Loss: 0.1512\n",
      "Epoch [4/5], Step [648/842], Loss: 0.0096\n",
      "Epoch [4/5], Step [649/842], Loss: 0.0372\n",
      "Epoch [4/5], Step [650/842], Loss: 0.0092\n",
      "Epoch [4/5], Step [651/842], Loss: 0.0280\n",
      "Epoch [4/5], Step [652/842], Loss: 0.1178\n",
      "Epoch [4/5], Step [653/842], Loss: 0.0507\n",
      "Epoch [4/5], Step [654/842], Loss: 0.4484\n",
      "Epoch [4/5], Step [655/842], Loss: 0.0178\n",
      "Epoch [4/5], Step [656/842], Loss: 0.0388\n",
      "Epoch [4/5], Step [657/842], Loss: 0.0741\n",
      "Epoch [4/5], Step [658/842], Loss: 0.1915\n",
      "Epoch [4/5], Step [659/842], Loss: 0.2823\n",
      "Epoch [4/5], Step [660/842], Loss: 0.0487\n",
      "Epoch [4/5], Step [661/842], Loss: 0.0037\n",
      "Epoch [4/5], Step [662/842], Loss: 0.2295\n",
      "Epoch [4/5], Step [663/842], Loss: 0.0157\n",
      "Epoch [4/5], Step [664/842], Loss: 0.0412\n",
      "Epoch [4/5], Step [665/842], Loss: 0.0373\n",
      "Epoch [4/5], Step [666/842], Loss: 0.0193\n",
      "Epoch [4/5], Step [667/842], Loss: 0.0871\n",
      "Epoch [4/5], Step [668/842], Loss: 0.0627\n",
      "Epoch [4/5], Step [669/842], Loss: 0.1152\n",
      "Epoch [4/5], Step [670/842], Loss: 0.0361\n",
      "Epoch [4/5], Step [671/842], Loss: 0.2509\n",
      "Epoch [4/5], Step [672/842], Loss: 0.0029\n",
      "Epoch [4/5], Step [673/842], Loss: 0.0065\n",
      "Epoch [4/5], Step [674/842], Loss: 0.2397\n",
      "Epoch [4/5], Step [675/842], Loss: 0.1146\n",
      "Epoch [4/5], Step [676/842], Loss: 0.1037\n",
      "Epoch [4/5], Step [677/842], Loss: 0.1083\n",
      "Epoch [4/5], Step [678/842], Loss: 0.0083\n",
      "Epoch [4/5], Step [679/842], Loss: 0.0144\n",
      "Epoch [4/5], Step [680/842], Loss: 0.0330\n",
      "Epoch [4/5], Step [681/842], Loss: 0.0371\n",
      "Epoch [4/5], Step [682/842], Loss: 0.1723\n",
      "Epoch [4/5], Step [683/842], Loss: 0.1043\n",
      "Epoch [4/5], Step [684/842], Loss: 0.2496\n",
      "Epoch [4/5], Step [685/842], Loss: 0.0527\n",
      "Epoch [4/5], Step [686/842], Loss: 0.0944\n",
      "Epoch [4/5], Step [687/842], Loss: 0.0071\n",
      "Epoch [4/5], Step [688/842], Loss: 0.1493\n",
      "Epoch [4/5], Step [689/842], Loss: 0.0195\n",
      "Epoch [4/5], Step [690/842], Loss: 0.5302\n",
      "Epoch [4/5], Step [691/842], Loss: 0.0378\n",
      "Epoch [4/5], Step [692/842], Loss: 0.0842\n",
      "Epoch [4/5], Step [693/842], Loss: 0.0302\n",
      "Epoch [4/5], Step [694/842], Loss: 0.3039\n",
      "Epoch [4/5], Step [695/842], Loss: 0.1569\n",
      "Epoch [4/5], Step [696/842], Loss: 0.0154\n",
      "Epoch [4/5], Step [697/842], Loss: 0.0429\n",
      "Epoch [4/5], Step [698/842], Loss: 0.0233\n",
      "Epoch [4/5], Step [699/842], Loss: 0.0720\n",
      "Epoch [4/5], Step [700/842], Loss: 0.0026\n",
      "Epoch [4/5], Step [701/842], Loss: 0.0071\n",
      "Epoch [4/5], Step [702/842], Loss: 0.2016\n",
      "Epoch [4/5], Step [703/842], Loss: 0.0145\n",
      "Epoch [4/5], Step [704/842], Loss: 0.1066\n",
      "Epoch [4/5], Step [705/842], Loss: 0.0706\n",
      "Epoch [4/5], Step [706/842], Loss: 0.0054\n",
      "Epoch [4/5], Step [707/842], Loss: 0.0059\n",
      "Epoch [4/5], Step [708/842], Loss: 0.0169\n",
      "Epoch [4/5], Step [709/842], Loss: 0.0265\n",
      "Epoch [4/5], Step [710/842], Loss: 0.1791\n",
      "Epoch [4/5], Step [711/842], Loss: 0.0008\n",
      "Epoch [4/5], Step [712/842], Loss: 0.0771\n",
      "Epoch [4/5], Step [713/842], Loss: 0.2793\n",
      "Epoch [4/5], Step [714/842], Loss: 0.0455\n",
      "Epoch [4/5], Step [715/842], Loss: 0.0037\n",
      "Epoch [4/5], Step [716/842], Loss: 0.1146\n",
      "Epoch [4/5], Step [717/842], Loss: 0.0053\n",
      "Epoch [4/5], Step [718/842], Loss: 0.0075\n",
      "Epoch [4/5], Step [719/842], Loss: 0.0379\n",
      "Epoch [4/5], Step [720/842], Loss: 0.1254\n",
      "Epoch [4/5], Step [721/842], Loss: 0.0462\n",
      "Epoch [4/5], Step [722/842], Loss: 0.0017\n",
      "Epoch [4/5], Step [723/842], Loss: 0.0420\n",
      "Epoch [4/5], Step [724/842], Loss: 0.0061\n",
      "Epoch [4/5], Step [725/842], Loss: 0.1844\n",
      "Epoch [4/5], Step [726/842], Loss: 0.0353\n",
      "Epoch [4/5], Step [727/842], Loss: 0.0380\n",
      "Epoch [4/5], Step [728/842], Loss: 0.0032\n",
      "Epoch [4/5], Step [729/842], Loss: 0.0682\n",
      "Epoch [4/5], Step [730/842], Loss: 0.1395\n",
      "Epoch [4/5], Step [731/842], Loss: 0.2592\n",
      "Epoch [4/5], Step [732/842], Loss: 0.1660\n",
      "Epoch [4/5], Step [733/842], Loss: 0.0846\n",
      "Epoch [4/5], Step [734/842], Loss: 0.2619\n",
      "Epoch [4/5], Step [735/842], Loss: 0.0205\n",
      "Epoch [4/5], Step [736/842], Loss: 0.0825\n",
      "Epoch [4/5], Step [737/842], Loss: 0.0858\n",
      "Epoch [4/5], Step [738/842], Loss: 0.2190\n",
      "Epoch [4/5], Step [739/842], Loss: 0.0081\n",
      "Epoch [4/5], Step [740/842], Loss: 0.0740\n",
      "Epoch [4/5], Step [741/842], Loss: 0.0579\n",
      "Epoch [4/5], Step [742/842], Loss: 0.3034\n",
      "Epoch [4/5], Step [743/842], Loss: 0.2305\n",
      "Epoch [4/5], Step [744/842], Loss: 0.1029\n",
      "Epoch [4/5], Step [745/842], Loss: 0.0064\n",
      "Epoch [4/5], Step [746/842], Loss: 0.0174\n",
      "Epoch [4/5], Step [747/842], Loss: 0.0036\n",
      "Epoch [4/5], Step [748/842], Loss: 0.0555\n",
      "Epoch [4/5], Step [749/842], Loss: 0.0671\n",
      "Epoch [4/5], Step [750/842], Loss: 0.0997\n",
      "Epoch [4/5], Step [751/842], Loss: 0.1293\n",
      "Epoch [4/5], Step [752/842], Loss: 0.0643\n",
      "Epoch [4/5], Step [753/842], Loss: 0.0034\n",
      "Epoch [4/5], Step [754/842], Loss: 0.0411\n",
      "Epoch [4/5], Step [755/842], Loss: 0.1110\n",
      "Epoch [4/5], Step [756/842], Loss: 0.0066\n",
      "Epoch [4/5], Step [757/842], Loss: 0.0063\n",
      "Epoch [4/5], Step [758/842], Loss: 0.0032\n",
      "Epoch [4/5], Step [759/842], Loss: 0.1250\n",
      "Epoch [4/5], Step [760/842], Loss: 0.2715\n",
      "Epoch [4/5], Step [761/842], Loss: 0.1096\n",
      "Epoch [4/5], Step [762/842], Loss: 0.0535\n",
      "Epoch [4/5], Step [763/842], Loss: 0.0192\n",
      "Epoch [4/5], Step [764/842], Loss: 0.0044\n",
      "Epoch [4/5], Step [765/842], Loss: 0.0804\n",
      "Epoch [4/5], Step [766/842], Loss: 0.0397\n",
      "Epoch [4/5], Step [767/842], Loss: 0.0189\n",
      "Epoch [4/5], Step [768/842], Loss: 0.0091\n",
      "Epoch [4/5], Step [769/842], Loss: 0.0062\n",
      "Epoch [4/5], Step [770/842], Loss: 0.1996\n",
      "Epoch [4/5], Step [771/842], Loss: 0.0063\n",
      "Epoch [4/5], Step [772/842], Loss: 0.3240\n",
      "Epoch [4/5], Step [773/842], Loss: 0.0731\n",
      "Epoch [4/5], Step [774/842], Loss: 0.0021\n",
      "Epoch [4/5], Step [775/842], Loss: 0.2398\n",
      "Epoch [4/5], Step [776/842], Loss: 0.0684\n",
      "Epoch [4/5], Step [777/842], Loss: 0.0150\n",
      "Epoch [4/5], Step [778/842], Loss: 0.2978\n",
      "Epoch [4/5], Step [779/842], Loss: 0.0292\n",
      "Epoch [4/5], Step [780/842], Loss: 0.0906\n",
      "Epoch [4/5], Step [781/842], Loss: 0.0039\n",
      "Epoch [4/5], Step [782/842], Loss: 0.2669\n",
      "Epoch [4/5], Step [783/842], Loss: 0.0058\n",
      "Epoch [4/5], Step [784/842], Loss: 0.0074\n",
      "Epoch [4/5], Step [785/842], Loss: 0.0481\n",
      "Epoch [4/5], Step [786/842], Loss: 0.0122\n",
      "Epoch [4/5], Step [787/842], Loss: 0.0227\n",
      "Epoch [4/5], Step [788/842], Loss: 0.0115\n",
      "Epoch [4/5], Step [789/842], Loss: 0.0468\n",
      "Epoch [4/5], Step [790/842], Loss: 0.1561\n",
      "Epoch [4/5], Step [791/842], Loss: 0.0069\n",
      "Epoch [4/5], Step [792/842], Loss: 0.0192\n",
      "Epoch [4/5], Step [793/842], Loss: 0.2457\n",
      "Epoch [4/5], Step [794/842], Loss: 0.2476\n",
      "Epoch [4/5], Step [795/842], Loss: 0.0168\n",
      "Epoch [4/5], Step [796/842], Loss: 0.0500\n",
      "Epoch [4/5], Step [797/842], Loss: 0.0243\n",
      "Epoch [4/5], Step [798/842], Loss: 0.1953\n",
      "Epoch [4/5], Step [799/842], Loss: 0.0440\n",
      "Epoch [4/5], Step [800/842], Loss: 0.0043\n",
      "Epoch [4/5], Step [801/842], Loss: 0.0033\n",
      "Epoch [4/5], Step [802/842], Loss: 0.1108\n",
      "Epoch [4/5], Step [803/842], Loss: 0.2686\n",
      "Epoch [4/5], Step [804/842], Loss: 0.3696\n",
      "Epoch [4/5], Step [805/842], Loss: 0.0272\n",
      "Epoch [4/5], Step [806/842], Loss: 0.0907\n",
      "Epoch [4/5], Step [807/842], Loss: 0.2373\n",
      "Epoch [4/5], Step [808/842], Loss: 0.0094\n",
      "Epoch [4/5], Step [809/842], Loss: 0.2269\n",
      "Epoch [4/5], Step [810/842], Loss: 0.0280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [811/842], Loss: 0.0127\n",
      "Epoch [4/5], Step [812/842], Loss: 0.0075\n",
      "Epoch [4/5], Step [813/842], Loss: 0.0594\n",
      "Epoch [4/5], Step [814/842], Loss: 0.1937\n",
      "Epoch [4/5], Step [815/842], Loss: 0.0372\n",
      "Epoch [4/5], Step [816/842], Loss: 0.1618\n",
      "Epoch [4/5], Step [817/842], Loss: 0.0735\n",
      "Epoch [4/5], Step [818/842], Loss: 0.0443\n",
      "Epoch [4/5], Step [819/842], Loss: 0.0511\n",
      "Epoch [4/5], Step [820/842], Loss: 0.0151\n",
      "Epoch [4/5], Step [821/842], Loss: 0.0463\n",
      "Epoch [4/5], Step [822/842], Loss: 0.6182\n",
      "Epoch [4/5], Step [823/842], Loss: 0.0351\n",
      "Epoch [4/5], Step [824/842], Loss: 0.2563\n",
      "Epoch [4/5], Step [825/842], Loss: 0.3183\n",
      "Epoch [4/5], Step [826/842], Loss: 0.0661\n",
      "Epoch [4/5], Step [827/842], Loss: 0.0681\n",
      "Epoch [4/5], Step [828/842], Loss: 0.2096\n",
      "Epoch [4/5], Step [829/842], Loss: 0.3072\n",
      "Epoch [4/5], Step [830/842], Loss: 0.1460\n",
      "Epoch [4/5], Step [831/842], Loss: 0.2745\n",
      "Epoch [4/5], Step [832/842], Loss: 0.1265\n",
      "Epoch [4/5], Step [833/842], Loss: 0.0312\n",
      "Epoch [4/5], Step [834/842], Loss: 0.0994\n",
      "Epoch [4/5], Step [835/842], Loss: 0.0909\n",
      "Epoch [4/5], Step [836/842], Loss: 0.0868\n",
      "Epoch [4/5], Step [837/842], Loss: 0.0278\n",
      "Epoch [4/5], Step [838/842], Loss: 0.1746\n",
      "Epoch [4/5], Step [839/842], Loss: 0.2452\n",
      "Epoch [4/5], Step [840/842], Loss: 0.1099\n",
      "Epoch [4/5], Step [841/842], Loss: 0.2964\n",
      "Epoch [4/5], Step [842/842], Loss: 0.0629\n",
      "Epoch [4/5], Step [843/842], Loss: 0.0197\n",
      "Epoch [5/5], Step [1/842], Loss: 0.4252\n",
      "Epoch [5/5], Step [2/842], Loss: 0.1629\n",
      "Epoch [5/5], Step [3/842], Loss: 0.0300\n",
      "Epoch [5/5], Step [4/842], Loss: 0.0210\n",
      "Epoch [5/5], Step [5/842], Loss: 0.0410\n",
      "Epoch [5/5], Step [6/842], Loss: 0.0164\n",
      "Epoch [5/5], Step [7/842], Loss: 0.0047\n",
      "Epoch [5/5], Step [8/842], Loss: 0.1027\n",
      "Epoch [5/5], Step [9/842], Loss: 0.0140\n",
      "Epoch [5/5], Step [10/842], Loss: 0.0067\n",
      "Epoch [5/5], Step [11/842], Loss: 0.0607\n",
      "Epoch [5/5], Step [12/842], Loss: 0.0041\n",
      "Epoch [5/5], Step [13/842], Loss: 0.0739\n",
      "Epoch [5/5], Step [14/842], Loss: 0.0808\n",
      "Epoch [5/5], Step [15/842], Loss: 0.0083\n",
      "Epoch [5/5], Step [16/842], Loss: 0.0718\n",
      "Epoch [5/5], Step [17/842], Loss: 0.0111\n",
      "Epoch [5/5], Step [18/842], Loss: 0.1238\n",
      "Epoch [5/5], Step [19/842], Loss: 0.0012\n",
      "Epoch [5/5], Step [20/842], Loss: 0.0091\n",
      "Epoch [5/5], Step [21/842], Loss: 0.0726\n",
      "Epoch [5/5], Step [22/842], Loss: 0.0099\n",
      "Epoch [5/5], Step [23/842], Loss: 0.0335\n",
      "Epoch [5/5], Step [24/842], Loss: 0.0179\n",
      "Epoch [5/5], Step [25/842], Loss: 0.1882\n",
      "Epoch [5/5], Step [26/842], Loss: 0.0006\n",
      "Epoch [5/5], Step [27/842], Loss: 0.0673\n",
      "Epoch [5/5], Step [28/842], Loss: 0.0558\n",
      "Epoch [5/5], Step [29/842], Loss: 0.0357\n",
      "Epoch [5/5], Step [30/842], Loss: 0.0048\n",
      "Epoch [5/5], Step [31/842], Loss: 0.0086\n",
      "Epoch [5/5], Step [32/842], Loss: 0.0806\n",
      "Epoch [5/5], Step [33/842], Loss: 0.0075\n",
      "Epoch [5/5], Step [34/842], Loss: 0.0265\n",
      "Epoch [5/5], Step [35/842], Loss: 0.0030\n",
      "Epoch [5/5], Step [36/842], Loss: 0.0066\n",
      "Epoch [5/5], Step [37/842], Loss: 0.0012\n",
      "Epoch [5/5], Step [38/842], Loss: 0.0369\n",
      "Epoch [5/5], Step [39/842], Loss: 0.0095\n",
      "Epoch [5/5], Step [40/842], Loss: 0.0127\n",
      "Epoch [5/5], Step [41/842], Loss: 0.0044\n",
      "Epoch [5/5], Step [42/842], Loss: 0.0071\n",
      "Epoch [5/5], Step [43/842], Loss: 0.0178\n",
      "Epoch [5/5], Step [44/842], Loss: 0.0162\n",
      "Epoch [5/5], Step [45/842], Loss: 0.0151\n",
      "Epoch [5/5], Step [46/842], Loss: 0.1533\n",
      "Epoch [5/5], Step [47/842], Loss: 0.0115\n",
      "Epoch [5/5], Step [48/842], Loss: 0.0010\n",
      "Epoch [5/5], Step [49/842], Loss: 0.0017\n",
      "Epoch [5/5], Step [50/842], Loss: 0.1958\n",
      "Epoch [5/5], Step [51/842], Loss: 0.0141\n",
      "Epoch [5/5], Step [52/842], Loss: 0.0379\n",
      "Epoch [5/5], Step [53/842], Loss: 0.0104\n",
      "Epoch [5/5], Step [54/842], Loss: 0.0051\n",
      "Epoch [5/5], Step [55/842], Loss: 0.0402\n",
      "Epoch [5/5], Step [56/842], Loss: 0.0339\n",
      "Epoch [5/5], Step [57/842], Loss: 0.0054\n",
      "Epoch [5/5], Step [58/842], Loss: 0.0042\n",
      "Epoch [5/5], Step [59/842], Loss: 0.1246\n",
      "Epoch [5/5], Step [60/842], Loss: 0.2170\n",
      "Epoch [5/5], Step [61/842], Loss: 0.0027\n",
      "Epoch [5/5], Step [62/842], Loss: 0.0086\n",
      "Epoch [5/5], Step [63/842], Loss: 0.0096\n",
      "Epoch [5/5], Step [64/842], Loss: 0.0424\n",
      "Epoch [5/5], Step [65/842], Loss: 0.0025\n",
      "Epoch [5/5], Step [66/842], Loss: 0.0414\n",
      "Epoch [5/5], Step [67/842], Loss: 0.0422\n",
      "Epoch [5/5], Step [68/842], Loss: 0.0572\n",
      "Epoch [5/5], Step [69/842], Loss: 0.0105\n",
      "Epoch [5/5], Step [70/842], Loss: 0.0021\n",
      "Epoch [5/5], Step [71/842], Loss: 0.0040\n",
      "Epoch [5/5], Step [72/842], Loss: 0.1170\n",
      "Epoch [5/5], Step [73/842], Loss: 0.1418\n",
      "Epoch [5/5], Step [74/842], Loss: 0.0201\n",
      "Epoch [5/5], Step [75/842], Loss: 0.0777\n",
      "Epoch [5/5], Step [76/842], Loss: 0.0035\n",
      "Epoch [5/5], Step [77/842], Loss: 0.0009\n",
      "Epoch [5/5], Step [78/842], Loss: 0.0076\n",
      "Epoch [5/5], Step [79/842], Loss: 0.1446\n",
      "Epoch [5/5], Step [80/842], Loss: 0.0342\n",
      "Epoch [5/5], Step [81/842], Loss: 0.0377\n",
      "Epoch [5/5], Step [82/842], Loss: 0.0155\n",
      "Epoch [5/5], Step [83/842], Loss: 0.0159\n",
      "Epoch [5/5], Step [84/842], Loss: 0.0094\n",
      "Epoch [5/5], Step [85/842], Loss: 0.0436\n",
      "Epoch [5/5], Step [86/842], Loss: 0.0074\n",
      "Epoch [5/5], Step [87/842], Loss: 0.0125\n",
      "Epoch [5/5], Step [88/842], Loss: 0.0174\n",
      "Epoch [5/5], Step [89/842], Loss: 0.0026\n",
      "Epoch [5/5], Step [90/842], Loss: 0.0385\n",
      "Epoch [5/5], Step [91/842], Loss: 0.0150\n",
      "Epoch [5/5], Step [92/842], Loss: 0.0024\n",
      "Epoch [5/5], Step [93/842], Loss: 0.0010\n",
      "Epoch [5/5], Step [94/842], Loss: 0.0013\n",
      "Epoch [5/5], Step [95/842], Loss: 0.1447\n",
      "Epoch [5/5], Step [96/842], Loss: 0.0874\n",
      "Epoch [5/5], Step [97/842], Loss: 0.0153\n",
      "Epoch [5/5], Step [98/842], Loss: 0.0020\n",
      "Epoch [5/5], Step [99/842], Loss: 0.0038\n",
      "Epoch [5/5], Step [100/842], Loss: 0.0535\n",
      "Epoch [5/5], Step [101/842], Loss: 0.0014\n",
      "Epoch [5/5], Step [102/842], Loss: 0.1109\n",
      "Epoch [5/5], Step [103/842], Loss: 0.0222\n",
      "Epoch [5/5], Step [104/842], Loss: 0.0073\n",
      "Epoch [5/5], Step [105/842], Loss: 0.0111\n",
      "Epoch [5/5], Step [106/842], Loss: 0.0033\n",
      "Epoch [5/5], Step [107/842], Loss: 0.0046\n",
      "Epoch [5/5], Step [108/842], Loss: 0.0173\n",
      "Epoch [5/5], Step [109/842], Loss: 0.0547\n",
      "Epoch [5/5], Step [110/842], Loss: 0.0049\n",
      "Epoch [5/5], Step [111/842], Loss: 0.0803\n",
      "Epoch [5/5], Step [112/842], Loss: 0.0094\n",
      "Epoch [5/5], Step [113/842], Loss: 0.2994\n",
      "Epoch [5/5], Step [114/842], Loss: 0.0026\n",
      "Epoch [5/5], Step [115/842], Loss: 0.0047\n",
      "Epoch [5/5], Step [116/842], Loss: 0.1587\n",
      "Epoch [5/5], Step [117/842], Loss: 0.0471\n",
      "Epoch [5/5], Step [118/842], Loss: 0.0053\n",
      "Epoch [5/5], Step [119/842], Loss: 0.1257\n",
      "Epoch [5/5], Step [120/842], Loss: 0.0897\n",
      "Epoch [5/5], Step [121/842], Loss: 0.0353\n",
      "Epoch [5/5], Step [122/842], Loss: 0.1063\n",
      "Epoch [5/5], Step [123/842], Loss: 0.0046\n",
      "Epoch [5/5], Step [124/842], Loss: 0.0175\n",
      "Epoch [5/5], Step [125/842], Loss: 0.0029\n",
      "Epoch [5/5], Step [126/842], Loss: 0.0083\n",
      "Epoch [5/5], Step [127/842], Loss: 0.0114\n",
      "Epoch [5/5], Step [128/842], Loss: 0.0111\n",
      "Epoch [5/5], Step [129/842], Loss: 0.0361\n",
      "Epoch [5/5], Step [130/842], Loss: 0.0098\n",
      "Epoch [5/5], Step [131/842], Loss: 0.1774\n",
      "Epoch [5/5], Step [132/842], Loss: 0.1328\n",
      "Epoch [5/5], Step [133/842], Loss: 0.0048\n",
      "Epoch [5/5], Step [134/842], Loss: 0.0029\n",
      "Epoch [5/5], Step [135/842], Loss: 0.0535\n",
      "Epoch [5/5], Step [136/842], Loss: 0.0204\n",
      "Epoch [5/5], Step [137/842], Loss: 0.0108\n",
      "Epoch [5/5], Step [138/842], Loss: 0.0610\n",
      "Epoch [5/5], Step [139/842], Loss: 0.0860\n",
      "Epoch [5/5], Step [140/842], Loss: 0.0028\n",
      "Epoch [5/5], Step [141/842], Loss: 0.0052\n",
      "Epoch [5/5], Step [142/842], Loss: 0.0184\n",
      "Epoch [5/5], Step [143/842], Loss: 0.0571\n",
      "Epoch [5/5], Step [144/842], Loss: 0.0720\n",
      "Epoch [5/5], Step [145/842], Loss: 0.0307\n",
      "Epoch [5/5], Step [146/842], Loss: 0.0090\n",
      "Epoch [5/5], Step [147/842], Loss: 0.0034\n",
      "Epoch [5/5], Step [148/842], Loss: 0.0143\n",
      "Epoch [5/5], Step [149/842], Loss: 0.0077\n",
      "Epoch [5/5], Step [150/842], Loss: 0.0286\n",
      "Epoch [5/5], Step [151/842], Loss: 0.0022\n",
      "Epoch [5/5], Step [152/842], Loss: 0.0017\n",
      "Epoch [5/5], Step [153/842], Loss: 0.3638\n",
      "Epoch [5/5], Step [154/842], Loss: 0.1021\n",
      "Epoch [5/5], Step [155/842], Loss: 0.0155\n",
      "Epoch [5/5], Step [156/842], Loss: 0.0332\n",
      "Epoch [5/5], Step [157/842], Loss: 0.0014\n",
      "Epoch [5/5], Step [158/842], Loss: 0.0234\n",
      "Epoch [5/5], Step [159/842], Loss: 0.0110\n",
      "Epoch [5/5], Step [160/842], Loss: 0.0497\n",
      "Epoch [5/5], Step [161/842], Loss: 0.0465\n",
      "Epoch [5/5], Step [162/842], Loss: 0.0317\n",
      "Epoch [5/5], Step [163/842], Loss: 0.0097\n",
      "Epoch [5/5], Step [164/842], Loss: 0.0400\n",
      "Epoch [5/5], Step [165/842], Loss: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [166/842], Loss: 0.0261\n",
      "Epoch [5/5], Step [167/842], Loss: 0.0145\n",
      "Epoch [5/5], Step [168/842], Loss: 0.0108\n",
      "Epoch [5/5], Step [169/842], Loss: 0.0104\n",
      "Epoch [5/5], Step [170/842], Loss: 0.0080\n",
      "Epoch [5/5], Step [171/842], Loss: 0.0024\n",
      "Epoch [5/5], Step [172/842], Loss: 0.0036\n",
      "Epoch [5/5], Step [173/842], Loss: 0.0094\n",
      "Epoch [5/5], Step [174/842], Loss: 0.0294\n",
      "Epoch [5/5], Step [175/842], Loss: 0.0025\n",
      "Epoch [5/5], Step [176/842], Loss: 0.0053\n",
      "Epoch [5/5], Step [177/842], Loss: 0.0014\n",
      "Epoch [5/5], Step [178/842], Loss: 0.0225\n",
      "Epoch [5/5], Step [179/842], Loss: 0.0332\n",
      "Epoch [5/5], Step [180/842], Loss: 0.0052\n",
      "Epoch [5/5], Step [181/842], Loss: 0.0055\n",
      "Epoch [5/5], Step [182/842], Loss: 0.0011\n",
      "Epoch [5/5], Step [183/842], Loss: 0.0121\n",
      "Epoch [5/5], Step [184/842], Loss: 0.0412\n",
      "Epoch [5/5], Step [185/842], Loss: 0.0681\n",
      "Epoch [5/5], Step [186/842], Loss: 0.0796\n",
      "Epoch [5/5], Step [187/842], Loss: 0.0177\n",
      "Epoch [5/5], Step [188/842], Loss: 0.2229\n",
      "Epoch [5/5], Step [189/842], Loss: 0.0083\n",
      "Epoch [5/5], Step [190/842], Loss: 0.0021\n",
      "Epoch [5/5], Step [191/842], Loss: 0.0264\n",
      "Epoch [5/5], Step [192/842], Loss: 0.1835\n",
      "Epoch [5/5], Step [193/842], Loss: 0.0074\n",
      "Epoch [5/5], Step [194/842], Loss: 0.0523\n",
      "Epoch [5/5], Step [195/842], Loss: 0.0856\n",
      "Epoch [5/5], Step [196/842], Loss: 0.0028\n",
      "Epoch [5/5], Step [197/842], Loss: 0.0321\n",
      "Epoch [5/5], Step [198/842], Loss: 0.1893\n",
      "Epoch [5/5], Step [199/842], Loss: 0.0008\n",
      "Epoch [5/5], Step [200/842], Loss: 0.0420\n",
      "Epoch [5/5], Step [201/842], Loss: 0.0029\n",
      "Epoch [5/5], Step [202/842], Loss: 0.0292\n",
      "Epoch [5/5], Step [203/842], Loss: 0.0710\n",
      "Epoch [5/5], Step [204/842], Loss: 0.0279\n",
      "Epoch [5/5], Step [205/842], Loss: 0.0276\n",
      "Epoch [5/5], Step [206/842], Loss: 0.0012\n",
      "Epoch [5/5], Step [207/842], Loss: 0.0111\n",
      "Epoch [5/5], Step [208/842], Loss: 0.0028\n",
      "Epoch [5/5], Step [209/842], Loss: 0.0370\n",
      "Epoch [5/5], Step [210/842], Loss: 0.0070\n",
      "Epoch [5/5], Step [211/842], Loss: 0.0011\n",
      "Epoch [5/5], Step [212/842], Loss: 0.0016\n",
      "Epoch [5/5], Step [213/842], Loss: 0.0026\n",
      "Epoch [5/5], Step [214/842], Loss: 0.0132\n",
      "Epoch [5/5], Step [215/842], Loss: 0.0415\n",
      "Epoch [5/5], Step [216/842], Loss: 0.0068\n",
      "Epoch [5/5], Step [217/842], Loss: 0.0013\n",
      "Epoch [5/5], Step [218/842], Loss: 0.0343\n",
      "Epoch [5/5], Step [219/842], Loss: 0.0128\n",
      "Epoch [5/5], Step [220/842], Loss: 0.0099\n",
      "Epoch [5/5], Step [221/842], Loss: 0.0114\n",
      "Epoch [5/5], Step [222/842], Loss: 0.0095\n",
      "Epoch [5/5], Step [223/842], Loss: 0.1121\n",
      "Epoch [5/5], Step [224/842], Loss: 0.0056\n",
      "Epoch [5/5], Step [225/842], Loss: 0.0071\n",
      "Epoch [5/5], Step [226/842], Loss: 0.1608\n",
      "Epoch [5/5], Step [227/842], Loss: 0.0837\n",
      "Epoch [5/5], Step [228/842], Loss: 0.0027\n",
      "Epoch [5/5], Step [229/842], Loss: 0.1694\n",
      "Epoch [5/5], Step [230/842], Loss: 0.0013\n",
      "Epoch [5/5], Step [231/842], Loss: 0.0047\n",
      "Epoch [5/5], Step [232/842], Loss: 0.0101\n",
      "Epoch [5/5], Step [233/842], Loss: 0.0070\n",
      "Epoch [5/5], Step [234/842], Loss: 0.0045\n",
      "Epoch [5/5], Step [235/842], Loss: 0.0150\n",
      "Epoch [5/5], Step [236/842], Loss: 0.0580\n",
      "Epoch [5/5], Step [237/842], Loss: 0.0046\n",
      "Epoch [5/5], Step [238/842], Loss: 0.0619\n",
      "Epoch [5/5], Step [239/842], Loss: 0.0028\n",
      "Epoch [5/5], Step [240/842], Loss: 0.1576\n",
      "Epoch [5/5], Step [241/842], Loss: 0.0212\n",
      "Epoch [5/5], Step [242/842], Loss: 0.0277\n",
      "Epoch [5/5], Step [243/842], Loss: 0.0081\n",
      "Epoch [5/5], Step [244/842], Loss: 0.0100\n",
      "Epoch [5/5], Step [245/842], Loss: 0.0020\n",
      "Epoch [5/5], Step [246/842], Loss: 0.0051\n",
      "Epoch [5/5], Step [247/842], Loss: 0.0234\n",
      "Epoch [5/5], Step [248/842], Loss: 0.1419\n",
      "Epoch [5/5], Step [249/842], Loss: 0.0472\n",
      "Epoch [5/5], Step [250/842], Loss: 0.0095\n",
      "Epoch [5/5], Step [251/842], Loss: 0.0010\n",
      "Epoch [5/5], Step [252/842], Loss: 0.0018\n",
      "Epoch [5/5], Step [253/842], Loss: 0.0057\n",
      "Epoch [5/5], Step [254/842], Loss: 0.0030\n",
      "Epoch [5/5], Step [255/842], Loss: 0.0615\n",
      "Epoch [5/5], Step [256/842], Loss: 0.0272\n",
      "Epoch [5/5], Step [257/842], Loss: 0.0625\n",
      "Epoch [5/5], Step [258/842], Loss: 0.1683\n",
      "Epoch [5/5], Step [259/842], Loss: 0.0040\n",
      "Epoch [5/5], Step [260/842], Loss: 0.0506\n",
      "Epoch [5/5], Step [261/842], Loss: 0.1745\n",
      "Epoch [5/5], Step [262/842], Loss: 0.0530\n",
      "Epoch [5/5], Step [263/842], Loss: 0.0536\n",
      "Epoch [5/5], Step [264/842], Loss: 0.0022\n",
      "Epoch [5/5], Step [265/842], Loss: 0.0119\n",
      "Epoch [5/5], Step [266/842], Loss: 0.1009\n",
      "Epoch [5/5], Step [267/842], Loss: 0.2269\n",
      "Epoch [5/5], Step [268/842], Loss: 0.0603\n",
      "Epoch [5/5], Step [269/842], Loss: 0.0022\n",
      "Epoch [5/5], Step [270/842], Loss: 0.0199\n",
      "Epoch [5/5], Step [271/842], Loss: 0.0064\n",
      "Epoch [5/5], Step [272/842], Loss: 0.0058\n",
      "Epoch [5/5], Step [273/842], Loss: 0.0031\n",
      "Epoch [5/5], Step [274/842], Loss: 0.0103\n",
      "Epoch [5/5], Step [275/842], Loss: 0.0098\n",
      "Epoch [5/5], Step [276/842], Loss: 0.0728\n",
      "Epoch [5/5], Step [277/842], Loss: 0.0839\n",
      "Epoch [5/5], Step [278/842], Loss: 0.0825\n",
      "Epoch [5/5], Step [279/842], Loss: 0.0078\n",
      "Epoch [5/5], Step [280/842], Loss: 0.0023\n",
      "Epoch [5/5], Step [281/842], Loss: 0.0073\n",
      "Epoch [5/5], Step [282/842], Loss: 0.0222\n",
      "Epoch [5/5], Step [283/842], Loss: 0.0024\n",
      "Epoch [5/5], Step [284/842], Loss: 0.0474\n",
      "Epoch [5/5], Step [285/842], Loss: 0.0245\n",
      "Epoch [5/5], Step [286/842], Loss: 0.0072\n",
      "Epoch [5/5], Step [287/842], Loss: 0.0064\n",
      "Epoch [5/5], Step [288/842], Loss: 0.0148\n",
      "Epoch [5/5], Step [289/842], Loss: 0.0038\n",
      "Epoch [5/5], Step [290/842], Loss: 0.1713\n",
      "Epoch [5/5], Step [291/842], Loss: 0.0391\n",
      "Epoch [5/5], Step [292/842], Loss: 0.0015\n",
      "Epoch [5/5], Step [293/842], Loss: 0.0369\n",
      "Epoch [5/5], Step [294/842], Loss: 0.2583\n",
      "Epoch [5/5], Step [295/842], Loss: 0.0092\n",
      "Epoch [5/5], Step [296/842], Loss: 0.2863\n",
      "Epoch [5/5], Step [297/842], Loss: 0.0609\n",
      "Epoch [5/5], Step [298/842], Loss: 0.0068\n",
      "Epoch [5/5], Step [299/842], Loss: 0.0078\n",
      "Epoch [5/5], Step [300/842], Loss: 0.0575\n",
      "Epoch [5/5], Step [301/842], Loss: 0.1134\n",
      "Epoch [5/5], Step [302/842], Loss: 0.0025\n",
      "Epoch [5/5], Step [303/842], Loss: 0.0434\n",
      "Epoch [5/5], Step [304/842], Loss: 0.0085\n",
      "Epoch [5/5], Step [305/842], Loss: 0.0013\n",
      "Epoch [5/5], Step [306/842], Loss: 0.0980\n",
      "Epoch [5/5], Step [307/842], Loss: 0.1026\n",
      "Epoch [5/5], Step [308/842], Loss: 0.0118\n",
      "Epoch [5/5], Step [309/842], Loss: 0.0168\n",
      "Epoch [5/5], Step [310/842], Loss: 0.0030\n",
      "Epoch [5/5], Step [311/842], Loss: 0.0547\n",
      "Epoch [5/5], Step [312/842], Loss: 0.0018\n",
      "Epoch [5/5], Step [313/842], Loss: 0.0235\n",
      "Epoch [5/5], Step [314/842], Loss: 0.0127\n",
      "Epoch [5/5], Step [315/842], Loss: 0.0081\n",
      "Epoch [5/5], Step [316/842], Loss: 0.0063\n",
      "Epoch [5/5], Step [317/842], Loss: 0.0051\n",
      "Epoch [5/5], Step [318/842], Loss: 0.0069\n",
      "Epoch [5/5], Step [319/842], Loss: 0.0262\n",
      "Epoch [5/5], Step [320/842], Loss: 0.0044\n",
      "Epoch [5/5], Step [321/842], Loss: 0.0102\n",
      "Epoch [5/5], Step [322/842], Loss: 0.0430\n",
      "Epoch [5/5], Step [323/842], Loss: 0.1472\n",
      "Epoch [5/5], Step [324/842], Loss: 0.0069\n",
      "Epoch [5/5], Step [325/842], Loss: 0.0013\n",
      "Epoch [5/5], Step [326/842], Loss: 0.0232\n",
      "Epoch [5/5], Step [327/842], Loss: 0.0763\n",
      "Epoch [5/5], Step [328/842], Loss: 0.1164\n",
      "Epoch [5/5], Step [329/842], Loss: 0.0577\n",
      "Epoch [5/5], Step [330/842], Loss: 0.0023\n",
      "Epoch [5/5], Step [331/842], Loss: 0.0089\n",
      "Epoch [5/5], Step [332/842], Loss: 0.1730\n",
      "Epoch [5/5], Step [333/842], Loss: 0.0044\n",
      "Epoch [5/5], Step [334/842], Loss: 0.0060\n",
      "Epoch [5/5], Step [335/842], Loss: 0.1361\n",
      "Epoch [5/5], Step [336/842], Loss: 0.1101\n",
      "Epoch [5/5], Step [337/842], Loss: 0.0009\n",
      "Epoch [5/5], Step [338/842], Loss: 0.1832\n",
      "Epoch [5/5], Step [339/842], Loss: 0.0379\n",
      "Epoch [5/5], Step [340/842], Loss: 0.0005\n",
      "Epoch [5/5], Step [341/842], Loss: 0.2461\n",
      "Epoch [5/5], Step [342/842], Loss: 0.0019\n",
      "Epoch [5/5], Step [343/842], Loss: 0.0649\n",
      "Epoch [5/5], Step [344/842], Loss: 0.0084\n",
      "Epoch [5/5], Step [345/842], Loss: 0.0023\n",
      "Epoch [5/5], Step [346/842], Loss: 0.2660\n",
      "Epoch [5/5], Step [347/842], Loss: 0.0028\n",
      "Epoch [5/5], Step [348/842], Loss: 0.0068\n",
      "Epoch [5/5], Step [349/842], Loss: 0.0151\n",
      "Epoch [5/5], Step [350/842], Loss: 0.0110\n",
      "Epoch [5/5], Step [351/842], Loss: 0.2032\n",
      "Epoch [5/5], Step [352/842], Loss: 0.0049\n",
      "Epoch [5/5], Step [353/842], Loss: 0.0028\n",
      "Epoch [5/5], Step [354/842], Loss: 0.0362\n",
      "Epoch [5/5], Step [355/842], Loss: 0.0366\n",
      "Epoch [5/5], Step [356/842], Loss: 0.0194\n",
      "Epoch [5/5], Step [357/842], Loss: 0.0052\n",
      "Epoch [5/5], Step [358/842], Loss: 0.0033\n",
      "Epoch [5/5], Step [359/842], Loss: 0.0385\n",
      "Epoch [5/5], Step [360/842], Loss: 0.0672\n",
      "Epoch [5/5], Step [361/842], Loss: 0.0705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [362/842], Loss: 0.1476\n",
      "Epoch [5/5], Step [363/842], Loss: 0.1997\n",
      "Epoch [5/5], Step [364/842], Loss: 0.0364\n",
      "Epoch [5/5], Step [365/842], Loss: 0.1251\n",
      "Epoch [5/5], Step [366/842], Loss: 0.0862\n",
      "Epoch [5/5], Step [367/842], Loss: 0.3018\n",
      "Epoch [5/5], Step [368/842], Loss: 0.0165\n",
      "Epoch [5/5], Step [369/842], Loss: 0.0128\n",
      "Epoch [5/5], Step [370/842], Loss: 0.0011\n",
      "Epoch [5/5], Step [371/842], Loss: 0.0081\n",
      "Epoch [5/5], Step [372/842], Loss: 0.1799\n",
      "Epoch [5/5], Step [373/842], Loss: 0.0441\n",
      "Epoch [5/5], Step [374/842], Loss: 0.0317\n",
      "Epoch [5/5], Step [375/842], Loss: 0.0015\n",
      "Epoch [5/5], Step [376/842], Loss: 0.1339\n",
      "Epoch [5/5], Step [377/842], Loss: 0.0031\n",
      "Epoch [5/5], Step [378/842], Loss: 0.0396\n",
      "Epoch [5/5], Step [379/842], Loss: 0.0111\n",
      "Epoch [5/5], Step [380/842], Loss: 0.0660\n",
      "Epoch [5/5], Step [381/842], Loss: 0.0796\n",
      "Epoch [5/5], Step [382/842], Loss: 0.0052\n",
      "Epoch [5/5], Step [383/842], Loss: 0.0677\n",
      "Epoch [5/5], Step [384/842], Loss: 0.1792\n",
      "Epoch [5/5], Step [385/842], Loss: 0.0116\n",
      "Epoch [5/5], Step [386/842], Loss: 0.0259\n",
      "Epoch [5/5], Step [387/842], Loss: 0.1049\n",
      "Epoch [5/5], Step [388/842], Loss: 0.0297\n",
      "Epoch [5/5], Step [389/842], Loss: 0.0124\n",
      "Epoch [5/5], Step [390/842], Loss: 0.1301\n",
      "Epoch [5/5], Step [391/842], Loss: 0.0216\n",
      "Epoch [5/5], Step [392/842], Loss: 0.0596\n",
      "Epoch [5/5], Step [393/842], Loss: 0.0857\n",
      "Epoch [5/5], Step [394/842], Loss: 0.0109\n",
      "Epoch [5/5], Step [395/842], Loss: 0.1319\n",
      "Epoch [5/5], Step [396/842], Loss: 0.0013\n",
      "Epoch [5/5], Step [397/842], Loss: 0.0993\n",
      "Epoch [5/5], Step [398/842], Loss: 0.0028\n",
      "Epoch [5/5], Step [399/842], Loss: 0.0053\n",
      "Epoch [5/5], Step [400/842], Loss: 0.0139\n",
      "Epoch [5/5], Step [401/842], Loss: 0.0469\n",
      "Epoch [5/5], Step [402/842], Loss: 0.1074\n",
      "Epoch [5/5], Step [403/842], Loss: 0.0214\n",
      "Epoch [5/5], Step [404/842], Loss: 0.0321\n",
      "Epoch [5/5], Step [405/842], Loss: 0.0056\n",
      "Epoch [5/5], Step [406/842], Loss: 0.0088\n",
      "Epoch [5/5], Step [407/842], Loss: 0.0667\n",
      "Epoch [5/5], Step [408/842], Loss: 0.1975\n",
      "Epoch [5/5], Step [409/842], Loss: 0.0011\n",
      "Epoch [5/5], Step [410/842], Loss: 0.0632\n",
      "Epoch [5/5], Step [411/842], Loss: 0.0052\n",
      "Epoch [5/5], Step [412/842], Loss: 0.0013\n",
      "Epoch [5/5], Step [413/842], Loss: 0.0041\n",
      "Epoch [5/5], Step [414/842], Loss: 0.0314\n",
      "Epoch [5/5], Step [415/842], Loss: 0.0251\n",
      "Epoch [5/5], Step [416/842], Loss: 0.0248\n",
      "Epoch [5/5], Step [417/842], Loss: 0.0353\n",
      "Epoch [5/5], Step [418/842], Loss: 0.0035\n",
      "Epoch [5/5], Step [419/842], Loss: 0.0068\n",
      "Epoch [5/5], Step [420/842], Loss: 0.0008\n",
      "Epoch [5/5], Step [421/842], Loss: 0.0017\n",
      "Epoch [5/5], Step [422/842], Loss: 0.0198\n",
      "Epoch [5/5], Step [423/842], Loss: 0.0919\n",
      "Epoch [5/5], Step [424/842], Loss: 0.0039\n",
      "Epoch [5/5], Step [425/842], Loss: 0.0320\n",
      "Epoch [5/5], Step [426/842], Loss: 0.0365\n",
      "Epoch [5/5], Step [427/842], Loss: 0.0012\n",
      "Epoch [5/5], Step [428/842], Loss: 0.1212\n",
      "Epoch [5/5], Step [429/842], Loss: 0.0284\n",
      "Epoch [5/5], Step [430/842], Loss: 0.0120\n",
      "Epoch [5/5], Step [431/842], Loss: 0.0110\n",
      "Epoch [5/5], Step [432/842], Loss: 0.0059\n",
      "Epoch [5/5], Step [433/842], Loss: 0.1203\n",
      "Epoch [5/5], Step [434/842], Loss: 0.0094\n",
      "Epoch [5/5], Step [435/842], Loss: 0.0090\n",
      "Epoch [5/5], Step [436/842], Loss: 0.0079\n",
      "Epoch [5/5], Step [437/842], Loss: 0.0061\n",
      "Epoch [5/5], Step [438/842], Loss: 0.0235\n",
      "Epoch [5/5], Step [439/842], Loss: 0.2039\n",
      "Epoch [5/5], Step [440/842], Loss: 0.0251\n",
      "Epoch [5/5], Step [441/842], Loss: 0.1366\n",
      "Epoch [5/5], Step [442/842], Loss: 0.0852\n",
      "Epoch [5/5], Step [443/842], Loss: 0.0083\n",
      "Epoch [5/5], Step [444/842], Loss: 0.2128\n",
      "Epoch [5/5], Step [445/842], Loss: 0.0028\n",
      "Epoch [5/5], Step [446/842], Loss: 0.0217\n",
      "Epoch [5/5], Step [447/842], Loss: 0.0034\n",
      "Epoch [5/5], Step [448/842], Loss: 0.0220\n",
      "Epoch [5/5], Step [449/842], Loss: 0.0027\n",
      "Epoch [5/5], Step [450/842], Loss: 0.0194\n",
      "Epoch [5/5], Step [451/842], Loss: 0.0236\n",
      "Epoch [5/5], Step [452/842], Loss: 0.0822\n",
      "Epoch [5/5], Step [453/842], Loss: 0.1057\n",
      "Epoch [5/5], Step [454/842], Loss: 0.1404\n",
      "Epoch [5/5], Step [455/842], Loss: 0.0527\n",
      "Epoch [5/5], Step [456/842], Loss: 0.0016\n",
      "Epoch [5/5], Step [457/842], Loss: 0.0266\n",
      "Epoch [5/5], Step [458/842], Loss: 0.0092\n",
      "Epoch [5/5], Step [459/842], Loss: 0.0381\n",
      "Epoch [5/5], Step [460/842], Loss: 0.0114\n",
      "Epoch [5/5], Step [461/842], Loss: 0.0206\n",
      "Epoch [5/5], Step [462/842], Loss: 0.0392\n",
      "Epoch [5/5], Step [463/842], Loss: 0.1913\n",
      "Epoch [5/5], Step [464/842], Loss: 0.0200\n",
      "Epoch [5/5], Step [465/842], Loss: 0.1214\n",
      "Epoch [5/5], Step [466/842], Loss: 0.0087\n",
      "Epoch [5/5], Step [467/842], Loss: 0.0560\n",
      "Epoch [5/5], Step [468/842], Loss: 0.1295\n",
      "Epoch [5/5], Step [469/842], Loss: 0.0290\n",
      "Epoch [5/5], Step [470/842], Loss: 0.1188\n",
      "Epoch [5/5], Step [471/842], Loss: 0.1670\n",
      "Epoch [5/5], Step [472/842], Loss: 0.0003\n",
      "Epoch [5/5], Step [473/842], Loss: 0.1153\n",
      "Epoch [5/5], Step [474/842], Loss: 0.0071\n",
      "Epoch [5/5], Step [475/842], Loss: 0.0346\n",
      "Epoch [5/5], Step [476/842], Loss: 0.0067\n",
      "Epoch [5/5], Step [477/842], Loss: 0.1014\n",
      "Epoch [5/5], Step [478/842], Loss: 0.0335\n",
      "Epoch [5/5], Step [479/842], Loss: 0.0063\n",
      "Epoch [5/5], Step [480/842], Loss: 0.0050\n",
      "Epoch [5/5], Step [481/842], Loss: 0.0428\n",
      "Epoch [5/5], Step [482/842], Loss: 0.0093\n",
      "Epoch [5/5], Step [483/842], Loss: 0.3960\n",
      "Epoch [5/5], Step [484/842], Loss: 0.2626\n",
      "Epoch [5/5], Step [485/842], Loss: 0.0131\n",
      "Epoch [5/5], Step [486/842], Loss: 0.0310\n",
      "Epoch [5/5], Step [487/842], Loss: 0.1520\n",
      "Epoch [5/5], Step [488/842], Loss: 0.0060\n",
      "Epoch [5/5], Step [489/842], Loss: 0.0175\n",
      "Epoch [5/5], Step [490/842], Loss: 0.0909\n",
      "Epoch [5/5], Step [491/842], Loss: 0.0025\n",
      "Epoch [5/5], Step [492/842], Loss: 0.0340\n",
      "Epoch [5/5], Step [493/842], Loss: 0.0034\n",
      "Epoch [5/5], Step [494/842], Loss: 0.1469\n",
      "Epoch [5/5], Step [495/842], Loss: 0.0096\n",
      "Epoch [5/5], Step [496/842], Loss: 0.0172\n",
      "Epoch [5/5], Step [497/842], Loss: 0.0126\n",
      "Epoch [5/5], Step [498/842], Loss: 0.0096\n",
      "Epoch [5/5], Step [499/842], Loss: 0.0451\n",
      "Epoch [5/5], Step [500/842], Loss: 0.2489\n",
      "Epoch [5/5], Step [501/842], Loss: 0.0286\n",
      "Epoch [5/5], Step [502/842], Loss: 0.0723\n",
      "Epoch [5/5], Step [503/842], Loss: 0.0962\n",
      "Epoch [5/5], Step [504/842], Loss: 0.0587\n",
      "Epoch [5/5], Step [505/842], Loss: 0.0524\n",
      "Epoch [5/5], Step [506/842], Loss: 0.0810\n",
      "Epoch [5/5], Step [507/842], Loss: 0.1144\n",
      "Epoch [5/5], Step [508/842], Loss: 0.0701\n",
      "Epoch [5/5], Step [509/842], Loss: 0.0182\n",
      "Epoch [5/5], Step [510/842], Loss: 0.0072\n",
      "Epoch [5/5], Step [511/842], Loss: 0.0021\n",
      "Epoch [5/5], Step [512/842], Loss: 0.1188\n",
      "Epoch [5/5], Step [513/842], Loss: 0.0401\n",
      "Epoch [5/5], Step [514/842], Loss: 0.1250\n",
      "Epoch [5/5], Step [515/842], Loss: 0.0824\n",
      "Epoch [5/5], Step [516/842], Loss: 0.0732\n",
      "Epoch [5/5], Step [517/842], Loss: 0.0108\n",
      "Epoch [5/5], Step [518/842], Loss: 0.0177\n",
      "Epoch [5/5], Step [519/842], Loss: 0.0030\n",
      "Epoch [5/5], Step [520/842], Loss: 0.0021\n",
      "Epoch [5/5], Step [521/842], Loss: 0.1540\n",
      "Epoch [5/5], Step [522/842], Loss: 0.0007\n",
      "Epoch [5/5], Step [523/842], Loss: 0.1572\n",
      "Epoch [5/5], Step [524/842], Loss: 0.0087\n",
      "Epoch [5/5], Step [525/842], Loss: 0.0259\n",
      "Epoch [5/5], Step [526/842], Loss: 0.0342\n",
      "Epoch [5/5], Step [527/842], Loss: 0.0184\n",
      "Epoch [5/5], Step [528/842], Loss: 0.0866\n",
      "Epoch [5/5], Step [529/842], Loss: 0.1783\n",
      "Epoch [5/5], Step [530/842], Loss: 0.1362\n",
      "Epoch [5/5], Step [531/842], Loss: 0.0017\n",
      "Epoch [5/5], Step [532/842], Loss: 0.0098\n",
      "Epoch [5/5], Step [533/842], Loss: 0.0881\n",
      "Epoch [5/5], Step [534/842], Loss: 0.0146\n",
      "Epoch [5/5], Step [535/842], Loss: 0.0195\n",
      "Epoch [5/5], Step [536/842], Loss: 0.1088\n",
      "Epoch [5/5], Step [537/842], Loss: 0.0088\n",
      "Epoch [5/5], Step [538/842], Loss: 0.0733\n",
      "Epoch [5/5], Step [539/842], Loss: 0.0296\n",
      "Epoch [5/5], Step [540/842], Loss: 0.1468\n",
      "Epoch [5/5], Step [541/842], Loss: 0.0359\n",
      "Epoch [5/5], Step [542/842], Loss: 0.2331\n",
      "Epoch [5/5], Step [543/842], Loss: 0.1259\n",
      "Epoch [5/5], Step [544/842], Loss: 0.1667\n",
      "Epoch [5/5], Step [545/842], Loss: 0.1040\n",
      "Epoch [5/5], Step [546/842], Loss: 0.0574\n",
      "Epoch [5/5], Step [547/842], Loss: 0.1692\n",
      "Epoch [5/5], Step [548/842], Loss: 0.0092\n",
      "Epoch [5/5], Step [549/842], Loss: 0.0915\n",
      "Epoch [5/5], Step [550/842], Loss: 0.0146\n",
      "Epoch [5/5], Step [551/842], Loss: 0.1869\n",
      "Epoch [5/5], Step [552/842], Loss: 0.0971\n",
      "Epoch [5/5], Step [553/842], Loss: 0.0561\n",
      "Epoch [5/5], Step [554/842], Loss: 0.0181\n",
      "Epoch [5/5], Step [555/842], Loss: 0.0015\n",
      "Epoch [5/5], Step [556/842], Loss: 0.1357\n",
      "Epoch [5/5], Step [557/842], Loss: 0.0171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [558/842], Loss: 0.0073\n",
      "Epoch [5/5], Step [559/842], Loss: 0.2061\n",
      "Epoch [5/5], Step [560/842], Loss: 0.0055\n",
      "Epoch [5/5], Step [561/842], Loss: 0.0272\n",
      "Epoch [5/5], Step [562/842], Loss: 0.0084\n",
      "Epoch [5/5], Step [563/842], Loss: 0.2838\n",
      "Epoch [5/5], Step [564/842], Loss: 0.0209\n",
      "Epoch [5/5], Step [565/842], Loss: 0.1359\n",
      "Epoch [5/5], Step [566/842], Loss: 0.0111\n",
      "Epoch [5/5], Step [567/842], Loss: 0.0031\n",
      "Epoch [5/5], Step [568/842], Loss: 0.2043\n",
      "Epoch [5/5], Step [569/842], Loss: 0.0044\n",
      "Epoch [5/5], Step [570/842], Loss: 0.0276\n",
      "Epoch [5/5], Step [571/842], Loss: 0.0063\n",
      "Epoch [5/5], Step [572/842], Loss: 0.0229\n",
      "Epoch [5/5], Step [573/842], Loss: 0.1443\n",
      "Epoch [5/5], Step [574/842], Loss: 0.0081\n",
      "Epoch [5/5], Step [575/842], Loss: 0.0698\n",
      "Epoch [5/5], Step [576/842], Loss: 0.0046\n",
      "Epoch [5/5], Step [577/842], Loss: 0.0031\n",
      "Epoch [5/5], Step [578/842], Loss: 0.0128\n",
      "Epoch [5/5], Step [579/842], Loss: 0.0377\n",
      "Epoch [5/5], Step [580/842], Loss: 0.0687\n",
      "Epoch [5/5], Step [581/842], Loss: 0.0027\n",
      "Epoch [5/5], Step [582/842], Loss: 0.0626\n",
      "Epoch [5/5], Step [583/842], Loss: 0.0224\n",
      "Epoch [5/5], Step [584/842], Loss: 0.0400\n",
      "Epoch [5/5], Step [585/842], Loss: 0.0214\n",
      "Epoch [5/5], Step [586/842], Loss: 0.0010\n",
      "Epoch [5/5], Step [587/842], Loss: 0.1173\n",
      "Epoch [5/5], Step [588/842], Loss: 0.1483\n",
      "Epoch [5/5], Step [589/842], Loss: 0.0441\n",
      "Epoch [5/5], Step [590/842], Loss: 0.0065\n",
      "Epoch [5/5], Step [591/842], Loss: 0.0087\n",
      "Epoch [5/5], Step [592/842], Loss: 0.1592\n",
      "Epoch [5/5], Step [593/842], Loss: 0.0198\n",
      "Epoch [5/5], Step [594/842], Loss: 0.1097\n",
      "Epoch [5/5], Step [595/842], Loss: 0.1705\n",
      "Epoch [5/5], Step [596/842], Loss: 0.0008\n",
      "Epoch [5/5], Step [597/842], Loss: 0.0005\n",
      "Epoch [5/5], Step [598/842], Loss: 0.0204\n",
      "Epoch [5/5], Step [599/842], Loss: 0.0197\n",
      "Epoch [5/5], Step [600/842], Loss: 0.0052\n",
      "Epoch [5/5], Step [601/842], Loss: 0.0011\n",
      "Epoch [5/5], Step [602/842], Loss: 0.0049\n",
      "Epoch [5/5], Step [603/842], Loss: 0.1981\n",
      "Epoch [5/5], Step [604/842], Loss: 0.0713\n",
      "Epoch [5/5], Step [605/842], Loss: 0.0992\n",
      "Epoch [5/5], Step [606/842], Loss: 0.0010\n",
      "Epoch [5/5], Step [607/842], Loss: 0.0529\n",
      "Epoch [5/5], Step [608/842], Loss: 0.0199\n",
      "Epoch [5/5], Step [609/842], Loss: 0.0119\n",
      "Epoch [5/5], Step [610/842], Loss: 0.1580\n",
      "Epoch [5/5], Step [611/842], Loss: 0.0461\n",
      "Epoch [5/5], Step [612/842], Loss: 0.0028\n",
      "Epoch [5/5], Step [613/842], Loss: 0.1485\n",
      "Epoch [5/5], Step [614/842], Loss: 0.0501\n",
      "Epoch [5/5], Step [615/842], Loss: 0.0663\n",
      "Epoch [5/5], Step [616/842], Loss: 0.0495\n",
      "Epoch [5/5], Step [617/842], Loss: 0.0077\n",
      "Epoch [5/5], Step [618/842], Loss: 0.1138\n",
      "Epoch [5/5], Step [619/842], Loss: 0.0015\n",
      "Epoch [5/5], Step [620/842], Loss: 0.1858\n",
      "Epoch [5/5], Step [621/842], Loss: 0.0064\n",
      "Epoch [5/5], Step [622/842], Loss: 0.0928\n",
      "Epoch [5/5], Step [623/842], Loss: 0.0170\n",
      "Epoch [5/5], Step [624/842], Loss: 0.0105\n",
      "Epoch [5/5], Step [625/842], Loss: 0.1576\n",
      "Epoch [5/5], Step [626/842], Loss: 0.0474\n",
      "Epoch [5/5], Step [627/842], Loss: 0.0385\n",
      "Epoch [5/5], Step [628/842], Loss: 0.1427\n",
      "Epoch [5/5], Step [629/842], Loss: 0.0154\n",
      "Epoch [5/5], Step [630/842], Loss: 0.0397\n",
      "Epoch [5/5], Step [631/842], Loss: 0.0006\n",
      "Epoch [5/5], Step [632/842], Loss: 0.0105\n",
      "Epoch [5/5], Step [633/842], Loss: 0.0095\n",
      "Epoch [5/5], Step [634/842], Loss: 0.1367\n",
      "Epoch [5/5], Step [635/842], Loss: 0.0110\n",
      "Epoch [5/5], Step [636/842], Loss: 0.0167\n",
      "Epoch [5/5], Step [637/842], Loss: 0.0171\n",
      "Epoch [5/5], Step [638/842], Loss: 0.2910\n",
      "Epoch [5/5], Step [639/842], Loss: 0.0708\n",
      "Epoch [5/5], Step [640/842], Loss: 0.0073\n",
      "Epoch [5/5], Step [641/842], Loss: 0.1773\n",
      "Epoch [5/5], Step [642/842], Loss: 0.1406\n",
      "Epoch [5/5], Step [643/842], Loss: 0.1303\n",
      "Epoch [5/5], Step [644/842], Loss: 0.0117\n",
      "Epoch [5/5], Step [645/842], Loss: 0.0076\n",
      "Epoch [5/5], Step [646/842], Loss: 0.0006\n",
      "Epoch [5/5], Step [647/842], Loss: 0.0400\n",
      "Epoch [5/5], Step [648/842], Loss: 0.0039\n",
      "Epoch [5/5], Step [649/842], Loss: 0.0686\n",
      "Epoch [5/5], Step [650/842], Loss: 0.0098\n",
      "Epoch [5/5], Step [651/842], Loss: 0.0569\n",
      "Epoch [5/5], Step [652/842], Loss: 0.0289\n",
      "Epoch [5/5], Step [653/842], Loss: 0.0025\n",
      "Epoch [5/5], Step [654/842], Loss: 0.1946\n",
      "Epoch [5/5], Step [655/842], Loss: 0.0364\n",
      "Epoch [5/5], Step [656/842], Loss: 0.0278\n",
      "Epoch [5/5], Step [657/842], Loss: 0.0108\n",
      "Epoch [5/5], Step [658/842], Loss: 0.0035\n",
      "Epoch [5/5], Step [659/842], Loss: 0.0881\n",
      "Epoch [5/5], Step [660/842], Loss: 0.0360\n",
      "Epoch [5/5], Step [661/842], Loss: 0.2269\n",
      "Epoch [5/5], Step [662/842], Loss: 0.0051\n",
      "Epoch [5/5], Step [663/842], Loss: 0.1632\n",
      "Epoch [5/5], Step [664/842], Loss: 0.0483\n",
      "Epoch [5/5], Step [665/842], Loss: 0.1766\n",
      "Epoch [5/5], Step [666/842], Loss: 0.0049\n",
      "Epoch [5/5], Step [667/842], Loss: 0.0389\n",
      "Epoch [5/5], Step [668/842], Loss: 0.2379\n",
      "Epoch [5/5], Step [669/842], Loss: 0.0109\n",
      "Epoch [5/5], Step [670/842], Loss: 0.0906\n",
      "Epoch [5/5], Step [671/842], Loss: 0.0053\n",
      "Epoch [5/5], Step [672/842], Loss: 0.0023\n",
      "Epoch [5/5], Step [673/842], Loss: 0.0281\n",
      "Epoch [5/5], Step [674/842], Loss: 0.1249\n",
      "Epoch [5/5], Step [675/842], Loss: 0.0316\n",
      "Epoch [5/5], Step [676/842], Loss: 0.0919\n",
      "Epoch [5/5], Step [677/842], Loss: 0.0077\n",
      "Epoch [5/5], Step [678/842], Loss: 0.0033\n",
      "Epoch [5/5], Step [679/842], Loss: 0.0633\n",
      "Epoch [5/5], Step [680/842], Loss: 0.0064\n",
      "Epoch [5/5], Step [681/842], Loss: 0.1906\n",
      "Epoch [5/5], Step [682/842], Loss: 0.0844\n",
      "Epoch [5/5], Step [683/842], Loss: 0.2277\n",
      "Epoch [5/5], Step [684/842], Loss: 0.0251\n",
      "Epoch [5/5], Step [685/842], Loss: 0.0578\n",
      "Epoch [5/5], Step [686/842], Loss: 0.0398\n",
      "Epoch [5/5], Step [687/842], Loss: 0.2930\n",
      "Epoch [5/5], Step [688/842], Loss: 0.2046\n",
      "Epoch [5/5], Step [689/842], Loss: 0.0057\n",
      "Epoch [5/5], Step [690/842], Loss: 0.0043\n",
      "Epoch [5/5], Step [691/842], Loss: 0.0018\n",
      "Epoch [5/5], Step [692/842], Loss: 0.3373\n",
      "Epoch [5/5], Step [693/842], Loss: 0.0008\n",
      "Epoch [5/5], Step [694/842], Loss: 0.1679\n",
      "Epoch [5/5], Step [695/842], Loss: 0.0040\n",
      "Epoch [5/5], Step [696/842], Loss: 0.0729\n",
      "Epoch [5/5], Step [697/842], Loss: 0.2327\n",
      "Epoch [5/5], Step [698/842], Loss: 0.0791\n",
      "Epoch [5/5], Step [699/842], Loss: 0.0630\n",
      "Epoch [5/5], Step [700/842], Loss: 0.0044\n",
      "Epoch [5/5], Step [701/842], Loss: 0.2243\n",
      "Epoch [5/5], Step [702/842], Loss: 0.0044\n",
      "Epoch [5/5], Step [703/842], Loss: 0.0896\n",
      "Epoch [5/5], Step [704/842], Loss: 0.0135\n",
      "Epoch [5/5], Step [705/842], Loss: 0.0211\n",
      "Epoch [5/5], Step [706/842], Loss: 0.0812\n",
      "Epoch [5/5], Step [707/842], Loss: 0.1126\n",
      "Epoch [5/5], Step [708/842], Loss: 0.0405\n",
      "Epoch [5/5], Step [709/842], Loss: 0.0042\n",
      "Epoch [5/5], Step [710/842], Loss: 0.0040\n",
      "Epoch [5/5], Step [711/842], Loss: 0.0237\n",
      "Epoch [5/5], Step [712/842], Loss: 0.0723\n",
      "Epoch [5/5], Step [713/842], Loss: 0.0092\n",
      "Epoch [5/5], Step [714/842], Loss: 0.0229\n",
      "Epoch [5/5], Step [715/842], Loss: 0.0039\n",
      "Epoch [5/5], Step [716/842], Loss: 0.0403\n",
      "Epoch [5/5], Step [717/842], Loss: 0.1821\n",
      "Epoch [5/5], Step [718/842], Loss: 0.0257\n",
      "Epoch [5/5], Step [719/842], Loss: 0.1343\n",
      "Epoch [5/5], Step [720/842], Loss: 0.0324\n",
      "Epoch [5/5], Step [721/842], Loss: 0.1857\n",
      "Epoch [5/5], Step [722/842], Loss: 0.0019\n",
      "Epoch [5/5], Step [723/842], Loss: 0.0091\n",
      "Epoch [5/5], Step [724/842], Loss: 0.0075\n",
      "Epoch [5/5], Step [725/842], Loss: 0.0402\n",
      "Epoch [5/5], Step [726/842], Loss: 0.0086\n",
      "Epoch [5/5], Step [727/842], Loss: 0.1355\n",
      "Epoch [5/5], Step [728/842], Loss: 0.1404\n",
      "Epoch [5/5], Step [729/842], Loss: 0.0031\n",
      "Epoch [5/5], Step [730/842], Loss: 0.2181\n",
      "Epoch [5/5], Step [731/842], Loss: 0.1599\n",
      "Epoch [5/5], Step [732/842], Loss: 0.0050\n",
      "Epoch [5/5], Step [733/842], Loss: 0.0409\n",
      "Epoch [5/5], Step [734/842], Loss: 0.1480\n",
      "Epoch [5/5], Step [735/842], Loss: 0.0337\n",
      "Epoch [5/5], Step [736/842], Loss: 0.1445\n",
      "Epoch [5/5], Step [737/842], Loss: 0.0948\n",
      "Epoch [5/5], Step [738/842], Loss: 0.0245\n",
      "Epoch [5/5], Step [739/842], Loss: 0.0014\n",
      "Epoch [5/5], Step [740/842], Loss: 0.0030\n",
      "Epoch [5/5], Step [741/842], Loss: 0.1483\n",
      "Epoch [5/5], Step [742/842], Loss: 0.1409\n",
      "Epoch [5/5], Step [743/842], Loss: 0.0318\n",
      "Epoch [5/5], Step [744/842], Loss: 0.1810\n",
      "Epoch [5/5], Step [745/842], Loss: 0.0029\n",
      "Epoch [5/5], Step [746/842], Loss: 0.0118\n",
      "Epoch [5/5], Step [747/842], Loss: 0.0053\n",
      "Epoch [5/5], Step [748/842], Loss: 0.0321\n",
      "Epoch [5/5], Step [749/842], Loss: 0.0026\n",
      "Epoch [5/5], Step [750/842], Loss: 0.0880\n",
      "Epoch [5/5], Step [751/842], Loss: 0.0501\n",
      "Epoch [5/5], Step [752/842], Loss: 0.0261\n",
      "Epoch [5/5], Step [753/842], Loss: 0.0907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [754/842], Loss: 0.2954\n",
      "Epoch [5/5], Step [755/842], Loss: 0.0071\n",
      "Epoch [5/5], Step [756/842], Loss: 0.0622\n",
      "Epoch [5/5], Step [757/842], Loss: 0.0460\n",
      "Epoch [5/5], Step [758/842], Loss: 0.1655\n",
      "Epoch [5/5], Step [759/842], Loss: 0.0124\n",
      "Epoch [5/5], Step [760/842], Loss: 0.0093\n",
      "Epoch [5/5], Step [761/842], Loss: 0.0354\n",
      "Epoch [5/5], Step [762/842], Loss: 0.0766\n",
      "Epoch [5/5], Step [763/842], Loss: 0.1625\n",
      "Epoch [5/5], Step [764/842], Loss: 0.1639\n",
      "Epoch [5/5], Step [765/842], Loss: 0.0024\n",
      "Epoch [5/5], Step [766/842], Loss: 0.0014\n",
      "Epoch [5/5], Step [767/842], Loss: 0.1076\n",
      "Epoch [5/5], Step [768/842], Loss: 0.0291\n",
      "Epoch [5/5], Step [769/842], Loss: 0.0038\n",
      "Epoch [5/5], Step [770/842], Loss: 0.0731\n",
      "Epoch [5/5], Step [771/842], Loss: 0.2260\n",
      "Epoch [5/5], Step [772/842], Loss: 0.1016\n",
      "Epoch [5/5], Step [773/842], Loss: 0.0018\n",
      "Epoch [5/5], Step [774/842], Loss: 0.0699\n",
      "Epoch [5/5], Step [775/842], Loss: 0.0008\n",
      "Epoch [5/5], Step [776/842], Loss: 0.0026\n",
      "Epoch [5/5], Step [777/842], Loss: 0.0732\n",
      "Epoch [5/5], Step [778/842], Loss: 0.0088\n",
      "Epoch [5/5], Step [779/842], Loss: 0.0604\n",
      "Epoch [5/5], Step [780/842], Loss: 0.1229\n",
      "Epoch [5/5], Step [781/842], Loss: 0.0593\n",
      "Epoch [5/5], Step [782/842], Loss: 0.0778\n",
      "Epoch [5/5], Step [783/842], Loss: 0.1663\n",
      "Epoch [5/5], Step [784/842], Loss: 0.0408\n",
      "Epoch [5/5], Step [785/842], Loss: 0.1660\n",
      "Epoch [5/5], Step [786/842], Loss: 0.0049\n",
      "Epoch [5/5], Step [787/842], Loss: 0.1006\n",
      "Epoch [5/5], Step [788/842], Loss: 0.2158\n",
      "Epoch [5/5], Step [789/842], Loss: 0.0154\n",
      "Epoch [5/5], Step [790/842], Loss: 0.1665\n",
      "Epoch [5/5], Step [791/842], Loss: 0.0138\n",
      "Epoch [5/5], Step [792/842], Loss: 0.0296\n",
      "Epoch [5/5], Step [793/842], Loss: 0.0301\n",
      "Epoch [5/5], Step [794/842], Loss: 0.0834\n",
      "Epoch [5/5], Step [795/842], Loss: 0.0626\n",
      "Epoch [5/5], Step [796/842], Loss: 0.0148\n",
      "Epoch [5/5], Step [797/842], Loss: 0.2177\n",
      "Epoch [5/5], Step [798/842], Loss: 0.0544\n",
      "Epoch [5/5], Step [799/842], Loss: 0.1047\n",
      "Epoch [5/5], Step [800/842], Loss: 0.0015\n",
      "Epoch [5/5], Step [801/842], Loss: 0.0057\n",
      "Epoch [5/5], Step [802/842], Loss: 0.0044\n",
      "Epoch [5/5], Step [803/842], Loss: 0.1811\n",
      "Epoch [5/5], Step [804/842], Loss: 0.0312\n",
      "Epoch [5/5], Step [805/842], Loss: 0.0077\n",
      "Epoch [5/5], Step [806/842], Loss: 0.0279\n",
      "Epoch [5/5], Step [807/842], Loss: 0.0048\n",
      "Epoch [5/5], Step [808/842], Loss: 0.0054\n",
      "Epoch [5/5], Step [809/842], Loss: 0.0714\n",
      "Epoch [5/5], Step [810/842], Loss: 0.2947\n",
      "Epoch [5/5], Step [811/842], Loss: 0.0439\n",
      "Epoch [5/5], Step [812/842], Loss: 0.0138\n",
      "Epoch [5/5], Step [813/842], Loss: 0.0566\n",
      "Epoch [5/5], Step [814/842], Loss: 0.1363\n",
      "Epoch [5/5], Step [815/842], Loss: 0.0770\n",
      "Epoch [5/5], Step [816/842], Loss: 0.1994\n",
      "Epoch [5/5], Step [817/842], Loss: 0.2622\n",
      "Epoch [5/5], Step [818/842], Loss: 0.0699\n",
      "Epoch [5/5], Step [819/842], Loss: 0.0205\n",
      "Epoch [5/5], Step [820/842], Loss: 0.0982\n",
      "Epoch [5/5], Step [821/842], Loss: 0.0179\n",
      "Epoch [5/5], Step [822/842], Loss: 0.0417\n",
      "Epoch [5/5], Step [823/842], Loss: 0.0410\n",
      "Epoch [5/5], Step [824/842], Loss: 0.1114\n",
      "Epoch [5/5], Step [825/842], Loss: 0.1462\n",
      "Epoch [5/5], Step [826/842], Loss: 0.3938\n",
      "Epoch [5/5], Step [827/842], Loss: 0.2836\n",
      "Epoch [5/5], Step [828/842], Loss: 0.0300\n",
      "Epoch [5/5], Step [829/842], Loss: 0.0134\n",
      "Epoch [5/5], Step [830/842], Loss: 0.0017\n",
      "Epoch [5/5], Step [831/842], Loss: 0.0117\n",
      "Epoch [5/5], Step [832/842], Loss: 0.1584\n",
      "Epoch [5/5], Step [833/842], Loss: 0.0900\n",
      "Epoch [5/5], Step [834/842], Loss: 0.0212\n",
      "Epoch [5/5], Step [835/842], Loss: 0.0161\n",
      "Epoch [5/5], Step [836/842], Loss: 0.1963\n",
      "Epoch [5/5], Step [837/842], Loss: 0.1169\n",
      "Epoch [5/5], Step [838/842], Loss: 0.0285\n",
      "Epoch [5/5], Step [839/842], Loss: 0.0210\n",
      "Epoch [5/5], Step [840/842], Loss: 0.0428\n",
      "Epoch [5/5], Step [841/842], Loss: 0.1043\n",
      "Epoch [5/5], Step [842/842], Loss: 0.1308\n",
      "Epoch [5/5], Step [843/842], Loss: 0.1077\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXkQkyqnoRUSinIpeRCTK/X94heryrP2jjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb44db7d310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7h 35min 3s, sys: 2h 51min 26s, total: 10h 26min 29s\n",
      "Wall time: 5h 1min 53s\n"
     ]
    }
   ],
   "source": [
    "# %time train_vgg16()\n",
    "%time train_resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet18.state_dict(), 'resnet18_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Once finetuning is done we need to test it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images)\n",
    "        \n",
    "        if (use_gpu):\n",
    "            images = images.cuda()\n",
    "            \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "    print('Accuracy of the network on the %d test images: %d %%' % (total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1870 test images: 33 %\n",
      "CPU times: user 3min 59s, sys: 1min 29s, total: 5min 29s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "# %time test(vgg16)\n",
    "resnet18.load_state_dict(torch.load('resnet18_model.pkl'))\n",
    "%time test(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add more code to save the models if you want but otherwise this notebook is complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
